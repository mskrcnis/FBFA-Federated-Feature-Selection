{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_selection import f_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For IoT IDS\n",
    "csv_path = 'ACI/ACI-IoT-2023.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "#print(df.columns)\n",
    "#print(df.shape)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Port Scan             441260\n",
      "Benign                327505\n",
      "ICMP Flood            225234\n",
      "Ping Sweep             71928\n",
      "DNS Flood              46934\n",
      "Vulnerability Scan     39533\n",
      "OS Scan                37524\n",
      "Slowloris              18537\n",
      "SYN Flood              13857\n",
      "Dictionary Attack       6379\n",
      "UDP Flood                791\n",
      "ARP Spoofing               5\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "Attack    901982\n",
      "Benign    327505\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['Label'].value_counts())\n",
    "# Now, remap your classes as before\n",
    "def map_to_class(label):\n",
    "    if label == 'Benign':\n",
    "        return 'Benign'\n",
    "    else:\n",
    "        return 'Attack'\n",
    "df['class'] = df['Label'].apply(map_to_class)\n",
    "print(df['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP','Dst Port', 'Protocol', 'Timestamp','Label','Connection Type','class']\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "label_col = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "X = df[feature_cols].values\n",
    "y = df[label_col].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant features removed: 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold=0.0)\n",
    "X_var = var_thresh.fit_transform(X)\n",
    "print(f\"Constant features removed: {X.shape[1] - X_var.shape[1]}\")\n",
    "X = X_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 class distribution: [43268    23]\n",
      "Client 2 class distribution: [ 1667 38408]\n",
      "Client 3 class distribution: [278907     37]\n",
      "Client 4 class distribution: [98226 44756]\n",
      "Client 5 class distribution: [88716 12632]\n",
      "Client 6 class distribution: [14259   154]\n",
      "Client 7 class distribution: [23414 39636]\n",
      "Client 8 class distribution: [116528   6864]\n",
      "Client 9 class distribution: [54051 42726]\n",
      "Client 10 class distribution: [ 2549 76768]\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "full_dataset = TabularDataset(X, y)\n",
    "train_idx, test_idx = train_test_split(np.arange(len(full_dataset)), test_size=0.2, stratify=y, random_state=6)\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "def partition_tabular_dataset(dataset, labels, train_idx, num_clients=10, alpha=0.5):\n",
    "    np.random.seed(6)\n",
    "    targets = np.array(labels)[train_idx]\n",
    "    num_classes = np.max(targets) + 1\n",
    "    idxs = np.arange(len(targets))\n",
    "    client_idx = [[] for _ in range(num_clients)]\n",
    "    for c in range(num_classes):\n",
    "        idx_c = idxs[targets == c]\n",
    "        np.random.shuffle(idx_c)\n",
    "        proportions = np.random.dirichlet([alpha]*num_clients)\n",
    "        proportions = (np.cumsum(proportions) * len(idx_c)).astype(int)[:-1]\n",
    "        split_idxs = np.split(idx_c, proportions)\n",
    "        for i, idx in enumerate(split_idxs):\n",
    "            client_idx[i].extend(idx)\n",
    "    return client_idx\n",
    "\n",
    "num_clients = 10\n",
    "alpha = 0.5\n",
    "client_indices = partition_tabular_dataset(train_dataset, y, train_idx, num_clients, alpha)\n",
    "\n",
    "client_data_np = []\n",
    "for i in range(num_clients):\n",
    "    idxs = client_indices[i]\n",
    "    X_client = X[train_idx][idxs]\n",
    "    y_client = y[train_idx][idxs]\n",
    "    client_data_np.append((X_client, y_client))\n",
    "\n",
    "for i, (Xc, yc) in enumerate(client_data_np):\n",
    "    print(f\"Client {i+1} class distribution:\", np.bincount(yc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_scores(X, y):\n",
    "    scores, _ = f_classif(X, y)\n",
    "    # Normalize scores to [0,1]\n",
    "    min_val = np.min(scores)\n",
    "    max_val = np.max(scores)\n",
    "    if max_val > min_val:\n",
    "        normalized_scores = (scores - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        normalized_scores = np.zeros_like(scores)\n",
    "    return normalized_scores\n",
    "\n",
    "def compute_corr_matrix(X):\n",
    "    corr = np.corrcoef(X, rowvar=False)\n",
    "    return np.abs(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_feature_subset(subset, fisher_scores, corr_matrix, penalty_lambda=0.7):\n",
    "    if len(subset) == 0:\n",
    "        return 0\n",
    "    fisher_sum = np.sum(fisher_scores[subset])\n",
    "    if len(subset) > 1:\n",
    "        corr_penalty = np.sum(corr_matrix[np.ix_(subset, subset)]) - np.sum(np.diag(corr_matrix[subset][:, subset]))\n",
    "        corr_penalty /= 2\n",
    "    else:\n",
    "        corr_penalty = 0.0\n",
    "    return penalty_lambda * fisher_sum - (1 - penalty_lambda) * corr_penalty\n",
    "\n",
    "def one_step_binary_firefly(\n",
    "    firefly_mask_prev, global_mask_prev, local_best_mask_prev,\n",
    "    fisher_scores, corr_matrix, penalty_lambda=0.7, p_global=0.3, p_local=0.3, mutation_rate=0.05, verbose=False\n",
    "):\n",
    "    n_features = len(firefly_mask_prev)\n",
    "    new_mask = firefly_mask_prev.copy()\n",
    "    for i in range(n_features):\n",
    "        r = np.random.rand()\n",
    "        if r < p_global:\n",
    "            new_mask[i] = global_mask_prev[i]\n",
    "        elif r < p_global + p_local:\n",
    "            new_mask[i] = local_best_mask_prev[i]\n",
    "        elif np.random.rand() < mutation_rate:\n",
    "            new_mask[i] = 1 - new_mask[i]  # mutate\n",
    "\n",
    "    # Optional: flip one bit with small probability for extra exploration\n",
    "    if np.random.rand() < 0.2:\n",
    "        idx = np.random.randint(n_features)\n",
    "        new_mask[idx] = 1 - new_mask[idx]\n",
    "\n",
    "    if verbose:\n",
    "        sel = np.where(new_mask)[0]\n",
    "        fit = evaluate_feature_subset(sel, fisher_scores, corr_matrix, penalty_lambda)\n",
    "        print(f\"    - New mask: {np.sum(new_mask)} features, Fitness: {fit:.4f}\")\n",
    "\n",
    "    return new_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Federated BFA Round 1 ================\n",
      "  Adaptive rho for this round: 0.20\n",
      "    - New mask: 45 features, Fitness: -13.9524\n",
      "    - New mask: 40 features, Fitness: -12.0330\n",
      "    - New mask: 47 features, Fitness: -17.0082\n",
      "    - New mask: 55 features, Fitness: -22.6054\n",
      "    - New mask: 48 features, Fitness: -17.7321\n",
      "    - New mask: 34 features, Fitness: -8.9148\n",
      "    - New mask: 43 features, Fitness: -13.9229\n",
      "    - New mask: 44 features, Fitness: -13.5382\n",
      "    - New mask: 42 features, Fitness: -12.5026\n",
      "    - New mask: 43 features, Fitness: -14.3377\n",
      "    - New mask: 40 features, Fitness: -11.6040\n",
      "    - New mask: 49 features, Fitness: -20.4368\n",
      "    - New mask: 45 features, Fitness: -17.1015\n",
      "    - New mask: 38 features, Fitness: -7.7240\n",
      "    - New mask: 46 features, Fitness: -16.0311\n",
      "    - New mask: 41 features, Fitness: -9.9969\n",
      "    - New mask: 47 features, Fitness: -15.3677\n",
      "    - New mask: 44 features, Fitness: -14.8329\n",
      "    - New mask: 50 features, Fitness: -20.4227\n",
      "    - New mask: 43 features, Fitness: -14.2223\n",
      "    - New mask: 48 features, Fitness: -12.7699\n",
      "    - New mask: 49 features, Fitness: -16.5073\n",
      "    - New mask: 49 features, Fitness: -14.5376\n",
      "    - New mask: 42 features, Fitness: -9.3516\n",
      "    - New mask: 46 features, Fitness: -12.4180\n",
      "    - New mask: 34 features, Fitness: -5.6229\n",
      "    - New mask: 43 features, Fitness: -9.4149\n",
      "    - New mask: 47 features, Fitness: -12.7559\n",
      "    - New mask: 45 features, Fitness: -10.5711\n",
      "    - New mask: 53 features, Fitness: -16.4303\n",
      "    - New mask: 47 features, Fitness: -13.0757\n",
      "    - New mask: 42 features, Fitness: -11.6196\n",
      "    - New mask: 35 features, Fitness: -5.7686\n",
      "    - New mask: 44 features, Fitness: -9.8706\n",
      "    - New mask: 40 features, Fitness: -6.6656\n",
      "    - New mask: 48 features, Fitness: -14.8358\n",
      "    - New mask: 51 features, Fitness: -16.6777\n",
      "    - New mask: 39 features, Fitness: -7.8467\n",
      "    - New mask: 45 features, Fitness: -9.5792\n",
      "    - New mask: 43 features, Fitness: -9.9411\n",
      "    - New mask: 37 features, Fitness: -6.3745\n",
      "    - New mask: 45 features, Fitness: -9.4666\n",
      "    - New mask: 36 features, Fitness: -4.3175\n",
      "    - New mask: 45 features, Fitness: -10.5347\n",
      "    - New mask: 48 features, Fitness: -14.6104\n",
      "    - New mask: 50 features, Fitness: -12.0236\n",
      "    - New mask: 40 features, Fitness: -6.3851\n",
      "    - New mask: 43 features, Fitness: -11.3890\n",
      "    - New mask: 49 features, Fitness: -11.4269\n",
      "    - New mask: 44 features, Fitness: -11.4668\n",
      "    - New mask: 48 features, Fitness: -13.8950\n",
      "    - New mask: 39 features, Fitness: -6.3964\n",
      "    - New mask: 42 features, Fitness: -9.1639\n",
      "    - New mask: 44 features, Fitness: -10.3346\n",
      "    - New mask: 43 features, Fitness: -10.1515\n",
      "    - New mask: 40 features, Fitness: -6.4554\n",
      "    - New mask: 49 features, Fitness: -15.0792\n",
      "    - New mask: 37 features, Fitness: -6.7295\n",
      "    - New mask: 45 features, Fitness: -10.5560\n",
      "    - New mask: 39 features, Fitness: -7.7978\n",
      "    - New mask: 44 features, Fitness: -8.8388\n",
      "    - New mask: 41 features, Fitness: -6.9110\n",
      "    - New mask: 46 features, Fitness: -9.2774\n",
      "    - New mask: 41 features, Fitness: -7.6606\n",
      "    - New mask: 45 features, Fitness: -9.0248\n",
      "    - New mask: 40 features, Fitness: -6.9612\n",
      "    - New mask: 44 features, Fitness: -7.8722\n",
      "    - New mask: 47 features, Fitness: -10.7748\n",
      "    - New mask: 48 features, Fitness: -10.1883\n",
      "    - New mask: 48 features, Fitness: -8.2202\n",
      "    - New mask: 48 features, Fitness: -9.3859\n",
      "    - New mask: 42 features, Fitness: -5.9202\n",
      "    - New mask: 53 features, Fitness: -11.4492\n",
      "    - New mask: 45 features, Fitness: -7.4396\n",
      "    - New mask: 44 features, Fitness: -8.7834\n",
      "    - New mask: 46 features, Fitness: -8.2023\n",
      "    - New mask: 40 features, Fitness: -5.9664\n",
      "    - New mask: 35 features, Fitness: -3.4431\n",
      "    - New mask: 47 features, Fitness: -9.0324\n",
      "    - New mask: 45 features, Fitness: -6.8780\n",
      "    - New mask: 46 features, Fitness: -4.3703\n",
      "    - New mask: 45 features, Fitness: -2.1984\n",
      "    - New mask: 43 features, Fitness: -3.0120\n",
      "    - New mask: 44 features, Fitness: -2.2393\n",
      "    - New mask: 46 features, Fitness: -3.6775\n",
      "    - New mask: 50 features, Fitness: -4.9042\n",
      "    - New mask: 45 features, Fitness: -4.5169\n",
      "    - New mask: 45 features, Fitness: -3.8514\n",
      "    - New mask: 47 features, Fitness: -2.2816\n",
      "    - New mask: 36 features, Fitness: 0.5590\n",
      "    - New mask: 46 features, Fitness: -3.7466\n",
      "    - New mask: 42 features, Fitness: -1.8088\n",
      "    - New mask: 47 features, Fitness: -2.9824\n",
      "    - New mask: 44 features, Fitness: -2.9905\n",
      "    - New mask: 50 features, Fitness: -5.9717\n",
      "    - New mask: 47 features, Fitness: -2.4803\n",
      "    - New mask: 43 features, Fitness: -3.3327\n",
      "    - New mask: 47 features, Fitness: -2.4662\n",
      "    - New mask: 51 features, Fitness: -5.7962\n",
      "    - New mask: 39 features, Fitness: 1.4867\n",
      "    - New mask: 44 features, Fitness: -8.6583\n",
      "    - New mask: 47 features, Fitness: -12.7913\n",
      "    - New mask: 39 features, Fitness: -6.7110\n",
      "    - New mask: 43 features, Fitness: -12.6462\n",
      "    - New mask: 43 features, Fitness: -8.0948\n",
      "    - New mask: 45 features, Fitness: -11.7935\n",
      "    - New mask: 37 features, Fitness: -5.6268\n",
      "    - New mask: 38 features, Fitness: -6.0429\n",
      "    - New mask: 43 features, Fitness: -12.0524\n",
      "    - New mask: 45 features, Fitness: -9.0012\n",
      "    - New mask: 43 features, Fitness: -10.0780\n",
      "    - New mask: 51 features, Fitness: -15.7573\n",
      "    - New mask: 42 features, Fitness: -7.0581\n",
      "    - New mask: 42 features, Fitness: -8.0655\n",
      "    - New mask: 43 features, Fitness: -8.4850\n",
      "    - New mask: 41 features, Fitness: -8.1713\n",
      "    - New mask: 43 features, Fitness: -10.2828\n",
      "    - New mask: 41 features, Fitness: -6.6556\n",
      "    - New mask: 38 features, Fitness: -8.5774\n",
      "    - New mask: 45 features, Fitness: -10.8526\n",
      "    - New mask: 42 features, Fitness: -8.7588\n",
      "    - New mask: 43 features, Fitness: -8.0089\n",
      "    - New mask: 46 features, Fitness: -10.9676\n",
      "    - New mask: 44 features, Fitness: -9.2984\n",
      "    - New mask: 46 features, Fitness: -11.7358\n",
      "    - New mask: 50 features, Fitness: -15.3986\n",
      "    - New mask: 46 features, Fitness: -9.2511\n",
      "    - New mask: 46 features, Fitness: -10.6704\n",
      "    - New mask: 46 features, Fitness: -9.9508\n",
      "    - New mask: 39 features, Fitness: -6.0750\n",
      "    - New mask: 45 features, Fitness: -9.1479\n",
      "    - New mask: 41 features, Fitness: -8.4413\n",
      "    - New mask: 43 features, Fitness: -7.8854\n",
      "    - New mask: 51 features, Fitness: -14.5174\n",
      "    - New mask: 46 features, Fitness: -12.8238\n",
      "    - New mask: 41 features, Fitness: -9.1959\n",
      "    - New mask: 45 features, Fitness: -9.9055\n",
      "    - New mask: 46 features, Fitness: -9.0313\n",
      "    - New mask: 38 features, Fitness: -8.9036\n",
      "    - New mask: 48 features, Fitness: -12.8676\n",
      "    - New mask: 44 features, Fitness: -3.6966\n",
      "    - New mask: 43 features, Fitness: -3.1709\n",
      "    - New mask: 38 features, Fitness: -3.3096\n",
      "    - New mask: 41 features, Fitness: -3.6268\n",
      "    - New mask: 47 features, Fitness: -6.3137\n",
      "    - New mask: 42 features, Fitness: -6.4748\n",
      "    - New mask: 44 features, Fitness: -4.5385\n",
      "    - New mask: 44 features, Fitness: -7.1981\n",
      "    - New mask: 47 features, Fitness: -7.9433\n",
      "    - New mask: 52 features, Fitness: -9.7956\n",
      "    - New mask: 43 features, Fitness: -4.6022\n",
      "    - New mask: 43 features, Fitness: -4.4588\n",
      "    - New mask: 47 features, Fitness: -6.1893\n",
      "    - New mask: 41 features, Fitness: -3.3445\n",
      "    - New mask: 44 features, Fitness: -6.9131\n",
      "    - New mask: 48 features, Fitness: -7.3657\n",
      "    - New mask: 49 features, Fitness: -6.5229\n",
      "    - New mask: 46 features, Fitness: -7.3436\n",
      "    - New mask: 42 features, Fitness: -4.2316\n",
      "    - New mask: 43 features, Fitness: -4.3423\n",
      "    - New mask: 39 features, Fitness: -7.7450\n",
      "    - New mask: 41 features, Fitness: -6.9998\n",
      "    - New mask: 46 features, Fitness: -11.9041\n",
      "    - New mask: 38 features, Fitness: -5.6300\n",
      "    - New mask: 45 features, Fitness: -10.8154\n",
      "    - New mask: 46 features, Fitness: -9.3862\n",
      "    - New mask: 48 features, Fitness: -10.7540\n",
      "    - New mask: 41 features, Fitness: -7.2490\n",
      "    - New mask: 47 features, Fitness: -10.7347\n",
      "    - New mask: 44 features, Fitness: -10.7718\n",
      "    - New mask: 42 features, Fitness: -7.8254\n",
      "    - New mask: 42 features, Fitness: -8.9361\n",
      "    - New mask: 42 features, Fitness: -10.2933\n",
      "    - New mask: 40 features, Fitness: -7.8106\n",
      "    - New mask: 44 features, Fitness: -10.8158\n",
      "    - New mask: 39 features, Fitness: -7.1802\n",
      "    - New mask: 42 features, Fitness: -8.5169\n",
      "    - New mask: 36 features, Fitness: -6.5410\n",
      "    - New mask: 43 features, Fitness: -10.2249\n",
      "    - New mask: 47 features, Fitness: -12.6095\n",
      "    - New mask: 45 features, Fitness: -12.3077\n",
      "    - New mask: 45 features, Fitness: -10.2803\n",
      "    - New mask: 45 features, Fitness: -11.0235\n",
      "    - New mask: 41 features, Fitness: -8.6539\n",
      "    - New mask: 34 features, Fitness: -6.6579\n",
      "    - New mask: 48 features, Fitness: -12.7663\n",
      "    - New mask: 48 features, Fitness: -14.7567\n",
      "    - New mask: 40 features, Fitness: -8.6825\n",
      "    - New mask: 47 features, Fitness: -13.3498\n",
      "    - New mask: 42 features, Fitness: -10.2218\n",
      "    - New mask: 45 features, Fitness: -11.7564\n",
      "    - New mask: 45 features, Fitness: -11.9638\n",
      "    - New mask: 44 features, Fitness: -9.3498\n",
      "    - New mask: 40 features, Fitness: -9.5240\n",
      "    - New mask: 39 features, Fitness: -7.9078\n",
      "    - New mask: 41 features, Fitness: -8.0723\n",
      "    - New mask: 46 features, Fitness: -11.4631\n",
      "    - New mask: 43 features, Fitness: -9.6476\n",
      "    - New mask: 47 features, Fitness: -11.7839\n",
      "    - New mask: 47 features, Fitness: -11.4322\n",
      "=== End of Round 1: Vote mask selects 69 features (rho: 0.20)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 2 ================\n",
      "  Adaptive rho for this round: 0.23\n",
      "    - New mask: 49 features, Fitness: -16.3079\n",
      "    - New mask: 46 features, Fitness: -14.3481\n",
      "    - New mask: 46 features, Fitness: -13.0767\n",
      "    - New mask: 53 features, Fitness: -20.3782\n",
      "    - New mask: 54 features, Fitness: -21.6354\n",
      "    - New mask: 50 features, Fitness: -17.4511\n",
      "    - New mask: 41 features, Fitness: -10.0504\n",
      "    - New mask: 45 features, Fitness: -13.5434\n",
      "    - New mask: 48 features, Fitness: -16.0891\n",
      "    - New mask: 55 features, Fitness: -24.3917\n",
      "    - New mask: 50 features, Fitness: -19.8221\n",
      "    - New mask: 48 features, Fitness: -18.1504\n",
      "    - New mask: 57 features, Fitness: -27.1704\n",
      "    - New mask: 47 features, Fitness: -13.1028\n",
      "    - New mask: 47 features, Fitness: -15.5414\n",
      "    - New mask: 51 features, Fitness: -18.5599\n",
      "    - New mask: 46 features, Fitness: -13.7664\n",
      "    - New mask: 50 features, Fitness: -17.8475\n",
      "    - New mask: 50 features, Fitness: -20.4571\n",
      "    - New mask: 48 features, Fitness: -17.7986\n",
      "    - New mask: 51 features, Fitness: -15.6470\n",
      "    - New mask: 44 features, Fitness: -14.7013\n",
      "    - New mask: 49 features, Fitness: -13.4958\n",
      "    - New mask: 45 features, Fitness: -11.6886\n",
      "    - New mask: 45 features, Fitness: -12.2344\n",
      "    - New mask: 46 features, Fitness: -12.3265\n",
      "    - New mask: 49 features, Fitness: -15.6348\n",
      "    - New mask: 50 features, Fitness: -14.8792\n",
      "    - New mask: 45 features, Fitness: -10.9689\n",
      "    - New mask: 49 features, Fitness: -11.5736\n",
      "    - New mask: 48 features, Fitness: -12.7540\n",
      "    - New mask: 55 features, Fitness: -19.0691\n",
      "    - New mask: 43 features, Fitness: -9.5961\n",
      "    - New mask: 50 features, Fitness: -13.6870\n",
      "    - New mask: 46 features, Fitness: -11.6214\n",
      "    - New mask: 55 features, Fitness: -17.9753\n",
      "    - New mask: 50 features, Fitness: -15.7131\n",
      "    - New mask: 39 features, Fitness: -7.6529\n",
      "    - New mask: 47 features, Fitness: -12.9014\n",
      "    - New mask: 48 features, Fitness: -12.8456\n",
      "    - New mask: 43 features, Fitness: -10.5925\n",
      "    - New mask: 51 features, Fitness: -10.9912\n",
      "    - New mask: 46 features, Fitness: -10.1820\n",
      "    - New mask: 52 features, Fitness: -13.1899\n",
      "    - New mask: 54 features, Fitness: -17.3806\n",
      "    - New mask: 53 features, Fitness: -15.2689\n",
      "    - New mask: 53 features, Fitness: -16.5519\n",
      "    - New mask: 51 features, Fitness: -13.8673\n",
      "    - New mask: 43 features, Fitness: -7.5071\n",
      "    - New mask: 48 features, Fitness: -11.6981\n",
      "    - New mask: 50 features, Fitness: -13.4904\n",
      "    - New mask: 48 features, Fitness: -10.0984\n",
      "    - New mask: 47 features, Fitness: -11.7777\n",
      "    - New mask: 47 features, Fitness: -10.5920\n",
      "    - New mask: 48 features, Fitness: -12.1176\n",
      "    - New mask: 46 features, Fitness: -8.8317\n",
      "    - New mask: 47 features, Fitness: -11.8698\n",
      "    - New mask: 45 features, Fitness: -10.5145\n",
      "    - New mask: 47 features, Fitness: -10.4171\n",
      "    - New mask: 48 features, Fitness: -11.2923\n",
      "    - New mask: 44 features, Fitness: -7.7811\n",
      "    - New mask: 49 features, Fitness: -10.2100\n",
      "    - New mask: 44 features, Fitness: -7.4052\n",
      "    - New mask: 53 features, Fitness: -14.2838\n",
      "    - New mask: 47 features, Fitness: -8.7361\n",
      "    - New mask: 43 features, Fitness: -7.5306\n",
      "    - New mask: 51 features, Fitness: -9.5752\n",
      "    - New mask: 46 features, Fitness: -7.9670\n",
      "    - New mask: 43 features, Fitness: -7.2912\n",
      "    - New mask: 52 features, Fitness: -10.5141\n",
      "    - New mask: 52 features, Fitness: -11.2382\n",
      "    - New mask: 42 features, Fitness: -8.8733\n",
      "    - New mask: 54 features, Fitness: -13.0950\n",
      "    - New mask: 47 features, Fitness: -8.3498\n",
      "    - New mask: 57 features, Fitness: -13.5891\n",
      "    - New mask: 52 features, Fitness: -12.2636\n",
      "    - New mask: 42 features, Fitness: -5.9457\n",
      "    - New mask: 41 features, Fitness: -6.4573\n",
      "    - New mask: 48 features, Fitness: -7.8275\n",
      "    - New mask: 50 features, Fitness: -9.7343\n",
      "    - New mask: 49 features, Fitness: -5.7067\n",
      "    - New mask: 47 features, Fitness: -2.0609\n",
      "    - New mask: 51 features, Fitness: -5.3864\n",
      "    - New mask: 49 features, Fitness: -4.2031\n",
      "    - New mask: 51 features, Fitness: -5.8791\n",
      "    - New mask: 51 features, Fitness: -5.9514\n",
      "    - New mask: 49 features, Fitness: -5.8389\n",
      "    - New mask: 45 features, Fitness: -2.6406\n",
      "    - New mask: 49 features, Fitness: -6.0558\n",
      "    - New mask: 48 features, Fitness: -4.4245\n",
      "    - New mask: 53 features, Fitness: -5.4537\n",
      "    - New mask: 48 features, Fitness: -3.0483\n",
      "    - New mask: 53 features, Fitness: -5.0648\n",
      "    - New mask: 48 features, Fitness: -4.5244\n",
      "    - New mask: 54 features, Fitness: -6.8997\n",
      "    - New mask: 48 features, Fitness: -1.7663\n",
      "    - New mask: 49 features, Fitness: -5.4151\n",
      "    - New mask: 50 features, Fitness: -5.1863\n",
      "    - New mask: 42 features, Fitness: 0.3412\n",
      "    - New mask: 48 features, Fitness: -3.0562\n",
      "    - New mask: 47 features, Fitness: -11.4869\n",
      "    - New mask: 50 features, Fitness: -12.5100\n",
      "    - New mask: 47 features, Fitness: -11.8643\n",
      "    - New mask: 48 features, Fitness: -14.4674\n",
      "    - New mask: 45 features, Fitness: -9.1647\n",
      "    - New mask: 52 features, Fitness: -16.4487\n",
      "    - New mask: 43 features, Fitness: -9.7572\n",
      "    - New mask: 55 features, Fitness: -16.5597\n",
      "    - New mask: 50 features, Fitness: -14.5755\n",
      "    - New mask: 51 features, Fitness: -14.8074\n",
      "    - New mask: 49 features, Fitness: -12.2714\n",
      "    - New mask: 52 features, Fitness: -15.5210\n",
      "    - New mask: 51 features, Fitness: -13.1522\n",
      "    - New mask: 50 features, Fitness: -12.7875\n",
      "    - New mask: 51 features, Fitness: -14.1839\n",
      "    - New mask: 46 features, Fitness: -9.8072\n",
      "    - New mask: 50 features, Fitness: -15.2195\n",
      "    - New mask: 44 features, Fitness: -8.1442\n",
      "    - New mask: 45 features, Fitness: -11.7048\n",
      "    - New mask: 48 features, Fitness: -10.1852\n",
      "    - New mask: 53 features, Fitness: -16.1497\n",
      "    - New mask: 47 features, Fitness: -10.2344\n",
      "    - New mask: 48 features, Fitness: -11.9447\n",
      "    - New mask: 49 features, Fitness: -12.6157\n",
      "    - New mask: 52 features, Fitness: -14.5272\n",
      "    - New mask: 54 features, Fitness: -17.5917\n",
      "    - New mask: 56 features, Fitness: -16.1569\n",
      "    - New mask: 46 features, Fitness: -9.3936\n",
      "    - New mask: 53 features, Fitness: -16.5318\n",
      "    - New mask: 48 features, Fitness: -10.1683\n",
      "    - New mask: 50 features, Fitness: -11.2175\n",
      "    - New mask: 49 features, Fitness: -11.9286\n",
      "    - New mask: 56 features, Fitness: -16.0227\n",
      "    - New mask: 46 features, Fitness: -9.6241\n",
      "    - New mask: 54 features, Fitness: -17.6355\n",
      "    - New mask: 43 features, Fitness: -10.5861\n",
      "    - New mask: 51 features, Fitness: -13.9700\n",
      "    - New mask: 53 features, Fitness: -13.8895\n",
      "    - New mask: 46 features, Fitness: -12.1491\n",
      "    - New mask: 51 features, Fitness: -12.5281\n",
      "    - New mask: 50 features, Fitness: -6.9355\n",
      "    - New mask: 51 features, Fitness: -6.9409\n",
      "    - New mask: 47 features, Fitness: -5.5716\n",
      "    - New mask: 48 features, Fitness: -8.4200\n",
      "    - New mask: 58 features, Fitness: -11.2695\n",
      "    - New mask: 54 features, Fitness: -9.5252\n",
      "    - New mask: 52 features, Fitness: -8.6201\n",
      "    - New mask: 51 features, Fitness: -7.3931\n",
      "    - New mask: 52 features, Fitness: -9.1893\n",
      "    - New mask: 53 features, Fitness: -8.9682\n",
      "    - New mask: 51 features, Fitness: -8.9492\n",
      "    - New mask: 47 features, Fitness: -7.4997\n",
      "    - New mask: 52 features, Fitness: -8.3916\n",
      "    - New mask: 49 features, Fitness: -6.6542\n",
      "    - New mask: 50 features, Fitness: -8.2737\n",
      "    - New mask: 52 features, Fitness: -10.6143\n",
      "    - New mask: 49 features, Fitness: -6.5945\n",
      "    - New mask: 51 features, Fitness: -9.0494\n",
      "    - New mask: 50 features, Fitness: -7.6078\n",
      "    - New mask: 44 features, Fitness: -4.5371\n",
      "    - New mask: 45 features, Fitness: -9.0198\n",
      "    - New mask: 52 features, Fitness: -14.0718\n",
      "    - New mask: 54 features, Fitness: -16.6666\n",
      "    - New mask: 48 features, Fitness: -10.5353\n",
      "    - New mask: 52 features, Fitness: -14.8864\n",
      "    - New mask: 54 features, Fitness: -14.8508\n",
      "    - New mask: 47 features, Fitness: -11.9400\n",
      "    - New mask: 43 features, Fitness: -8.1276\n",
      "    - New mask: 49 features, Fitness: -11.8565\n",
      "    - New mask: 53 features, Fitness: -17.5216\n",
      "    - New mask: 46 features, Fitness: -11.8110\n",
      "    - New mask: 49 features, Fitness: -11.6232\n",
      "    - New mask: 54 features, Fitness: -15.8443\n",
      "    - New mask: 46 features, Fitness: -8.9445\n",
      "    - New mask: 50 features, Fitness: -13.9060\n",
      "    - New mask: 51 features, Fitness: -14.1448\n",
      "    - New mask: 54 features, Fitness: -14.4961\n",
      "    - New mask: 49 features, Fitness: -12.2897\n",
      "    - New mask: 49 features, Fitness: -11.2883\n",
      "    - New mask: 49 features, Fitness: -12.8395\n",
      "    - New mask: 45 features, Fitness: -13.0145\n",
      "    - New mask: 50 features, Fitness: -15.5593\n",
      "    - New mask: 47 features, Fitness: -13.9479\n",
      "    - New mask: 42 features, Fitness: -10.5751\n",
      "    - New mask: 50 features, Fitness: -14.8712\n",
      "    - New mask: 55 features, Fitness: -18.1642\n",
      "    - New mask: 47 features, Fitness: -15.1073\n",
      "    - New mask: 48 features, Fitness: -13.3259\n",
      "    - New mask: 48 features, Fitness: -12.6708\n",
      "    - New mask: 43 features, Fitness: -10.1880\n",
      "    - New mask: 54 features, Fitness: -18.8099\n",
      "    - New mask: 52 features, Fitness: -18.8688\n",
      "    - New mask: 47 features, Fitness: -11.6926\n",
      "    - New mask: 46 features, Fitness: -13.8352\n",
      "    - New mask: 52 features, Fitness: -15.2248\n",
      "    - New mask: 45 features, Fitness: -11.0165\n",
      "    - New mask: 51 features, Fitness: -15.3117\n",
      "    - New mask: 45 features, Fitness: -10.1472\n",
      "    - New mask: 50 features, Fitness: -13.1539\n",
      "    - New mask: 47 features, Fitness: -12.6268\n",
      "=== End of Round 2: Vote mask selects 67 features (rho: 0.23)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 3 ================\n",
      "  Adaptive rho for this round: 0.26\n",
      "    - New mask: 45 features, Fitness: -13.9824\n",
      "    - New mask: 52 features, Fitness: -20.6598\n",
      "    - New mask: 49 features, Fitness: -17.0424\n",
      "    - New mask: 58 features, Fitness: -25.7454\n",
      "    - New mask: 50 features, Fitness: -18.0242\n",
      "    - New mask: 56 features, Fitness: -22.9619\n",
      "    - New mask: 46 features, Fitness: -14.8812\n",
      "    - New mask: 49 features, Fitness: -16.3104\n",
      "    - New mask: 46 features, Fitness: -14.3170\n",
      "    - New mask: 56 features, Fitness: -23.9853\n",
      "    - New mask: 49 features, Fitness: -17.8028\n",
      "    - New mask: 57 features, Fitness: -24.4345\n",
      "    - New mask: 53 features, Fitness: -21.1850\n",
      "    - New mask: 54 features, Fitness: -22.3451\n",
      "    - New mask: 54 features, Fitness: -22.3623\n",
      "    - New mask: 47 features, Fitness: -14.9065\n",
      "    - New mask: 46 features, Fitness: -13.1692\n",
      "    - New mask: 49 features, Fitness: -16.8636\n",
      "    - New mask: 49 features, Fitness: -17.7844\n",
      "    - New mask: 49 features, Fitness: -21.5983\n",
      "    - New mask: 48 features, Fitness: -13.9932\n",
      "    - New mask: 48 features, Fitness: -13.4134\n",
      "    - New mask: 53 features, Fitness: -14.1632\n",
      "    - New mask: 49 features, Fitness: -13.0721\n",
      "    - New mask: 53 features, Fitness: -15.4181\n",
      "    - New mask: 53 features, Fitness: -16.9234\n",
      "    - New mask: 51 features, Fitness: -16.1954\n",
      "    - New mask: 49 features, Fitness: -13.2155\n",
      "    - New mask: 49 features, Fitness: -13.2333\n",
      "    - New mask: 56 features, Fitness: -17.3745\n",
      "    - New mask: 55 features, Fitness: -16.5954\n",
      "    - New mask: 63 features, Fitness: -22.7386\n",
      "    - New mask: 47 features, Fitness: -11.1487\n",
      "    - New mask: 55 features, Fitness: -16.6728\n",
      "    - New mask: 49 features, Fitness: -13.2075\n",
      "    - New mask: 50 features, Fitness: -15.4550\n",
      "    - New mask: 43 features, Fitness: -11.4064\n",
      "    - New mask: 47 features, Fitness: -12.9090\n",
      "    - New mask: 49 features, Fitness: -13.0441\n",
      "    - New mask: 50 features, Fitness: -15.0272\n",
      "    - New mask: 48 features, Fitness: -12.6871\n",
      "    - New mask: 56 features, Fitness: -17.6239\n",
      "    - New mask: 51 features, Fitness: -14.0301\n",
      "    - New mask: 52 features, Fitness: -13.2860\n",
      "    - New mask: 56 features, Fitness: -16.9830\n",
      "    - New mask: 51 features, Fitness: -11.8225\n",
      "    - New mask: 59 features, Fitness: -19.9498\n",
      "    - New mask: 51 features, Fitness: -13.4245\n",
      "    - New mask: 48 features, Fitness: -10.2628\n",
      "    - New mask: 55 features, Fitness: -18.2288\n",
      "    - New mask: 52 features, Fitness: -14.5199\n",
      "    - New mask: 50 features, Fitness: -13.5024\n",
      "    - New mask: 44 features, Fitness: -8.5006\n",
      "    - New mask: 51 features, Fitness: -13.6526\n",
      "    - New mask: 52 features, Fitness: -14.2160\n",
      "    - New mask: 54 features, Fitness: -16.5477\n",
      "    - New mask: 47 features, Fitness: -10.9085\n",
      "    - New mask: 50 features, Fitness: -12.2857\n",
      "    - New mask: 53 features, Fitness: -14.5516\n",
      "    - New mask: 51 features, Fitness: -12.6393\n",
      "    - New mask: 54 features, Fitness: -13.1729\n",
      "    - New mask: 52 features, Fitness: -11.0191\n",
      "    - New mask: 42 features, Fitness: -6.3484\n",
      "    - New mask: 56 features, Fitness: -13.7000\n",
      "    - New mask: 54 features, Fitness: -11.5488\n",
      "    - New mask: 47 features, Fitness: -9.7202\n",
      "    - New mask: 52 features, Fitness: -8.9182\n",
      "    - New mask: 50 features, Fitness: -10.6187\n",
      "    - New mask: 53 features, Fitness: -13.9453\n",
      "    - New mask: 55 features, Fitness: -11.7422\n",
      "    - New mask: 48 features, Fitness: -9.8168\n",
      "    - New mask: 52 features, Fitness: -11.5747\n",
      "    - New mask: 52 features, Fitness: -11.2120\n",
      "    - New mask: 44 features, Fitness: -6.7726\n",
      "    - New mask: 59 features, Fitness: -16.4495\n",
      "    - New mask: 54 features, Fitness: -11.5122\n",
      "    - New mask: 50 features, Fitness: -11.4026\n",
      "    - New mask: 54 features, Fitness: -12.3494\n",
      "    - New mask: 50 features, Fitness: -7.6851\n",
      "    - New mask: 53 features, Fitness: -12.5693\n",
      "    - New mask: 48 features, Fitness: -3.3688\n",
      "    - New mask: 52 features, Fitness: -4.1839\n",
      "    - New mask: 50 features, Fitness: -1.3763\n",
      "    - New mask: 53 features, Fitness: -5.6934\n",
      "    - New mask: 48 features, Fitness: -4.0560\n",
      "    - New mask: 48 features, Fitness: -4.8538\n",
      "    - New mask: 51 features, Fitness: -5.3700\n",
      "    - New mask: 49 features, Fitness: -5.2323\n",
      "    - New mask: 55 features, Fitness: -7.7345\n",
      "    - New mask: 49 features, Fitness: -3.5225\n",
      "    - New mask: 51 features, Fitness: -3.3312\n",
      "    - New mask: 50 features, Fitness: -5.0289\n",
      "    - New mask: 52 features, Fitness: -5.6989\n",
      "    - New mask: 51 features, Fitness: -4.2671\n",
      "    - New mask: 54 features, Fitness: -5.1280\n",
      "    - New mask: 54 features, Fitness: -3.5769\n",
      "    - New mask: 49 features, Fitness: -4.5888\n",
      "    - New mask: 58 features, Fitness: -9.3028\n",
      "    - New mask: 45 features, Fitness: -1.1148\n",
      "    - New mask: 51 features, Fitness: -5.4407\n",
      "    - New mask: 61 features, Fitness: -20.6049\n",
      "    - New mask: 48 features, Fitness: -11.3865\n",
      "    - New mask: 49 features, Fitness: -10.6061\n",
      "    - New mask: 54 features, Fitness: -17.4723\n",
      "    - New mask: 48 features, Fitness: -11.3739\n",
      "    - New mask: 51 features, Fitness: -12.3516\n",
      "    - New mask: 48 features, Fitness: -11.7497\n",
      "    - New mask: 54 features, Fitness: -14.6128\n",
      "    - New mask: 47 features, Fitness: -11.2051\n",
      "    - New mask: 55 features, Fitness: -16.3083\n",
      "    - New mask: 49 features, Fitness: -12.9431\n",
      "    - New mask: 56 features, Fitness: -17.1219\n",
      "    - New mask: 54 features, Fitness: -15.1397\n",
      "    - New mask: 56 features, Fitness: -17.9260\n",
      "    - New mask: 55 features, Fitness: -15.9908\n",
      "    - New mask: 50 features, Fitness: -11.9597\n",
      "    - New mask: 50 features, Fitness: -13.6272\n",
      "    - New mask: 48 features, Fitness: -10.4856\n",
      "    - New mask: 53 features, Fitness: -13.7771\n",
      "    - New mask: 54 features, Fitness: -15.2299\n",
      "    - New mask: 52 features, Fitness: -12.1643\n",
      "    - New mask: 48 features, Fitness: -11.1322\n",
      "    - New mask: 53 features, Fitness: -16.1562\n",
      "    - New mask: 54 features, Fitness: -14.6549\n",
      "    - New mask: 57 features, Fitness: -17.4482\n",
      "    - New mask: 60 features, Fitness: -18.4932\n",
      "    - New mask: 56 features, Fitness: -16.1063\n",
      "    - New mask: 55 features, Fitness: -15.0793\n",
      "    - New mask: 58 features, Fitness: -17.8479\n",
      "    - New mask: 57 features, Fitness: -16.3547\n",
      "    - New mask: 54 features, Fitness: -16.2697\n",
      "    - New mask: 52 features, Fitness: -13.1540\n",
      "    - New mask: 52 features, Fitness: -12.5367\n",
      "    - New mask: 51 features, Fitness: -13.0132\n",
      "    - New mask: 58 features, Fitness: -19.8823\n",
      "    - New mask: 54 features, Fitness: -15.4260\n",
      "    - New mask: 52 features, Fitness: -14.4151\n",
      "    - New mask: 55 features, Fitness: -16.4603\n",
      "    - New mask: 55 features, Fitness: -15.4149\n",
      "    - New mask: 58 features, Fitness: -18.6201\n",
      "    - New mask: 49 features, Fitness: -6.5476\n",
      "    - New mask: 56 features, Fitness: -10.6826\n",
      "    - New mask: 53 features, Fitness: -9.5921\n",
      "    - New mask: 58 features, Fitness: -13.3165\n",
      "    - New mask: 54 features, Fitness: -11.0048\n",
      "    - New mask: 53 features, Fitness: -7.9942\n",
      "    - New mask: 55 features, Fitness: -8.2223\n",
      "    - New mask: 58 features, Fitness: -11.7823\n",
      "    - New mask: 55 features, Fitness: -10.7265\n",
      "    - New mask: 56 features, Fitness: -11.2861\n",
      "    - New mask: 54 features, Fitness: -10.3787\n",
      "    - New mask: 49 features, Fitness: -7.5415\n",
      "    - New mask: 53 features, Fitness: -8.2934\n",
      "    - New mask: 47 features, Fitness: -7.5649\n",
      "    - New mask: 49 features, Fitness: -6.7302\n",
      "    - New mask: 54 features, Fitness: -11.5941\n",
      "    - New mask: 56 features, Fitness: -11.0714\n",
      "    - New mask: 56 features, Fitness: -11.6918\n",
      "    - New mask: 52 features, Fitness: -7.2284\n",
      "    - New mask: 54 features, Fitness: -9.2556\n",
      "    - New mask: 45 features, Fitness: -10.2692\n",
      "    - New mask: 50 features, Fitness: -12.3571\n",
      "    - New mask: 57 features, Fitness: -17.2160\n",
      "    - New mask: 51 features, Fitness: -13.5995\n",
      "    - New mask: 55 features, Fitness: -15.4667\n",
      "    - New mask: 50 features, Fitness: -12.0508\n",
      "    - New mask: 47 features, Fitness: -9.1674\n",
      "    - New mask: 54 features, Fitness: -14.3220\n",
      "    - New mask: 49 features, Fitness: -11.4526\n",
      "    - New mask: 55 features, Fitness: -16.2883\n",
      "    - New mask: 49 features, Fitness: -12.5334\n",
      "    - New mask: 52 features, Fitness: -15.7468\n",
      "    - New mask: 48 features, Fitness: -10.6355\n",
      "    - New mask: 54 features, Fitness: -14.9932\n",
      "    - New mask: 53 features, Fitness: -15.5959\n",
      "    - New mask: 50 features, Fitness: -11.8857\n",
      "    - New mask: 54 features, Fitness: -14.8029\n",
      "    - New mask: 53 features, Fitness: -14.1028\n",
      "    - New mask: 50 features, Fitness: -13.3415\n",
      "    - New mask: 54 features, Fitness: -14.2765\n",
      "    - New mask: 54 features, Fitness: -16.7455\n",
      "    - New mask: 56 features, Fitness: -19.4630\n",
      "    - New mask: 52 features, Fitness: -15.6857\n",
      "    - New mask: 50 features, Fitness: -13.9322\n",
      "    - New mask: 51 features, Fitness: -13.2862\n",
      "    - New mask: 57 features, Fitness: -16.8637\n",
      "    - New mask: 56 features, Fitness: -20.2588\n",
      "    - New mask: 50 features, Fitness: -13.8715\n",
      "    - New mask: 49 features, Fitness: -14.6308\n",
      "    - New mask: 47 features, Fitness: -12.9676\n",
      "    - New mask: 56 features, Fitness: -17.7523\n",
      "    - New mask: 53 features, Fitness: -15.4129\n",
      "    - New mask: 57 features, Fitness: -18.0616\n",
      "    - New mask: 53 features, Fitness: -14.9565\n",
      "    - New mask: 54 features, Fitness: -15.4896\n",
      "    - New mask: 55 features, Fitness: -16.5307\n",
      "    - New mask: 50 features, Fitness: -13.8408\n",
      "    - New mask: 51 features, Fitness: -13.9729\n",
      "    - New mask: 55 features, Fitness: -16.5407\n",
      "    - New mask: 55 features, Fitness: -18.1873\n",
      "=== End of Round 3: Vote mask selects 68 features (rho: 0.26)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 4 ================\n",
      "  Adaptive rho for this round: 0.29\n",
      "    - New mask: 50 features, Fitness: -18.9770\n",
      "    - New mask: 54 features, Fitness: -20.9230\n",
      "    - New mask: 55 features, Fitness: -22.6499\n",
      "    - New mask: 57 features, Fitness: -25.4566\n",
      "    - New mask: 54 features, Fitness: -23.6027\n",
      "    - New mask: 54 features, Fitness: -20.8568\n",
      "    - New mask: 55 features, Fitness: -24.7672\n",
      "    - New mask: 58 features, Fitness: -26.7924\n",
      "    - New mask: 55 features, Fitness: -21.9790\n",
      "    - New mask: 52 features, Fitness: -18.1146\n",
      "    - New mask: 50 features, Fitness: -17.3822\n",
      "    - New mask: 54 features, Fitness: -20.7747\n",
      "    - New mask: 56 features, Fitness: -23.8608\n",
      "    - New mask: 54 features, Fitness: -22.5926\n",
      "    - New mask: 57 features, Fitness: -25.4529\n",
      "    - New mask: 54 features, Fitness: -21.1186\n",
      "    - New mask: 49 features, Fitness: -16.2578\n",
      "    - New mask: 52 features, Fitness: -19.4532\n",
      "    - New mask: 51 features, Fitness: -19.5356\n",
      "    - New mask: 52 features, Fitness: -21.2184\n",
      "    - New mask: 54 features, Fitness: -15.9391\n",
      "    - New mask: 51 features, Fitness: -15.6653\n",
      "    - New mask: 55 features, Fitness: -18.5156\n",
      "    - New mask: 55 features, Fitness: -16.3135\n",
      "    - New mask: 53 features, Fitness: -15.2187\n",
      "    - New mask: 56 features, Fitness: -18.6579\n",
      "    - New mask: 58 features, Fitness: -18.5456\n",
      "    - New mask: 57 features, Fitness: -18.5085\n",
      "    - New mask: 54 features, Fitness: -15.4702\n",
      "    - New mask: 58 features, Fitness: -19.6646\n",
      "    - New mask: 60 features, Fitness: -20.7440\n",
      "    - New mask: 59 features, Fitness: -22.1261\n",
      "    - New mask: 54 features, Fitness: -16.4546\n",
      "    - New mask: 54 features, Fitness: -17.2453\n",
      "    - New mask: 49 features, Fitness: -12.2371\n",
      "    - New mask: 57 features, Fitness: -20.5801\n",
      "    - New mask: 53 features, Fitness: -18.5031\n",
      "    - New mask: 51 features, Fitness: -15.0503\n",
      "    - New mask: 48 features, Fitness: -12.4374\n",
      "    - New mask: 52 features, Fitness: -15.3785\n",
      "    - New mask: 49 features, Fitness: -11.4917\n",
      "    - New mask: 57 features, Fitness: -19.3404\n",
      "    - New mask: 54 features, Fitness: -14.6815\n",
      "    - New mask: 55 features, Fitness: -16.8487\n",
      "    - New mask: 50 features, Fitness: -13.2806\n",
      "    - New mask: 55 features, Fitness: -16.7078\n",
      "    - New mask: 54 features, Fitness: -16.1093\n",
      "    - New mask: 51 features, Fitness: -14.2525\n",
      "    - New mask: 49 features, Fitness: -13.4579\n",
      "    - New mask: 55 features, Fitness: -16.2736\n",
      "    - New mask: 58 features, Fitness: -18.1107\n",
      "    - New mask: 51 features, Fitness: -15.4912\n",
      "    - New mask: 50 features, Fitness: -12.0550\n",
      "    - New mask: 52 features, Fitness: -15.8091\n",
      "    - New mask: 51 features, Fitness: -14.0607\n",
      "    - New mask: 55 features, Fitness: -16.8328\n",
      "    - New mask: 51 features, Fitness: -12.6324\n",
      "    - New mask: 52 features, Fitness: -13.1941\n",
      "    - New mask: 55 features, Fitness: -16.4051\n",
      "    - New mask: 50 features, Fitness: -13.5380\n",
      "    - New mask: 55 features, Fitness: -13.6707\n",
      "    - New mask: 52 features, Fitness: -11.2421\n",
      "    - New mask: 52 features, Fitness: -13.2149\n",
      "    - New mask: 54 features, Fitness: -14.7430\n",
      "    - New mask: 54 features, Fitness: -11.6301\n",
      "    - New mask: 49 features, Fitness: -11.9920\n",
      "    - New mask: 51 features, Fitness: -8.6445\n",
      "    - New mask: 49 features, Fitness: -11.5051\n",
      "    - New mask: 51 features, Fitness: -10.3643\n",
      "    - New mask: 56 features, Fitness: -15.4758\n",
      "    - New mask: 52 features, Fitness: -12.2984\n",
      "    - New mask: 57 features, Fitness: -14.9919\n",
      "    - New mask: 59 features, Fitness: -16.9596\n",
      "    - New mask: 51 features, Fitness: -11.6374\n",
      "    - New mask: 51 features, Fitness: -12.8402\n",
      "    - New mask: 50 features, Fitness: -9.7757\n",
      "    - New mask: 48 features, Fitness: -10.2572\n",
      "    - New mask: 53 features, Fitness: -12.0902\n",
      "    - New mask: 51 features, Fitness: -9.9184\n",
      "    - New mask: 51 features, Fitness: -11.0918\n",
      "    - New mask: 53 features, Fitness: -7.3181\n",
      "    - New mask: 50 features, Fitness: -2.8316\n",
      "    - New mask: 48 features, Fitness: -1.7816\n",
      "    - New mask: 50 features, Fitness: -2.7924\n",
      "    - New mask: 56 features, Fitness: -6.7813\n",
      "    - New mask: 49 features, Fitness: -4.7500\n",
      "    - New mask: 55 features, Fitness: -8.0571\n",
      "    - New mask: 54 features, Fitness: -5.9219\n",
      "    - New mask: 58 features, Fitness: -8.2021\n",
      "    - New mask: 54 features, Fitness: -7.6411\n",
      "    - New mask: 51 features, Fitness: -3.4789\n",
      "    - New mask: 57 features, Fitness: -7.9483\n",
      "    - New mask: 53 features, Fitness: -4.7775\n",
      "    - New mask: 48 features, Fitness: -1.0397\n",
      "    - New mask: 57 features, Fitness: -9.1752\n",
      "    - New mask: 52 features, Fitness: -5.0195\n",
      "    - New mask: 54 features, Fitness: -6.0604\n",
      "    - New mask: 55 features, Fitness: -6.6230\n",
      "    - New mask: 51 features, Fitness: -3.0179\n",
      "    - New mask: 52 features, Fitness: -6.8687\n",
      "    - New mask: 59 features, Fitness: -19.9926\n",
      "    - New mask: 56 features, Fitness: -17.0539\n",
      "    - New mask: 52 features, Fitness: -13.8904\n",
      "    - New mask: 54 features, Fitness: -15.9509\n",
      "    - New mask: 53 features, Fitness: -14.1522\n",
      "    - New mask: 52 features, Fitness: -14.7670\n",
      "    - New mask: 54 features, Fitness: -14.9572\n",
      "    - New mask: 56 features, Fitness: -16.8678\n",
      "    - New mask: 53 features, Fitness: -15.2672\n",
      "    - New mask: 60 features, Fitness: -20.1541\n",
      "    - New mask: 52 features, Fitness: -15.1925\n",
      "    - New mask: 61 features, Fitness: -20.5435\n",
      "    - New mask: 53 features, Fitness: -13.2285\n",
      "    - New mask: 51 features, Fitness: -13.6828\n",
      "    - New mask: 57 features, Fitness: -17.8792\n",
      "    - New mask: 56 features, Fitness: -15.3068\n",
      "    - New mask: 55 features, Fitness: -18.6789\n",
      "    - New mask: 53 features, Fitness: -13.6011\n",
      "    - New mask: 54 features, Fitness: -15.9725\n",
      "    - New mask: 58 features, Fitness: -19.1941\n",
      "    - New mask: 60 features, Fitness: -20.3745\n",
      "    - New mask: 51 features, Fitness: -12.2963\n",
      "    - New mask: 57 features, Fitness: -17.6616\n",
      "    - New mask: 54 features, Fitness: -15.0521\n",
      "    - New mask: 56 features, Fitness: -18.0492\n",
      "    - New mask: 59 features, Fitness: -17.9747\n",
      "    - New mask: 59 features, Fitness: -20.6260\n",
      "    - New mask: 55 features, Fitness: -15.8045\n",
      "    - New mask: 59 features, Fitness: -19.0459\n",
      "    - New mask: 57 features, Fitness: -16.3238\n",
      "    - New mask: 57 features, Fitness: -18.5016\n",
      "    - New mask: 53 features, Fitness: -13.8494\n",
      "    - New mask: 53 features, Fitness: -13.0212\n",
      "    - New mask: 53 features, Fitness: -12.9289\n",
      "    - New mask: 55 features, Fitness: -17.6148\n",
      "    - New mask: 55 features, Fitness: -15.9292\n",
      "    - New mask: 54 features, Fitness: -17.3617\n",
      "    - New mask: 62 features, Fitness: -22.4991\n",
      "    - New mask: 55 features, Fitness: -15.6319\n",
      "    - New mask: 60 features, Fitness: -17.9870\n",
      "    - New mask: 54 features, Fitness: -10.4850\n",
      "    - New mask: 58 features, Fitness: -12.4384\n",
      "    - New mask: 57 features, Fitness: -12.4851\n",
      "    - New mask: 62 features, Fitness: -14.2418\n",
      "    - New mask: 56 features, Fitness: -10.6738\n",
      "    - New mask: 53 features, Fitness: -8.2567\n",
      "    - New mask: 60 features, Fitness: -14.5948\n",
      "    - New mask: 60 features, Fitness: -14.1369\n",
      "    - New mask: 56 features, Fitness: -11.2861\n",
      "    - New mask: 55 features, Fitness: -11.3600\n",
      "    - New mask: 55 features, Fitness: -9.6939\n",
      "    - New mask: 55 features, Fitness: -8.9780\n",
      "    - New mask: 60 features, Fitness: -13.4795\n",
      "    - New mask: 53 features, Fitness: -9.7296\n",
      "    - New mask: 47 features, Fitness: -5.2794\n",
      "    - New mask: 57 features, Fitness: -12.2945\n",
      "    - New mask: 57 features, Fitness: -11.5456\n",
      "    - New mask: 56 features, Fitness: -9.5145\n",
      "    - New mask: 55 features, Fitness: -10.9689\n",
      "    - New mask: 56 features, Fitness: -9.4546\n",
      "    - New mask: 53 features, Fitness: -14.7753\n",
      "    - New mask: 57 features, Fitness: -17.3940\n",
      "    - New mask: 55 features, Fitness: -16.1453\n",
      "    - New mask: 53 features, Fitness: -14.1522\n",
      "    - New mask: 55 features, Fitness: -15.8266\n",
      "    - New mask: 54 features, Fitness: -14.3260\n",
      "    - New mask: 51 features, Fitness: -10.5846\n",
      "    - New mask: 59 features, Fitness: -18.0683\n",
      "    - New mask: 55 features, Fitness: -15.9831\n",
      "    - New mask: 53 features, Fitness: -13.2899\n",
      "    - New mask: 55 features, Fitness: -14.7771\n",
      "    - New mask: 56 features, Fitness: -16.2578\n",
      "    - New mask: 53 features, Fitness: -13.3808\n",
      "    - New mask: 54 features, Fitness: -13.3103\n",
      "    - New mask: 58 features, Fitness: -17.5539\n",
      "    - New mask: 54 features, Fitness: -14.8815\n",
      "    - New mask: 53 features, Fitness: -13.8694\n",
      "    - New mask: 53 features, Fitness: -13.0534\n",
      "    - New mask: 58 features, Fitness: -18.0427\n",
      "    - New mask: 52 features, Fitness: -15.2647\n",
      "    - New mask: 54 features, Fitness: -16.0538\n",
      "    - New mask: 54 features, Fitness: -16.3263\n",
      "    - New mask: 60 features, Fitness: -20.3592\n",
      "    - New mask: 52 features, Fitness: -15.8602\n",
      "    - New mask: 55 features, Fitness: -15.3251\n",
      "    - New mask: 53 features, Fitness: -15.9188\n",
      "    - New mask: 58 features, Fitness: -22.0989\n",
      "    - New mask: 56 features, Fitness: -21.0671\n",
      "    - New mask: 55 features, Fitness: -18.5383\n",
      "    - New mask: 52 features, Fitness: -16.7695\n",
      "    - New mask: 58 features, Fitness: -20.1697\n",
      "    - New mask: 56 features, Fitness: -17.2009\n",
      "    - New mask: 55 features, Fitness: -15.6180\n",
      "    - New mask: 47 features, Fitness: -11.3649\n",
      "    - New mask: 59 features, Fitness: -21.3095\n",
      "    - New mask: 56 features, Fitness: -18.2821\n",
      "    - New mask: 53 features, Fitness: -16.3989\n",
      "    - New mask: 52 features, Fitness: -13.5796\n",
      "    - New mask: 56 features, Fitness: -16.9160\n",
      "    - New mask: 50 features, Fitness: -14.2812\n",
      "=== End of Round 4: Vote mask selects 67 features (rho: 0.29)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 5 ================\n",
      "  Adaptive rho for this round: 0.33\n",
      "    - New mask: 54 features, Fitness: -23.4846\n",
      "    - New mask: 57 features, Fitness: -24.1010\n",
      "    - New mask: 55 features, Fitness: -21.8716\n",
      "    - New mask: 58 features, Fitness: -24.8672\n",
      "    - New mask: 57 features, Fitness: -25.2275\n",
      "    - New mask: 55 features, Fitness: -20.8706\n",
      "    - New mask: 60 features, Fitness: -28.7644\n",
      "    - New mask: 57 features, Fitness: -25.3623\n",
      "    - New mask: 53 features, Fitness: -19.9936\n",
      "    - New mask: 54 features, Fitness: -19.8825\n",
      "    - New mask: 50 features, Fitness: -17.7888\n",
      "    - New mask: 57 features, Fitness: -23.9292\n",
      "    - New mask: 55 features, Fitness: -23.0287\n",
      "    - New mask: 54 features, Fitness: -23.5037\n",
      "    - New mask: 57 features, Fitness: -26.5408\n",
      "    - New mask: 53 features, Fitness: -20.7341\n",
      "    - New mask: 56 features, Fitness: -24.6307\n",
      "    - New mask: 59 features, Fitness: -28.4902\n",
      "    - New mask: 50 features, Fitness: -19.3875\n",
      "    - New mask: 56 features, Fitness: -25.9403\n",
      "    - New mask: 55 features, Fitness: -16.8111\n",
      "    - New mask: 57 features, Fitness: -16.7989\n",
      "    - New mask: 63 features, Fitness: -24.5652\n",
      "    - New mask: 57 features, Fitness: -18.3732\n",
      "    - New mask: 53 features, Fitness: -15.9497\n",
      "    - New mask: 54 features, Fitness: -15.9345\n",
      "    - New mask: 57 features, Fitness: -17.9586\n",
      "    - New mask: 56 features, Fitness: -16.9665\n",
      "    - New mask: 58 features, Fitness: -20.1561\n",
      "    - New mask: 56 features, Fitness: -16.6871\n",
      "    - New mask: 56 features, Fitness: -17.2668\n",
      "    - New mask: 59 features, Fitness: -19.9746\n",
      "    - New mask: 55 features, Fitness: -17.0212\n",
      "    - New mask: 55 features, Fitness: -16.1757\n",
      "    - New mask: 54 features, Fitness: -15.9772\n",
      "    - New mask: 58 features, Fitness: -21.3674\n",
      "    - New mask: 59 features, Fitness: -21.4846\n",
      "    - New mask: 58 features, Fitness: -17.7426\n",
      "    - New mask: 53 features, Fitness: -18.0228\n",
      "    - New mask: 60 features, Fitness: -21.9913\n",
      "    - New mask: 51 features, Fitness: -14.1284\n",
      "    - New mask: 54 features, Fitness: -16.1243\n",
      "    - New mask: 48 features, Fitness: -11.4814\n",
      "    - New mask: 61 features, Fitness: -20.4267\n",
      "    - New mask: 58 features, Fitness: -19.8553\n",
      "    - New mask: 52 features, Fitness: -12.6182\n",
      "    - New mask: 57 features, Fitness: -18.7525\n",
      "    - New mask: 58 features, Fitness: -19.7835\n",
      "    - New mask: 53 features, Fitness: -13.4688\n",
      "    - New mask: 53 features, Fitness: -15.1562\n",
      "    - New mask: 57 features, Fitness: -16.7492\n",
      "    - New mask: 53 features, Fitness: -14.9669\n",
      "    - New mask: 58 features, Fitness: -18.3279\n",
      "    - New mask: 54 features, Fitness: -16.4873\n",
      "    - New mask: 59 features, Fitness: -18.9387\n",
      "    - New mask: 58 features, Fitness: -20.0362\n",
      "    - New mask: 55 features, Fitness: -15.0331\n",
      "    - New mask: 55 features, Fitness: -16.7240\n",
      "    - New mask: 56 features, Fitness: -17.1542\n",
      "    - New mask: 55 features, Fitness: -18.3553\n",
      "    - New mask: 60 features, Fitness: -17.8152\n",
      "    - New mask: 51 features, Fitness: -10.0727\n",
      "    - New mask: 54 features, Fitness: -12.9864\n",
      "    - New mask: 58 features, Fitness: -15.9877\n",
      "    - New mask: 55 features, Fitness: -13.3408\n",
      "    - New mask: 55 features, Fitness: -15.3327\n",
      "    - New mask: 57 features, Fitness: -13.3459\n",
      "    - New mask: 54 features, Fitness: -13.2619\n",
      "    - New mask: 55 features, Fitness: -12.5411\n",
      "    - New mask: 57 features, Fitness: -15.3718\n",
      "    - New mask: 50 features, Fitness: -10.0988\n",
      "    - New mask: 57 features, Fitness: -13.9939\n",
      "    - New mask: 57 features, Fitness: -15.1280\n",
      "    - New mask: 57 features, Fitness: -13.4487\n",
      "    - New mask: 50 features, Fitness: -9.6867\n",
      "    - New mask: 53 features, Fitness: -12.4087\n",
      "    - New mask: 52 features, Fitness: -10.5055\n",
      "    - New mask: 58 features, Fitness: -15.4766\n",
      "    - New mask: 57 features, Fitness: -13.0631\n",
      "    - New mask: 55 features, Fitness: -13.6423\n",
      "    - New mask: 56 features, Fitness: -8.0813\n",
      "    - New mask: 56 features, Fitness: -7.3576\n",
      "    - New mask: 54 features, Fitness: -6.6602\n",
      "    - New mask: 53 features, Fitness: -4.2063\n",
      "    - New mask: 58 features, Fitness: -8.0659\n",
      "    - New mask: 54 features, Fitness: -5.0264\n",
      "    - New mask: 58 features, Fitness: -9.1761\n",
      "    - New mask: 49 features, Fitness: -3.8684\n",
      "    - New mask: 59 features, Fitness: -8.2925\n",
      "    - New mask: 53 features, Fitness: -5.2433\n",
      "    - New mask: 51 features, Fitness: -3.1950\n",
      "    - New mask: 58 features, Fitness: -6.8984\n",
      "    - New mask: 55 features, Fitness: -5.5448\n",
      "    - New mask: 53 features, Fitness: -6.6079\n",
      "    - New mask: 59 features, Fitness: -9.3667\n",
      "    - New mask: 53 features, Fitness: -5.5586\n",
      "    - New mask: 61 features, Fitness: -9.3096\n",
      "    - New mask: 55 features, Fitness: -8.3895\n",
      "    - New mask: 57 features, Fitness: -6.3547\n",
      "    - New mask: 54 features, Fitness: -6.5433\n",
      "    - New mask: 56 features, Fitness: -16.3760\n",
      "    - New mask: 60 features, Fitness: -19.0165\n",
      "    - New mask: 55 features, Fitness: -15.6222\n",
      "    - New mask: 56 features, Fitness: -17.4693\n",
      "    - New mask: 59 features, Fitness: -20.1329\n",
      "    - New mask: 58 features, Fitness: -19.3990\n",
      "    - New mask: 56 features, Fitness: -17.8950\n",
      "    - New mask: 58 features, Fitness: -17.4757\n",
      "    - New mask: 55 features, Fitness: -15.0252\n",
      "    - New mask: 60 features, Fitness: -22.0966\n",
      "    - New mask: 55 features, Fitness: -15.9035\n",
      "    - New mask: 59 features, Fitness: -19.8366\n",
      "    - New mask: 57 features, Fitness: -17.4391\n",
      "    - New mask: 57 features, Fitness: -16.5746\n",
      "    - New mask: 56 features, Fitness: -16.2157\n",
      "    - New mask: 55 features, Fitness: -15.1429\n",
      "    - New mask: 58 features, Fitness: -19.9784\n",
      "    - New mask: 55 features, Fitness: -14.4034\n",
      "    - New mask: 60 features, Fitness: -20.4745\n",
      "    - New mask: 60 features, Fitness: -20.2174\n",
      "    - New mask: 54 features, Fitness: -15.8166\n",
      "    - New mask: 52 features, Fitness: -13.0136\n",
      "    - New mask: 57 features, Fitness: -17.9054\n",
      "    - New mask: 56 features, Fitness: -15.7698\n",
      "    - New mask: 61 features, Fitness: -20.8198\n",
      "    - New mask: 56 features, Fitness: -15.4470\n",
      "    - New mask: 58 features, Fitness: -17.7564\n",
      "    - New mask: 57 features, Fitness: -16.6921\n",
      "    - New mask: 60 features, Fitness: -19.5386\n",
      "    - New mask: 57 features, Fitness: -16.1426\n",
      "    - New mask: 56 features, Fitness: -16.7237\n",
      "    - New mask: 54 features, Fitness: -13.2838\n",
      "    - New mask: 55 features, Fitness: -15.2352\n",
      "    - New mask: 56 features, Fitness: -14.7135\n",
      "    - New mask: 53 features, Fitness: -17.3554\n",
      "    - New mask: 58 features, Fitness: -17.5366\n",
      "    - New mask: 57 features, Fitness: -16.4708\n",
      "    - New mask: 58 features, Fitness: -16.8658\n",
      "    - New mask: 55 features, Fitness: -15.8137\n",
      "    - New mask: 58 features, Fitness: -16.9931\n",
      "    - New mask: 52 features, Fitness: -8.9165\n",
      "    - New mask: 58 features, Fitness: -12.2187\n",
      "    - New mask: 57 features, Fitness: -11.2344\n",
      "    - New mask: 55 features, Fitness: -11.2927\n",
      "    - New mask: 57 features, Fitness: -11.8789\n",
      "    - New mask: 55 features, Fitness: -9.6241\n",
      "    - New mask: 60 features, Fitness: -12.9207\n",
      "    - New mask: 53 features, Fitness: -9.1842\n",
      "    - New mask: 58 features, Fitness: -12.5049\n",
      "    - New mask: 53 features, Fitness: -8.9067\n",
      "    - New mask: 55 features, Fitness: -11.5443\n",
      "    - New mask: 62 features, Fitness: -14.3221\n",
      "    - New mask: 53 features, Fitness: -9.7870\n",
      "    - New mask: 53 features, Fitness: -9.0532\n",
      "    - New mask: 53 features, Fitness: -8.7421\n",
      "    - New mask: 62 features, Fitness: -15.8614\n",
      "    - New mask: 55 features, Fitness: -10.1895\n",
      "    - New mask: 56 features, Fitness: -11.4766\n",
      "    - New mask: 52 features, Fitness: -8.4710\n",
      "    - New mask: 57 features, Fitness: -11.4628\n",
      "    - New mask: 55 features, Fitness: -13.5775\n",
      "    - New mask: 55 features, Fitness: -14.9211\n",
      "    - New mask: 56 features, Fitness: -15.2079\n",
      "    - New mask: 55 features, Fitness: -14.1092\n",
      "    - New mask: 56 features, Fitness: -14.9022\n",
      "    - New mask: 59 features, Fitness: -18.2001\n",
      "    - New mask: 53 features, Fitness: -12.3739\n",
      "    - New mask: 57 features, Fitness: -14.8849\n",
      "    - New mask: 56 features, Fitness: -13.6369\n",
      "    - New mask: 57 features, Fitness: -16.1308\n",
      "    - New mask: 55 features, Fitness: -16.6765\n",
      "    - New mask: 52 features, Fitness: -11.9696\n",
      "    - New mask: 60 features, Fitness: -17.3324\n",
      "    - New mask: 53 features, Fitness: -12.5994\n",
      "    - New mask: 59 features, Fitness: -16.1437\n",
      "    - New mask: 59 features, Fitness: -17.0549\n",
      "    - New mask: 55 features, Fitness: -14.2506\n",
      "    - New mask: 56 features, Fitness: -14.7377\n",
      "    - New mask: 56 features, Fitness: -14.8919\n",
      "    - New mask: 56 features, Fitness: -16.1726\n",
      "    - New mask: 56 features, Fitness: -17.8146\n",
      "    - New mask: 51 features, Fitness: -13.3787\n",
      "    - New mask: 62 features, Fitness: -23.2844\n",
      "    - New mask: 55 features, Fitness: -17.3546\n",
      "    - New mask: 59 features, Fitness: -17.8295\n",
      "    - New mask: 56 features, Fitness: -17.4548\n",
      "    - New mask: 57 features, Fitness: -19.9017\n",
      "    - New mask: 54 features, Fitness: -17.6572\n",
      "    - New mask: 53 features, Fitness: -15.7364\n",
      "    - New mask: 52 features, Fitness: -17.4754\n",
      "    - New mask: 61 features, Fitness: -20.9520\n",
      "    - New mask: 52 features, Fitness: -14.8901\n",
      "    - New mask: 59 features, Fitness: -18.7840\n",
      "    - New mask: 51 features, Fitness: -15.4178\n",
      "    - New mask: 58 features, Fitness: -19.7185\n",
      "    - New mask: 60 features, Fitness: -21.8400\n",
      "    - New mask: 53 features, Fitness: -15.1345\n",
      "    - New mask: 57 features, Fitness: -18.9983\n",
      "    - New mask: 53 features, Fitness: -16.1994\n",
      "    - New mask: 54 features, Fitness: -16.0761\n",
      "=== End of Round 5: Vote mask selects 66 features (rho: 0.33)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 6 ================\n",
      "  Adaptive rho for this round: 0.36\n",
      "    - New mask: 53 features, Fitness: -22.7758\n",
      "    - New mask: 53 features, Fitness: -19.8433\n",
      "    - New mask: 51 features, Fitness: -18.8368\n",
      "    - New mask: 59 features, Fitness: -27.0953\n",
      "    - New mask: 56 features, Fitness: -25.7042\n",
      "    - New mask: 58 features, Fitness: -25.7392\n",
      "    - New mask: 57 features, Fitness: -25.5926\n",
      "    - New mask: 52 features, Fitness: -21.1847\n",
      "    - New mask: 52 features, Fitness: -20.1529\n",
      "    - New mask: 51 features, Fitness: -19.0514\n",
      "    - New mask: 57 features, Fitness: -25.4754\n",
      "    - New mask: 59 features, Fitness: -27.1742\n",
      "    - New mask: 56 features, Fitness: -27.7633\n",
      "    - New mask: 56 features, Fitness: -26.4226\n",
      "    - New mask: 55 features, Fitness: -25.0626\n",
      "    - New mask: 45 features, Fitness: -16.1095\n",
      "    - New mask: 56 features, Fitness: -23.3391\n",
      "    - New mask: 60 features, Fitness: -29.9674\n",
      "    - New mask: 56 features, Fitness: -24.8254\n",
      "    - New mask: 56 features, Fitness: -23.1917\n",
      "    - New mask: 58 features, Fitness: -21.1871\n",
      "    - New mask: 57 features, Fitness: -18.1970\n",
      "    - New mask: 54 features, Fitness: -17.8695\n",
      "    - New mask: 58 features, Fitness: -18.3369\n",
      "    - New mask: 59 features, Fitness: -20.7148\n",
      "    - New mask: 59 features, Fitness: -19.8443\n",
      "    - New mask: 60 features, Fitness: -19.8922\n",
      "    - New mask: 56 features, Fitness: -15.8001\n",
      "    - New mask: 62 features, Fitness: -23.0518\n",
      "    - New mask: 57 features, Fitness: -19.9599\n",
      "    - New mask: 59 features, Fitness: -20.7558\n",
      "    - New mask: 61 features, Fitness: -22.8241\n",
      "    - New mask: 57 features, Fitness: -17.0958\n",
      "    - New mask: 60 features, Fitness: -20.8674\n",
      "    - New mask: 61 features, Fitness: -20.9239\n",
      "    - New mask: 53 features, Fitness: -17.9511\n",
      "    - New mask: 60 features, Fitness: -19.9673\n",
      "    - New mask: 56 features, Fitness: -18.6437\n",
      "    - New mask: 56 features, Fitness: -18.2410\n",
      "    - New mask: 55 features, Fitness: -17.5767\n",
      "    - New mask: 52 features, Fitness: -15.2044\n",
      "    - New mask: 53 features, Fitness: -13.4139\n",
      "    - New mask: 53 features, Fitness: -14.5358\n",
      "    - New mask: 53 features, Fitness: -14.0762\n",
      "    - New mask: 55 features, Fitness: -17.8473\n",
      "    - New mask: 54 features, Fitness: -14.2422\n",
      "    - New mask: 56 features, Fitness: -16.3253\n",
      "    - New mask: 58 features, Fitness: -20.2024\n",
      "    - New mask: 54 features, Fitness: -15.0816\n",
      "    - New mask: 53 features, Fitness: -15.3727\n",
      "    - New mask: 54 features, Fitness: -14.7485\n",
      "    - New mask: 58 features, Fitness: -18.7922\n",
      "    - New mask: 58 features, Fitness: -18.0404\n",
      "    - New mask: 58 features, Fitness: -19.3881\n",
      "    - New mask: 60 features, Fitness: -19.7592\n",
      "    - New mask: 58 features, Fitness: -19.3062\n",
      "    - New mask: 60 features, Fitness: -21.2461\n",
      "    - New mask: 55 features, Fitness: -15.0849\n",
      "    - New mask: 61 features, Fitness: -20.9382\n",
      "    - New mask: 53 features, Fitness: -14.8855\n",
      "    - New mask: 56 features, Fitness: -14.4054\n",
      "    - New mask: 58 features, Fitness: -15.0696\n",
      "    - New mask: 55 features, Fitness: -13.1671\n",
      "    - New mask: 61 features, Fitness: -18.7640\n",
      "    - New mask: 56 features, Fitness: -13.8435\n",
      "    - New mask: 58 features, Fitness: -15.4447\n",
      "    - New mask: 56 features, Fitness: -14.0676\n",
      "    - New mask: 57 features, Fitness: -15.0992\n",
      "    - New mask: 57 features, Fitness: -13.7643\n",
      "    - New mask: 54 features, Fitness: -12.4124\n",
      "    - New mask: 60 features, Fitness: -16.8030\n",
      "    - New mask: 55 features, Fitness: -12.2462\n",
      "    - New mask: 59 features, Fitness: -16.7152\n",
      "    - New mask: 62 features, Fitness: -18.3342\n",
      "    - New mask: 57 features, Fitness: -13.5338\n",
      "    - New mask: 55 features, Fitness: -14.7096\n",
      "    - New mask: 56 features, Fitness: -15.3086\n",
      "    - New mask: 60 features, Fitness: -16.0024\n",
      "    - New mask: 57 features, Fitness: -13.7163\n",
      "    - New mask: 55 features, Fitness: -12.4281\n",
      "    - New mask: 55 features, Fitness: -6.6598\n",
      "    - New mask: 59 features, Fitness: -7.5485\n",
      "    - New mask: 58 features, Fitness: -6.9002\n",
      "    - New mask: 56 features, Fitness: -6.9805\n",
      "    - New mask: 55 features, Fitness: -4.9040\n",
      "    - New mask: 53 features, Fitness: -5.1795\n",
      "    - New mask: 58 features, Fitness: -8.2461\n",
      "    - New mask: 56 features, Fitness: -7.2199\n",
      "    - New mask: 58 features, Fitness: -9.0057\n",
      "    - New mask: 54 features, Fitness: -5.7972\n",
      "    - New mask: 56 features, Fitness: -5.6648\n",
      "    - New mask: 57 features, Fitness: -5.6236\n",
      "    - New mask: 56 features, Fitness: -6.1699\n",
      "    - New mask: 57 features, Fitness: -7.8211\n",
      "    - New mask: 59 features, Fitness: -7.6749\n",
      "    - New mask: 56 features, Fitness: -5.4361\n",
      "    - New mask: 63 features, Fitness: -10.4627\n",
      "    - New mask: 56 features, Fitness: -8.9449\n",
      "    - New mask: 60 features, Fitness: -8.9514\n",
      "    - New mask: 55 features, Fitness: -5.2556\n",
      "    - New mask: 53 features, Fitness: -13.0176\n",
      "    - New mask: 57 features, Fitness: -16.5671\n",
      "    - New mask: 56 features, Fitness: -16.6338\n",
      "    - New mask: 59 features, Fitness: -18.6665\n",
      "    - New mask: 60 features, Fitness: -20.1461\n",
      "    - New mask: 62 features, Fitness: -22.8304\n",
      "    - New mask: 62 features, Fitness: -21.8505\n",
      "    - New mask: 60 features, Fitness: -20.0999\n",
      "    - New mask: 58 features, Fitness: -16.9364\n",
      "    - New mask: 63 features, Fitness: -24.9762\n",
      "    - New mask: 59 features, Fitness: -19.9605\n",
      "    - New mask: 61 features, Fitness: -21.5527\n",
      "    - New mask: 60 features, Fitness: -21.1328\n",
      "    - New mask: 60 features, Fitness: -18.8138\n",
      "    - New mask: 61 features, Fitness: -21.2922\n",
      "    - New mask: 56 features, Fitness: -15.6208\n",
      "    - New mask: 57 features, Fitness: -18.6075\n",
      "    - New mask: 56 features, Fitness: -15.0637\n",
      "    - New mask: 57 features, Fitness: -15.8599\n",
      "    - New mask: 63 features, Fitness: -24.2641\n",
      "    - New mask: 53 features, Fitness: -14.5616\n",
      "    - New mask: 55 features, Fitness: -14.8106\n",
      "    - New mask: 57 features, Fitness: -16.1617\n",
      "    - New mask: 56 features, Fitness: -15.0589\n",
      "    - New mask: 60 features, Fitness: -19.7361\n",
      "    - New mask: 60 features, Fitness: -18.3255\n",
      "    - New mask: 55 features, Fitness: -14.5222\n",
      "    - New mask: 55 features, Fitness: -15.9497\n",
      "    - New mask: 60 features, Fitness: -20.2516\n",
      "    - New mask: 56 features, Fitness: -16.9239\n",
      "    - New mask: 59 features, Fitness: -18.1044\n",
      "    - New mask: 56 features, Fitness: -15.1198\n",
      "    - New mask: 60 features, Fitness: -18.0376\n",
      "    - New mask: 56 features, Fitness: -15.5106\n",
      "    - New mask: 58 features, Fitness: -19.7259\n",
      "    - New mask: 61 features, Fitness: -19.1666\n",
      "    - New mask: 57 features, Fitness: -16.1451\n",
      "    - New mask: 53 features, Fitness: -13.1595\n",
      "    - New mask: 56 features, Fitness: -17.3729\n",
      "    - New mask: 55 features, Fitness: -14.7156\n",
      "    - New mask: 56 features, Fitness: -10.5842\n",
      "    - New mask: 60 features, Fitness: -12.6501\n",
      "    - New mask: 57 features, Fitness: -10.6028\n",
      "    - New mask: 60 features, Fitness: -15.1233\n",
      "    - New mask: 58 features, Fitness: -11.1748\n",
      "    - New mask: 59 features, Fitness: -12.9822\n",
      "    - New mask: 60 features, Fitness: -13.0021\n",
      "    - New mask: 57 features, Fitness: -11.3269\n",
      "    - New mask: 54 features, Fitness: -7.4991\n",
      "    - New mask: 55 features, Fitness: -9.8594\n",
      "    - New mask: 57 features, Fitness: -12.4571\n",
      "    - New mask: 58 features, Fitness: -11.3000\n",
      "    - New mask: 59 features, Fitness: -12.8696\n",
      "    - New mask: 62 features, Fitness: -14.5559\n",
      "    - New mask: 55 features, Fitness: -10.3620\n",
      "    - New mask: 63 features, Fitness: -16.9254\n",
      "    - New mask: 57 features, Fitness: -11.0357\n",
      "    - New mask: 51 features, Fitness: -8.4871\n",
      "    - New mask: 58 features, Fitness: -14.3975\n",
      "    - New mask: 57 features, Fitness: -9.6104\n",
      "    - New mask: 56 features, Fitness: -15.0372\n",
      "    - New mask: 59 features, Fitness: -19.1350\n",
      "    - New mask: 52 features, Fitness: -12.9507\n",
      "    - New mask: 54 features, Fitness: -13.2968\n",
      "    - New mask: 59 features, Fitness: -18.6356\n",
      "    - New mask: 57 features, Fitness: -15.6464\n",
      "    - New mask: 58 features, Fitness: -15.7048\n",
      "    - New mask: 56 features, Fitness: -14.8901\n",
      "    - New mask: 56 features, Fitness: -14.8394\n",
      "    - New mask: 57 features, Fitness: -16.7735\n",
      "    - New mask: 55 features, Fitness: -15.4326\n",
      "    - New mask: 56 features, Fitness: -15.7603\n",
      "    - New mask: 63 features, Fitness: -20.8310\n",
      "    - New mask: 53 features, Fitness: -12.7099\n",
      "    - New mask: 54 features, Fitness: -12.4005\n",
      "    - New mask: 56 features, Fitness: -15.6861\n",
      "    - New mask: 55 features, Fitness: -13.4081\n",
      "    - New mask: 54 features, Fitness: -13.4463\n",
      "    - New mask: 50 features, Fitness: -11.1345\n",
      "    - New mask: 57 features, Fitness: -17.2903\n",
      "    - New mask: 57 features, Fitness: -18.9396\n",
      "    - New mask: 55 features, Fitness: -16.7356\n",
      "    - New mask: 61 features, Fitness: -21.7327\n",
      "    - New mask: 57 features, Fitness: -18.5839\n",
      "    - New mask: 59 features, Fitness: -19.8152\n",
      "    - New mask: 55 features, Fitness: -16.3840\n",
      "    - New mask: 55 features, Fitness: -15.8762\n",
      "    - New mask: 54 features, Fitness: -14.9643\n",
      "    - New mask: 53 features, Fitness: -14.2841\n",
      "    - New mask: 54 features, Fitness: -15.4546\n",
      "    - New mask: 58 features, Fitness: -18.1516\n",
      "    - New mask: 57 features, Fitness: -18.4951\n",
      "    - New mask: 58 features, Fitness: -20.8352\n",
      "    - New mask: 55 features, Fitness: -16.3527\n",
      "    - New mask: 57 features, Fitness: -18.3056\n",
      "    - New mask: 62 features, Fitness: -23.0725\n",
      "    - New mask: 57 features, Fitness: -19.0669\n",
      "    - New mask: 57 features, Fitness: -18.2185\n",
      "    - New mask: 54 features, Fitness: -16.8767\n",
      "    - New mask: 59 features, Fitness: -19.1236\n",
      "=== End of Round 6: Vote mask selects 66 features (rho: 0.36)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 7 ================\n",
      "  Adaptive rho for this round: 0.39\n",
      "    - New mask: 56 features, Fitness: -27.3011\n",
      "    - New mask: 55 features, Fitness: -24.0066\n",
      "    - New mask: 52 features, Fitness: -21.7279\n",
      "    - New mask: 51 features, Fitness: -19.5697\n",
      "    - New mask: 55 features, Fitness: -25.2951\n",
      "    - New mask: 54 features, Fitness: -24.6554\n",
      "    - New mask: 57 features, Fitness: -27.9357\n",
      "    - New mask: 54 features, Fitness: -22.7355\n",
      "    - New mask: 57 features, Fitness: -24.4852\n",
      "    - New mask: 54 features, Fitness: -20.7733\n",
      "    - New mask: 53 features, Fitness: -21.8027\n",
      "    - New mask: 58 features, Fitness: -27.9267\n",
      "    - New mask: 57 features, Fitness: -25.6797\n",
      "    - New mask: 58 features, Fitness: -27.9531\n",
      "    - New mask: 51 features, Fitness: -21.1261\n",
      "    - New mask: 50 features, Fitness: -20.4476\n",
      "    - New mask: 55 features, Fitness: -23.5220\n",
      "    - New mask: 59 features, Fitness: -27.2707\n",
      "    - New mask: 55 features, Fitness: -25.3863\n",
      "    - New mask: 58 features, Fitness: -27.1667\n",
      "    - New mask: 61 features, Fitness: -23.6429\n",
      "    - New mask: 58 features, Fitness: -19.3006\n",
      "    - New mask: 55 features, Fitness: -17.1967\n",
      "    - New mask: 59 features, Fitness: -19.2743\n",
      "    - New mask: 59 features, Fitness: -18.9345\n",
      "    - New mask: 62 features, Fitness: -22.3785\n",
      "    - New mask: 59 features, Fitness: -19.5704\n",
      "    - New mask: 60 features, Fitness: -19.5024\n",
      "    - New mask: 60 features, Fitness: -20.5334\n",
      "    - New mask: 56 features, Fitness: -17.0981\n",
      "    - New mask: 63 features, Fitness: -22.3135\n",
      "    - New mask: 57 features, Fitness: -18.0651\n",
      "    - New mask: 57 features, Fitness: -17.1658\n",
      "    - New mask: 59 features, Fitness: -19.4469\n",
      "    - New mask: 58 features, Fitness: -18.9866\n",
      "    - New mask: 52 features, Fitness: -15.7699\n",
      "    - New mask: 58 features, Fitness: -19.3297\n",
      "    - New mask: 56 features, Fitness: -18.9198\n",
      "    - New mask: 56 features, Fitness: -17.6216\n",
      "    - New mask: 60 features, Fitness: -21.1372\n",
      "    - New mask: 56 features, Fitness: -16.0373\n",
      "    - New mask: 56 features, Fitness: -15.0485\n",
      "    - New mask: 58 features, Fitness: -19.6372\n",
      "    - New mask: 60 features, Fitness: -19.3506\n",
      "    - New mask: 62 features, Fitness: -22.2994\n",
      "    - New mask: 55 features, Fitness: -15.0997\n",
      "    - New mask: 61 features, Fitness: -20.9176\n",
      "    - New mask: 58 features, Fitness: -18.0434\n",
      "    - New mask: 54 features, Fitness: -15.4883\n",
      "    - New mask: 56 features, Fitness: -15.9556\n",
      "    - New mask: 58 features, Fitness: -17.6287\n",
      "    - New mask: 56 features, Fitness: -17.7576\n",
      "    - New mask: 62 features, Fitness: -21.6570\n",
      "    - New mask: 57 features, Fitness: -16.7131\n",
      "    - New mask: 58 features, Fitness: -16.3454\n",
      "    - New mask: 60 features, Fitness: -19.8113\n",
      "    - New mask: 61 features, Fitness: -21.5125\n",
      "    - New mask: 53 features, Fitness: -11.8644\n",
      "    - New mask: 60 features, Fitness: -20.1422\n",
      "    - New mask: 56 features, Fitness: -15.3161\n",
      "    - New mask: 58 features, Fitness: -16.1689\n",
      "    - New mask: 58 features, Fitness: -16.0566\n",
      "    - New mask: 54 features, Fitness: -12.7489\n",
      "    - New mask: 60 features, Fitness: -16.5859\n",
      "    - New mask: 54 features, Fitness: -12.0075\n",
      "    - New mask: 61 features, Fitness: -16.4202\n",
      "    - New mask: 58 features, Fitness: -14.5961\n",
      "    - New mask: 58 features, Fitness: -15.3569\n",
      "    - New mask: 62 features, Fitness: -17.8618\n",
      "    - New mask: 48 features, Fitness: -9.9953\n",
      "    - New mask: 60 features, Fitness: -16.5487\n",
      "    - New mask: 60 features, Fitness: -16.5499\n",
      "    - New mask: 60 features, Fitness: -17.4650\n",
      "    - New mask: 57 features, Fitness: -14.3205\n",
      "    - New mask: 59 features, Fitness: -15.8915\n",
      "    - New mask: 63 features, Fitness: -18.2101\n",
      "    - New mask: 59 features, Fitness: -16.5771\n",
      "    - New mask: 56 features, Fitness: -12.8822\n",
      "    - New mask: 60 features, Fitness: -16.5624\n",
      "    - New mask: 56 features, Fitness: -12.6653\n",
      "    - New mask: 54 features, Fitness: -4.6198\n",
      "    - New mask: 56 features, Fitness: -5.5369\n",
      "    - New mask: 61 features, Fitness: -9.2812\n",
      "    - New mask: 60 features, Fitness: -8.6909\n",
      "    - New mask: 60 features, Fitness: -8.7070\n",
      "    - New mask: 57 features, Fitness: -7.2723\n",
      "    - New mask: 57 features, Fitness: -7.1570\n",
      "    - New mask: 61 features, Fitness: -9.6117\n",
      "    - New mask: 62 features, Fitness: -10.6730\n",
      "    - New mask: 59 features, Fitness: -7.8820\n",
      "    - New mask: 58 features, Fitness: -7.3204\n",
      "    - New mask: 58 features, Fitness: -7.7450\n",
      "    - New mask: 54 features, Fitness: -5.2011\n",
      "    - New mask: 60 features, Fitness: -8.7782\n",
      "    - New mask: 59 features, Fitness: -7.6438\n",
      "    - New mask: 58 features, Fitness: -7.9031\n",
      "    - New mask: 61 features, Fitness: -8.8450\n",
      "    - New mask: 56 features, Fitness: -7.9687\n",
      "    - New mask: 59 features, Fitness: -8.1227\n",
      "    - New mask: 57 features, Fitness: -6.9091\n",
      "    - New mask: 54 features, Fitness: -13.5834\n",
      "    - New mask: 61 features, Fitness: -19.9274\n",
      "    - New mask: 59 features, Fitness: -19.8242\n",
      "    - New mask: 59 features, Fitness: -19.1395\n",
      "    - New mask: 57 features, Fitness: -18.3673\n",
      "    - New mask: 60 features, Fitness: -21.0761\n",
      "    - New mask: 61 features, Fitness: -20.8370\n",
      "    - New mask: 60 features, Fitness: -20.5032\n",
      "    - New mask: 56 features, Fitness: -14.7898\n",
      "    - New mask: 56 features, Fitness: -17.4197\n",
      "    - New mask: 59 features, Fitness: -18.9053\n",
      "    - New mask: 60 features, Fitness: -20.9086\n",
      "    - New mask: 60 features, Fitness: -18.9327\n",
      "    - New mask: 54 features, Fitness: -13.7483\n",
      "    - New mask: 59 features, Fitness: -18.8053\n",
      "    - New mask: 55 features, Fitness: -14.5022\n",
      "    - New mask: 58 features, Fitness: -18.8992\n",
      "    - New mask: 60 features, Fitness: -18.8799\n",
      "    - New mask: 56 features, Fitness: -17.8881\n",
      "    - New mask: 63 features, Fitness: -23.7248\n",
      "    - New mask: 57 features, Fitness: -16.8625\n",
      "    - New mask: 55 features, Fitness: -15.4403\n",
      "    - New mask: 55 features, Fitness: -14.7553\n",
      "    - New mask: 57 features, Fitness: -16.4282\n",
      "    - New mask: 56 features, Fitness: -16.1683\n",
      "    - New mask: 59 features, Fitness: -16.9292\n",
      "    - New mask: 58 features, Fitness: -17.1114\n",
      "    - New mask: 54 features, Fitness: -14.2260\n",
      "    - New mask: 58 features, Fitness: -18.4760\n",
      "    - New mask: 52 features, Fitness: -14.1376\n",
      "    - New mask: 59 features, Fitness: -18.0395\n",
      "    - New mask: 55 features, Fitness: -15.3220\n",
      "    - New mask: 58 features, Fitness: -17.1211\n",
      "    - New mask: 61 features, Fitness: -19.5545\n",
      "    - New mask: 58 features, Fitness: -18.0097\n",
      "    - New mask: 60 features, Fitness: -18.8569\n",
      "    - New mask: 55 features, Fitness: -14.9565\n",
      "    - New mask: 54 features, Fitness: -14.1843\n",
      "    - New mask: 60 features, Fitness: -18.2486\n",
      "    - New mask: 57 features, Fitness: -16.4978\n",
      "    - New mask: 57 features, Fitness: -11.0074\n",
      "    - New mask: 57 features, Fitness: -10.6356\n",
      "    - New mask: 58 features, Fitness: -12.4615\n",
      "    - New mask: 55 features, Fitness: -9.7867\n",
      "    - New mask: 53 features, Fitness: -8.0619\n",
      "    - New mask: 56 features, Fitness: -9.8650\n",
      "    - New mask: 61 features, Fitness: -13.4707\n",
      "    - New mask: 55 features, Fitness: -10.0661\n",
      "    - New mask: 55 features, Fitness: -9.0258\n",
      "    - New mask: 58 features, Fitness: -10.6852\n",
      "    - New mask: 59 features, Fitness: -10.7997\n",
      "    - New mask: 62 features, Fitness: -13.8668\n",
      "    - New mask: 59 features, Fitness: -11.6973\n",
      "    - New mask: 59 features, Fitness: -12.1116\n",
      "    - New mask: 57 features, Fitness: -10.7355\n",
      "    - New mask: 57 features, Fitness: -10.7635\n",
      "    - New mask: 59 features, Fitness: -12.0952\n",
      "    - New mask: 55 features, Fitness: -10.1089\n",
      "    - New mask: 62 features, Fitness: -14.2027\n",
      "    - New mask: 61 features, Fitness: -12.8892\n",
      "    - New mask: 58 features, Fitness: -15.5172\n",
      "    - New mask: 59 features, Fitness: -19.1009\n",
      "    - New mask: 57 features, Fitness: -16.0002\n",
      "    - New mask: 55 features, Fitness: -14.0617\n",
      "    - New mask: 51 features, Fitness: -13.8430\n",
      "    - New mask: 59 features, Fitness: -18.0941\n",
      "    - New mask: 59 features, Fitness: -17.4772\n",
      "    - New mask: 52 features, Fitness: -14.4300\n",
      "    - New mask: 57 features, Fitness: -15.8094\n",
      "    - New mask: 57 features, Fitness: -16.3121\n",
      "    - New mask: 58 features, Fitness: -15.7740\n",
      "    - New mask: 58 features, Fitness: -16.5364\n",
      "    - New mask: 59 features, Fitness: -17.6273\n",
      "    - New mask: 56 features, Fitness: -14.5458\n",
      "    - New mask: 54 features, Fitness: -13.6826\n",
      "    - New mask: 56 features, Fitness: -16.7163\n",
      "    - New mask: 52 features, Fitness: -11.8119\n",
      "    - New mask: 53 features, Fitness: -12.7136\n",
      "    - New mask: 53 features, Fitness: -13.5782\n",
      "    - New mask: 60 features, Fitness: -19.1647\n",
      "    - New mask: 58 features, Fitness: -19.4901\n",
      "    - New mask: 56 features, Fitness: -16.8299\n",
      "    - New mask: 59 features, Fitness: -20.1308\n",
      "    - New mask: 60 features, Fitness: -21.3693\n",
      "    - New mask: 59 features, Fitness: -19.4738\n",
      "    - New mask: 59 features, Fitness: -20.3172\n",
      "    - New mask: 53 features, Fitness: -13.6319\n",
      "    - New mask: 59 features, Fitness: -19.6492\n",
      "    - New mask: 58 features, Fitness: -17.6361\n",
      "    - New mask: 59 features, Fitness: -19.6995\n",
      "    - New mask: 60 features, Fitness: -19.6979\n",
      "    - New mask: 58 features, Fitness: -18.6602\n",
      "    - New mask: 56 features, Fitness: -16.9965\n",
      "    - New mask: 57 features, Fitness: -18.1537\n",
      "    - New mask: 55 features, Fitness: -16.5038\n",
      "    - New mask: 56 features, Fitness: -15.6957\n",
      "    - New mask: 56 features, Fitness: -17.2937\n",
      "    - New mask: 58 features, Fitness: -19.6056\n",
      "    - New mask: 51 features, Fitness: -15.9615\n",
      "    - New mask: 58 features, Fitness: -19.1947\n",
      "=== End of Round 7: Vote mask selects 66 features (rho: 0.39)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 8 ================\n",
      "  Adaptive rho for this round: 0.42\n",
      "    - New mask: 56 features, Fitness: -27.1677\n",
      "    - New mask: 52 features, Fitness: -20.4826\n",
      "    - New mask: 52 features, Fitness: -21.1282\n",
      "    - New mask: 58 features, Fitness: -27.4519\n",
      "    - New mask: 53 features, Fitness: -20.3989\n",
      "    - New mask: 51 features, Fitness: -20.2273\n",
      "    - New mask: 58 features, Fitness: -26.9743\n",
      "    - New mask: 58 features, Fitness: -26.0551\n",
      "    - New mask: 57 features, Fitness: -25.5249\n",
      "    - New mask: 54 features, Fitness: -22.4261\n",
      "    - New mask: 57 features, Fitness: -27.4656\n",
      "    - New mask: 60 features, Fitness: -31.2030\n",
      "    - New mask: 57 features, Fitness: -27.3130\n",
      "    - New mask: 58 features, Fitness: -25.7376\n",
      "    - New mask: 54 features, Fitness: -23.5843\n",
      "    - New mask: 57 features, Fitness: -27.4228\n",
      "    - New mask: 55 features, Fitness: -23.5997\n",
      "    - New mask: 57 features, Fitness: -26.5083\n",
      "    - New mask: 59 features, Fitness: -26.8625\n",
      "    - New mask: 55 features, Fitness: -24.0518\n",
      "    - New mask: 55 features, Fitness: -19.8460\n",
      "    - New mask: 61 features, Fitness: -20.6820\n",
      "    - New mask: 49 features, Fitness: -13.0955\n",
      "    - New mask: 55 features, Fitness: -16.7545\n",
      "    - New mask: 61 features, Fitness: -21.1948\n",
      "    - New mask: 58 features, Fitness: -18.9452\n",
      "    - New mask: 53 features, Fitness: -15.3333\n",
      "    - New mask: 59 features, Fitness: -20.2302\n",
      "    - New mask: 56 features, Fitness: -17.8967\n",
      "    - New mask: 58 features, Fitness: -17.9679\n",
      "    - New mask: 59 features, Fitness: -21.0137\n",
      "    - New mask: 54 features, Fitness: -17.7439\n",
      "    - New mask: 55 features, Fitness: -16.3324\n",
      "    - New mask: 57 features, Fitness: -18.7207\n",
      "    - New mask: 55 features, Fitness: -16.3948\n",
      "    - New mask: 51 features, Fitness: -14.3035\n",
      "    - New mask: 59 features, Fitness: -19.8434\n",
      "    - New mask: 59 features, Fitness: -21.2745\n",
      "    - New mask: 58 features, Fitness: -19.8774\n",
      "    - New mask: 54 features, Fitness: -17.3004\n",
      "    - New mask: 55 features, Fitness: -13.6066\n",
      "    - New mask: 56 features, Fitness: -14.9462\n",
      "    - New mask: 59 features, Fitness: -19.6740\n",
      "    - New mask: 65 features, Fitness: -25.0044\n",
      "    - New mask: 59 features, Fitness: -18.4290\n",
      "    - New mask: 57 features, Fitness: -17.0557\n",
      "    - New mask: 59 features, Fitness: -17.8597\n",
      "    - New mask: 58 features, Fitness: -19.2962\n",
      "    - New mask: 59 features, Fitness: -18.0500\n",
      "    - New mask: 55 features, Fitness: -15.9750\n",
      "    - New mask: 58 features, Fitness: -18.1007\n",
      "    - New mask: 63 features, Fitness: -21.9274\n",
      "    - New mask: 60 features, Fitness: -18.8755\n",
      "    - New mask: 58 features, Fitness: -17.6758\n",
      "    - New mask: 58 features, Fitness: -16.6395\n",
      "    - New mask: 60 features, Fitness: -19.9705\n",
      "    - New mask: 58 features, Fitness: -18.0139\n",
      "    - New mask: 55 features, Fitness: -13.8003\n",
      "    - New mask: 53 features, Fitness: -12.2243\n",
      "    - New mask: 61 features, Fitness: -20.5462\n",
      "    - New mask: 52 features, Fitness: -12.2603\n",
      "    - New mask: 55 features, Fitness: -12.3432\n",
      "    - New mask: 55 features, Fitness: -13.8340\n",
      "    - New mask: 56 features, Fitness: -15.7963\n",
      "    - New mask: 55 features, Fitness: -11.9545\n",
      "    - New mask: 57 features, Fitness: -14.1632\n",
      "    - New mask: 58 features, Fitness: -15.5248\n",
      "    - New mask: 57 features, Fitness: -16.3367\n",
      "    - New mask: 55 features, Fitness: -14.0982\n",
      "    - New mask: 55 features, Fitness: -13.5255\n",
      "    - New mask: 57 features, Fitness: -14.4800\n",
      "    - New mask: 56 features, Fitness: -14.4279\n",
      "    - New mask: 56 features, Fitness: -15.3218\n",
      "    - New mask: 55 features, Fitness: -14.4265\n",
      "    - New mask: 55 features, Fitness: -13.4791\n",
      "    - New mask: 56 features, Fitness: -15.2684\n",
      "    - New mask: 58 features, Fitness: -15.6770\n",
      "    - New mask: 54 features, Fitness: -11.4866\n",
      "    - New mask: 55 features, Fitness: -14.9714\n",
      "    - New mask: 54 features, Fitness: -11.9927\n",
      "    - New mask: 56 features, Fitness: -5.8885\n",
      "    - New mask: 56 features, Fitness: -6.1306\n",
      "    - New mask: 60 features, Fitness: -9.4588\n",
      "    - New mask: 60 features, Fitness: -8.6210\n",
      "    - New mask: 61 features, Fitness: -8.7532\n",
      "    - New mask: 58 features, Fitness: -7.1862\n",
      "    - New mask: 55 features, Fitness: -5.9344\n",
      "    - New mask: 60 features, Fitness: -8.9688\n",
      "    - New mask: 56 features, Fitness: -6.6339\n",
      "    - New mask: 65 features, Fitness: -11.9786\n",
      "    - New mask: 61 features, Fitness: -9.7437\n",
      "    - New mask: 60 features, Fitness: -10.3092\n",
      "    - New mask: 58 features, Fitness: -8.3373\n",
      "    - New mask: 57 features, Fitness: -5.5744\n",
      "    - New mask: 58 features, Fitness: -9.2413\n",
      "    - New mask: 59 features, Fitness: -7.0289\n",
      "    - New mask: 62 features, Fitness: -9.1569\n",
      "    - New mask: 51 features, Fitness: -4.6036\n",
      "    - New mask: 58 features, Fitness: -6.8342\n",
      "    - New mask: 56 features, Fitness: -6.1424\n",
      "    - New mask: 58 features, Fitness: -16.5992\n",
      "    - New mask: 61 features, Fitness: -20.4440\n",
      "    - New mask: 61 features, Fitness: -21.3300\n",
      "    - New mask: 57 features, Fitness: -16.8384\n",
      "    - New mask: 57 features, Fitness: -18.6340\n",
      "    - New mask: 58 features, Fitness: -18.1827\n",
      "    - New mask: 61 features, Fitness: -21.2487\n",
      "    - New mask: 59 features, Fitness: -18.6981\n",
      "    - New mask: 51 features, Fitness: -12.9789\n",
      "    - New mask: 56 features, Fitness: -15.6474\n",
      "    - New mask: 56 features, Fitness: -14.7344\n",
      "    - New mask: 59 features, Fitness: -19.9166\n",
      "    - New mask: 57 features, Fitness: -16.1943\n",
      "    - New mask: 54 features, Fitness: -14.4786\n",
      "    - New mask: 59 features, Fitness: -19.4476\n",
      "    - New mask: 56 features, Fitness: -16.9779\n",
      "    - New mask: 61 features, Fitness: -21.0249\n",
      "    - New mask: 58 features, Fitness: -16.5418\n",
      "    - New mask: 54 features, Fitness: -15.6429\n",
      "    - New mask: 59 features, Fitness: -19.3024\n",
      "    - New mask: 57 features, Fitness: -17.8671\n",
      "    - New mask: 56 features, Fitness: -16.9394\n",
      "    - New mask: 58 features, Fitness: -16.4813\n",
      "    - New mask: 58 features, Fitness: -16.5981\n",
      "    - New mask: 58 features, Fitness: -17.0512\n",
      "    - New mask: 53 features, Fitness: -13.3295\n",
      "    - New mask: 60 features, Fitness: -18.9379\n",
      "    - New mask: 55 features, Fitness: -15.9908\n",
      "    - New mask: 57 features, Fitness: -18.1166\n",
      "    - New mask: 52 features, Fitness: -12.1133\n",
      "    - New mask: 58 features, Fitness: -18.3633\n",
      "    - New mask: 57 features, Fitness: -16.0558\n",
      "    - New mask: 59 features, Fitness: -18.7908\n",
      "    - New mask: 60 features, Fitness: -19.5200\n",
      "    - New mask: 59 features, Fitness: -18.2888\n",
      "    - New mask: 53 features, Fitness: -14.9716\n",
      "    - New mask: 58 features, Fitness: -16.2616\n",
      "    - New mask: 53 features, Fitness: -14.3807\n",
      "    - New mask: 58 features, Fitness: -17.5733\n",
      "    - New mask: 58 features, Fitness: -16.4093\n",
      "    - New mask: 56 features, Fitness: -9.5861\n",
      "    - New mask: 59 features, Fitness: -11.5482\n",
      "    - New mask: 58 features, Fitness: -12.0311\n",
      "    - New mask: 62 features, Fitness: -14.9550\n",
      "    - New mask: 57 features, Fitness: -10.2827\n",
      "    - New mask: 55 features, Fitness: -8.9097\n",
      "    - New mask: 63 features, Fitness: -14.9364\n",
      "    - New mask: 50 features, Fitness: -7.2746\n",
      "    - New mask: 61 features, Fitness: -14.0586\n",
      "    - New mask: 58 features, Fitness: -11.3484\n",
      "    - New mask: 58 features, Fitness: -11.0492\n",
      "    - New mask: 62 features, Fitness: -14.4586\n",
      "    - New mask: 56 features, Fitness: -9.0824\n",
      "    - New mask: 58 features, Fitness: -12.0452\n",
      "    - New mask: 55 features, Fitness: -8.7097\n",
      "    - New mask: 59 features, Fitness: -12.6362\n",
      "    - New mask: 57 features, Fitness: -9.5507\n",
      "    - New mask: 57 features, Fitness: -11.3227\n",
      "    - New mask: 59 features, Fitness: -11.3945\n",
      "    - New mask: 60 features, Fitness: -13.2129\n",
      "    - New mask: 54 features, Fitness: -14.2225\n",
      "    - New mask: 55 features, Fitness: -15.6094\n",
      "    - New mask: 59 features, Fitness: -17.0590\n",
      "    - New mask: 50 features, Fitness: -10.8190\n",
      "    - New mask: 49 features, Fitness: -13.1347\n",
      "    - New mask: 56 features, Fitness: -15.4835\n",
      "    - New mask: 54 features, Fitness: -12.7212\n",
      "    - New mask: 54 features, Fitness: -12.6476\n",
      "    - New mask: 52 features, Fitness: -12.9885\n",
      "    - New mask: 57 features, Fitness: -17.8141\n",
      "    - New mask: 56 features, Fitness: -15.9098\n",
      "    - New mask: 56 features, Fitness: -14.7603\n",
      "    - New mask: 59 features, Fitness: -17.8369\n",
      "    - New mask: 58 features, Fitness: -16.0895\n",
      "    - New mask: 57 features, Fitness: -15.0631\n",
      "    - New mask: 61 features, Fitness: -20.1790\n",
      "    - New mask: 49 features, Fitness: -10.2347\n",
      "    - New mask: 55 features, Fitness: -14.7205\n",
      "    - New mask: 56 features, Fitness: -15.4172\n",
      "    - New mask: 60 features, Fitness: -17.8364\n",
      "    - New mask: 59 features, Fitness: -18.8839\n",
      "    - New mask: 55 features, Fitness: -16.4369\n",
      "    - New mask: 59 features, Fitness: -19.6854\n",
      "    - New mask: 59 features, Fitness: -20.2157\n",
      "    - New mask: 58 features, Fitness: -18.2556\n",
      "    - New mask: 59 features, Fitness: -19.6535\n",
      "    - New mask: 59 features, Fitness: -18.8540\n",
      "    - New mask: 57 features, Fitness: -18.3688\n",
      "    - New mask: 58 features, Fitness: -18.3885\n",
      "    - New mask: 60 features, Fitness: -20.3695\n",
      "    - New mask: 55 features, Fitness: -16.5631\n",
      "    - New mask: 55 features, Fitness: -16.0499\n",
      "    - New mask: 57 features, Fitness: -17.3485\n",
      "    - New mask: 58 features, Fitness: -19.5314\n",
      "    - New mask: 57 features, Fitness: -17.7174\n",
      "    - New mask: 57 features, Fitness: -16.7425\n",
      "    - New mask: 56 features, Fitness: -17.6780\n",
      "    - New mask: 56 features, Fitness: -17.0342\n",
      "    - New mask: 55 features, Fitness: -16.6202\n",
      "    - New mask: 58 features, Fitness: -18.5257\n",
      "=== End of Round 8: Vote mask selects 65 features (rho: 0.42)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 9 ================\n",
      "  Adaptive rho for this round: 0.45\n",
      "    - New mask: 62 features, Fitness: -30.5362\n",
      "    - New mask: 60 features, Fitness: -28.8450\n",
      "    - New mask: 53 features, Fitness: -21.5272\n",
      "    - New mask: 57 features, Fitness: -25.5064\n",
      "    - New mask: 53 features, Fitness: -20.8318\n",
      "    - New mask: 61 features, Fitness: -30.4071\n",
      "    - New mask: 60 features, Fitness: -28.0751\n",
      "    - New mask: 57 features, Fitness: -24.1212\n",
      "    - New mask: 60 features, Fitness: -27.8898\n",
      "    - New mask: 56 features, Fitness: -24.9080\n",
      "    - New mask: 57 features, Fitness: -26.1193\n",
      "    - New mask: 64 features, Fitness: -32.8497\n",
      "    - New mask: 57 features, Fitness: -26.4030\n",
      "    - New mask: 53 features, Fitness: -20.9168\n",
      "    - New mask: 62 features, Fitness: -30.6785\n",
      "    - New mask: 59 features, Fitness: -27.8790\n",
      "    - New mask: 56 features, Fitness: -24.4706\n",
      "    - New mask: 54 features, Fitness: -23.4888\n",
      "    - New mask: 58 features, Fitness: -24.4842\n",
      "    - New mask: 61 features, Fitness: -29.2201\n",
      "    - New mask: 57 features, Fitness: -18.8356\n",
      "    - New mask: 56 features, Fitness: -17.6352\n",
      "    - New mask: 52 features, Fitness: -15.3324\n",
      "    - New mask: 56 features, Fitness: -16.2486\n",
      "    - New mask: 54 features, Fitness: -16.6660\n",
      "    - New mask: 58 features, Fitness: -19.7170\n",
      "    - New mask: 56 features, Fitness: -17.5586\n",
      "    - New mask: 53 features, Fitness: -15.7907\n",
      "    - New mask: 55 features, Fitness: -18.2556\n",
      "    - New mask: 58 features, Fitness: -18.4177\n",
      "    - New mask: 54 features, Fitness: -15.6984\n",
      "    - New mask: 55 features, Fitness: -16.6889\n",
      "    - New mask: 56 features, Fitness: -15.8928\n",
      "    - New mask: 52 features, Fitness: -16.1438\n",
      "    - New mask: 55 features, Fitness: -15.9741\n",
      "    - New mask: 55 features, Fitness: -16.9898\n",
      "    - New mask: 56 features, Fitness: -17.1181\n",
      "    - New mask: 54 features, Fitness: -18.0941\n",
      "    - New mask: 53 features, Fitness: -17.3226\n",
      "    - New mask: 53 features, Fitness: -17.2259\n",
      "    - New mask: 54 features, Fitness: -13.7595\n",
      "    - New mask: 59 features, Fitness: -19.4900\n",
      "    - New mask: 59 features, Fitness: -17.8629\n",
      "    - New mask: 58 features, Fitness: -16.8931\n",
      "    - New mask: 61 features, Fitness: -19.8844\n",
      "    - New mask: 58 features, Fitness: -17.5245\n",
      "    - New mask: 59 features, Fitness: -18.2855\n",
      "    - New mask: 62 features, Fitness: -20.2392\n",
      "    - New mask: 62 features, Fitness: -20.9914\n",
      "    - New mask: 56 features, Fitness: -15.7220\n",
      "    - New mask: 55 features, Fitness: -14.6042\n",
      "    - New mask: 61 features, Fitness: -20.0714\n",
      "    - New mask: 57 features, Fitness: -16.8034\n",
      "    - New mask: 58 features, Fitness: -17.7273\n",
      "    - New mask: 61 features, Fitness: -19.8556\n",
      "    - New mask: 59 features, Fitness: -17.7202\n",
      "    - New mask: 59 features, Fitness: -18.8909\n",
      "    - New mask: 58 features, Fitness: -17.5706\n",
      "    - New mask: 56 features, Fitness: -14.7672\n",
      "    - New mask: 60 features, Fitness: -19.1310\n",
      "    - New mask: 58 features, Fitness: -13.5921\n",
      "    - New mask: 58 features, Fitness: -13.5070\n",
      "    - New mask: 58 features, Fitness: -15.3564\n",
      "    - New mask: 54 features, Fitness: -12.8937\n",
      "    - New mask: 56 features, Fitness: -13.4860\n",
      "    - New mask: 58 features, Fitness: -13.7117\n",
      "    - New mask: 53 features, Fitness: -12.8264\n",
      "    - New mask: 57 features, Fitness: -15.2806\n",
      "    - New mask: 60 features, Fitness: -18.0488\n",
      "    - New mask: 59 features, Fitness: -15.9469\n",
      "    - New mask: 60 features, Fitness: -15.5181\n",
      "    - New mask: 52 features, Fitness: -10.3838\n",
      "    - New mask: 61 features, Fitness: -18.3508\n",
      "    - New mask: 58 features, Fitness: -16.7828\n",
      "    - New mask: 56 features, Fitness: -14.5385\n",
      "    - New mask: 53 features, Fitness: -11.8185\n",
      "    - New mask: 62 features, Fitness: -18.6164\n",
      "    - New mask: 59 features, Fitness: -15.5494\n",
      "    - New mask: 57 features, Fitness: -14.6850\n",
      "    - New mask: 57 features, Fitness: -14.3281\n",
      "    - New mask: 54 features, Fitness: -7.3874\n",
      "    - New mask: 53 features, Fitness: -6.0177\n",
      "    - New mask: 60 features, Fitness: -8.7902\n",
      "    - New mask: 58 features, Fitness: -7.3767\n",
      "    - New mask: 58 features, Fitness: -6.8113\n",
      "    - New mask: 55 features, Fitness: -7.0108\n",
      "    - New mask: 61 features, Fitness: -9.4970\n",
      "    - New mask: 60 features, Fitness: -9.8017\n",
      "    - New mask: 55 features, Fitness: -8.4453\n",
      "    - New mask: 56 features, Fitness: -6.9649\n",
      "    - New mask: 55 features, Fitness: -7.1225\n",
      "    - New mask: 58 features, Fitness: -10.3722\n",
      "    - New mask: 60 features, Fitness: -9.3590\n",
      "    - New mask: 56 features, Fitness: -5.2031\n",
      "    - New mask: 55 features, Fitness: -6.0286\n",
      "    - New mask: 54 features, Fitness: -4.1315\n",
      "    - New mask: 60 features, Fitness: -9.3069\n",
      "    - New mask: 48 features, Fitness: -4.1817\n",
      "    - New mask: 57 features, Fitness: -8.4972\n",
      "    - New mask: 55 features, Fitness: -5.8984\n",
      "    - New mask: 58 features, Fitness: -18.5441\n",
      "    - New mask: 60 features, Fitness: -21.2662\n",
      "    - New mask: 56 features, Fitness: -17.0042\n",
      "    - New mask: 56 features, Fitness: -18.2394\n",
      "    - New mask: 54 features, Fitness: -14.9689\n",
      "    - New mask: 58 features, Fitness: -17.0676\n",
      "    - New mask: 56 features, Fitness: -17.8473\n",
      "    - New mask: 59 features, Fitness: -19.1412\n",
      "    - New mask: 53 features, Fitness: -14.3293\n",
      "    - New mask: 59 features, Fitness: -19.7194\n",
      "    - New mask: 57 features, Fitness: -16.2912\n",
      "    - New mask: 55 features, Fitness: -16.0485\n",
      "    - New mask: 57 features, Fitness: -18.3375\n",
      "    - New mask: 54 features, Fitness: -13.4649\n",
      "    - New mask: 61 features, Fitness: -20.8778\n",
      "    - New mask: 59 features, Fitness: -20.3974\n",
      "    - New mask: 60 features, Fitness: -19.7664\n",
      "    - New mask: 56 features, Fitness: -16.3046\n",
      "    - New mask: 55 features, Fitness: -15.9890\n",
      "    - New mask: 57 features, Fitness: -16.9083\n",
      "    - New mask: 56 features, Fitness: -16.6177\n",
      "    - New mask: 56 features, Fitness: -15.0612\n",
      "    - New mask: 58 features, Fitness: -16.5228\n",
      "    - New mask: 56 features, Fitness: -14.2399\n",
      "    - New mask: 58 features, Fitness: -16.5526\n",
      "    - New mask: 57 features, Fitness: -15.9215\n",
      "    - New mask: 56 features, Fitness: -15.7137\n",
      "    - New mask: 55 features, Fitness: -15.7377\n",
      "    - New mask: 57 features, Fitness: -18.0703\n",
      "    - New mask: 55 features, Fitness: -13.5983\n",
      "    - New mask: 59 features, Fitness: -17.0200\n",
      "    - New mask: 59 features, Fitness: -17.2137\n",
      "    - New mask: 59 features, Fitness: -17.9654\n",
      "    - New mask: 61 features, Fitness: -19.7929\n",
      "    - New mask: 58 features, Fitness: -16.8499\n",
      "    - New mask: 56 features, Fitness: -15.7609\n",
      "    - New mask: 61 features, Fitness: -19.1459\n",
      "    - New mask: 58 features, Fitness: -16.2853\n",
      "    - New mask: 55 features, Fitness: -14.4490\n",
      "    - New mask: 62 features, Fitness: -19.4063\n",
      "    - New mask: 60 features, Fitness: -13.2937\n",
      "    - New mask: 57 features, Fitness: -9.9445\n",
      "    - New mask: 57 features, Fitness: -11.4962\n",
      "    - New mask: 59 features, Fitness: -11.8783\n",
      "    - New mask: 55 features, Fitness: -8.5963\n",
      "    - New mask: 57 features, Fitness: -11.1145\n",
      "    - New mask: 58 features, Fitness: -11.4272\n",
      "    - New mask: 51 features, Fitness: -6.8656\n",
      "    - New mask: 56 features, Fitness: -12.0040\n",
      "    - New mask: 55 features, Fitness: -8.3235\n",
      "    - New mask: 57 features, Fitness: -11.6179\n",
      "    - New mask: 58 features, Fitness: -11.1496\n",
      "    - New mask: 52 features, Fitness: -8.3884\n",
      "    - New mask: 55 features, Fitness: -8.9054\n",
      "    - New mask: 58 features, Fitness: -12.3309\n",
      "    - New mask: 61 features, Fitness: -13.2849\n",
      "    - New mask: 56 features, Fitness: -12.2267\n",
      "    - New mask: 57 features, Fitness: -11.3862\n",
      "    - New mask: 57 features, Fitness: -9.3723\n",
      "    - New mask: 57 features, Fitness: -11.0392\n",
      "    - New mask: 58 features, Fitness: -17.5774\n",
      "    - New mask: 55 features, Fitness: -15.3959\n",
      "    - New mask: 54 features, Fitness: -13.5221\n",
      "    - New mask: 50 features, Fitness: -12.2147\n",
      "    - New mask: 52 features, Fitness: -12.4685\n",
      "    - New mask: 55 features, Fitness: -13.5342\n",
      "    - New mask: 56 features, Fitness: -14.0931\n",
      "    - New mask: 53 features, Fitness: -13.2319\n",
      "    - New mask: 57 features, Fitness: -16.6561\n",
      "    - New mask: 53 features, Fitness: -15.0249\n",
      "    - New mask: 56 features, Fitness: -15.9512\n",
      "    - New mask: 55 features, Fitness: -15.6307\n",
      "    - New mask: 58 features, Fitness: -16.2737\n",
      "    - New mask: 53 features, Fitness: -13.9137\n",
      "    - New mask: 51 features, Fitness: -10.5057\n",
      "    - New mask: 57 features, Fitness: -15.7720\n",
      "    - New mask: 50 features, Fitness: -10.5140\n",
      "    - New mask: 53 features, Fitness: -14.2319\n",
      "    - New mask: 55 features, Fitness: -14.4838\n",
      "    - New mask: 59 features, Fitness: -18.5423\n",
      "    - New mask: 61 features, Fitness: -21.4692\n",
      "    - New mask: 57 features, Fitness: -19.3764\n",
      "    - New mask: 59 features, Fitness: -19.2180\n",
      "    - New mask: 61 features, Fitness: -20.8401\n",
      "    - New mask: 59 features, Fitness: -19.4964\n",
      "    - New mask: 59 features, Fitness: -20.8408\n",
      "    - New mask: 59 features, Fitness: -19.0292\n",
      "    - New mask: 58 features, Fitness: -18.3122\n",
      "    - New mask: 53 features, Fitness: -14.2850\n",
      "    - New mask: 61 features, Fitness: -20.8570\n",
      "    - New mask: 57 features, Fitness: -18.0883\n",
      "    - New mask: 60 features, Fitness: -19.4762\n",
      "    - New mask: 58 features, Fitness: -17.2483\n",
      "    - New mask: 58 features, Fitness: -17.4678\n",
      "    - New mask: 56 features, Fitness: -17.6808\n",
      "    - New mask: 57 features, Fitness: -17.4708\n",
      "    - New mask: 55 features, Fitness: -15.9581\n",
      "    - New mask: 54 features, Fitness: -16.1489\n",
      "    - New mask: 63 features, Fitness: -22.2462\n",
      "    - New mask: 60 features, Fitness: -20.5373\n",
      "=== End of Round 9: Vote mask selects 64 features (rho: 0.45)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 10 ================\n",
      "  Adaptive rho for this round: 0.48\n",
      "    - New mask: 60 features, Fitness: -27.8966\n",
      "    - New mask: 59 features, Fitness: -26.7053\n",
      "    - New mask: 59 features, Fitness: -27.4193\n",
      "    - New mask: 57 features, Fitness: -25.6110\n",
      "    - New mask: 57 features, Fitness: -24.9865\n",
      "    - New mask: 59 features, Fitness: -28.9607\n",
      "    - New mask: 56 features, Fitness: -23.6578\n",
      "    - New mask: 56 features, Fitness: -21.6938\n",
      "    - New mask: 59 features, Fitness: -28.0670\n",
      "    - New mask: 54 features, Fitness: -21.7923\n",
      "    - New mask: 56 features, Fitness: -25.6463\n",
      "    - New mask: 59 features, Fitness: -26.7240\n",
      "    - New mask: 57 features, Fitness: -24.9049\n",
      "    - New mask: 53 features, Fitness: -21.9810\n",
      "    - New mask: 59 features, Fitness: -26.8726\n",
      "    - New mask: 59 features, Fitness: -27.7638\n",
      "    - New mask: 55 features, Fitness: -25.2325\n",
      "    - New mask: 55 features, Fitness: -24.3564\n",
      "    - New mask: 55 features, Fitness: -22.1694\n",
      "    - New mask: 57 features, Fitness: -24.4338\n",
      "    - New mask: 55 features, Fitness: -17.3801\n",
      "    - New mask: 56 features, Fitness: -16.5617\n",
      "    - New mask: 52 features, Fitness: -15.2176\n",
      "    - New mask: 58 features, Fitness: -19.2072\n",
      "    - New mask: 51 features, Fitness: -14.0235\n",
      "    - New mask: 57 features, Fitness: -17.6834\n",
      "    - New mask: 59 features, Fitness: -20.3614\n",
      "    - New mask: 55 features, Fitness: -17.4354\n",
      "    - New mask: 58 features, Fitness: -19.7982\n",
      "    - New mask: 59 features, Fitness: -19.1702\n",
      "    - New mask: 56 features, Fitness: -17.4236\n",
      "    - New mask: 55 features, Fitness: -16.0994\n",
      "    - New mask: 55 features, Fitness: -16.7925\n",
      "    - New mask: 54 features, Fitness: -17.5202\n",
      "    - New mask: 59 features, Fitness: -19.5592\n",
      "    - New mask: 58 features, Fitness: -20.1304\n",
      "    - New mask: 57 features, Fitness: -17.8551\n",
      "    - New mask: 53 features, Fitness: -16.4959\n",
      "    - New mask: 53 features, Fitness: -17.1664\n",
      "    - New mask: 55 features, Fitness: -20.1022\n",
      "    - New mask: 54 features, Fitness: -14.3308\n",
      "    - New mask: 60 features, Fitness: -20.2320\n",
      "    - New mask: 60 features, Fitness: -19.7253\n",
      "    - New mask: 53 features, Fitness: -13.3536\n",
      "    - New mask: 56 features, Fitness: -15.4585\n",
      "    - New mask: 56 features, Fitness: -16.0574\n",
      "    - New mask: 58 features, Fitness: -18.5295\n",
      "    - New mask: 60 features, Fitness: -17.7748\n",
      "    - New mask: 57 features, Fitness: -18.6243\n",
      "    - New mask: 57 features, Fitness: -16.0632\n",
      "    - New mask: 53 features, Fitness: -13.4325\n",
      "    - New mask: 59 features, Fitness: -18.2378\n",
      "    - New mask: 54 features, Fitness: -15.8325\n",
      "    - New mask: 55 features, Fitness: -14.0443\n",
      "    - New mask: 63 features, Fitness: -23.1396\n",
      "    - New mask: 56 features, Fitness: -14.4572\n",
      "    - New mask: 63 features, Fitness: -23.7275\n",
      "    - New mask: 56 features, Fitness: -14.1562\n",
      "    - New mask: 58 features, Fitness: -17.3006\n",
      "    - New mask: 55 features, Fitness: -15.1241\n",
      "    - New mask: 54 features, Fitness: -11.5710\n",
      "    - New mask: 57 features, Fitness: -12.5726\n",
      "    - New mask: 55 features, Fitness: -12.2195\n",
      "    - New mask: 51 features, Fitness: -10.4949\n",
      "    - New mask: 58 features, Fitness: -16.8881\n",
      "    - New mask: 57 features, Fitness: -15.0032\n",
      "    - New mask: 57 features, Fitness: -15.1357\n",
      "    - New mask: 57 features, Fitness: -14.7078\n",
      "    - New mask: 57 features, Fitness: -14.2748\n",
      "    - New mask: 57 features, Fitness: -14.0154\n",
      "    - New mask: 58 features, Fitness: -14.6708\n",
      "    - New mask: 56 features, Fitness: -13.7468\n",
      "    - New mask: 54 features, Fitness: -12.8857\n",
      "    - New mask: 53 features, Fitness: -12.1656\n",
      "    - New mask: 60 features, Fitness: -17.6882\n",
      "    - New mask: 55 features, Fitness: -14.3131\n",
      "    - New mask: 61 features, Fitness: -16.6726\n",
      "    - New mask: 55 features, Fitness: -14.1812\n",
      "    - New mask: 57 features, Fitness: -15.5578\n",
      "    - New mask: 56 features, Fitness: -12.3702\n",
      "    - New mask: 60 features, Fitness: -8.3639\n",
      "    - New mask: 55 features, Fitness: -5.6432\n",
      "    - New mask: 62 features, Fitness: -8.9420\n",
      "    - New mask: 58 features, Fitness: -6.9481\n",
      "    - New mask: 55 features, Fitness: -5.9144\n",
      "    - New mask: 57 features, Fitness: -8.7513\n",
      "    - New mask: 55 features, Fitness: -4.7867\n",
      "    - New mask: 55 features, Fitness: -6.2155\n",
      "    - New mask: 54 features, Fitness: -5.9240\n",
      "    - New mask: 54 features, Fitness: -5.1314\n",
      "    - New mask: 56 features, Fitness: -7.2858\n",
      "    - New mask: 57 features, Fitness: -9.5757\n",
      "    - New mask: 54 features, Fitness: -3.9464\n",
      "    - New mask: 60 features, Fitness: -7.4142\n",
      "    - New mask: 51 features, Fitness: -1.7777\n",
      "    - New mask: 54 features, Fitness: -4.7379\n",
      "    - New mask: 57 features, Fitness: -6.5751\n",
      "    - New mask: 52 features, Fitness: -4.8864\n",
      "    - New mask: 58 features, Fitness: -8.2588\n",
      "    - New mask: 56 features, Fitness: -4.7926\n",
      "    - New mask: 54 features, Fitness: -14.5842\n",
      "    - New mask: 57 features, Fitness: -15.8397\n",
      "    - New mask: 58 features, Fitness: -19.1384\n",
      "    - New mask: 60 features, Fitness: -19.9914\n",
      "    - New mask: 57 features, Fitness: -18.2846\n",
      "    - New mask: 57 features, Fitness: -16.7042\n",
      "    - New mask: 57 features, Fitness: -17.5807\n",
      "    - New mask: 58 features, Fitness: -17.2936\n",
      "    - New mask: 57 features, Fitness: -16.3584\n",
      "    - New mask: 58 features, Fitness: -17.4691\n",
      "    - New mask: 54 features, Fitness: -13.8878\n",
      "    - New mask: 55 features, Fitness: -14.2598\n",
      "    - New mask: 57 features, Fitness: -16.3145\n",
      "    - New mask: 54 features, Fitness: -14.1898\n",
      "    - New mask: 60 features, Fitness: -19.6066\n",
      "    - New mask: 60 features, Fitness: -19.7370\n",
      "    - New mask: 55 features, Fitness: -16.1310\n",
      "    - New mask: 57 features, Fitness: -17.1578\n",
      "    - New mask: 54 features, Fitness: -16.6106\n",
      "    - New mask: 52 features, Fitness: -13.9690\n",
      "    - New mask: 60 features, Fitness: -18.2522\n",
      "    - New mask: 54 features, Fitness: -12.8426\n",
      "    - New mask: 60 features, Fitness: -17.5452\n",
      "    - New mask: 59 features, Fitness: -16.8258\n",
      "    - New mask: 56 features, Fitness: -15.4618\n",
      "    - New mask: 60 features, Fitness: -19.4257\n",
      "    - New mask: 57 features, Fitness: -15.7614\n",
      "    - New mask: 57 features, Fitness: -15.6313\n",
      "    - New mask: 59 features, Fitness: -19.8237\n",
      "    - New mask: 55 features, Fitness: -14.6013\n",
      "    - New mask: 54 features, Fitness: -13.0091\n",
      "    - New mask: 59 features, Fitness: -17.3899\n",
      "    - New mask: 59 features, Fitness: -17.8783\n",
      "    - New mask: 59 features, Fitness: -17.2718\n",
      "    - New mask: 59 features, Fitness: -17.0797\n",
      "    - New mask: 57 features, Fitness: -15.8817\n",
      "    - New mask: 59 features, Fitness: -16.9929\n",
      "    - New mask: 57 features, Fitness: -15.3527\n",
      "    - New mask: 54 features, Fitness: -14.1150\n",
      "    - New mask: 60 features, Fitness: -18.7623\n",
      "    - New mask: 57 features, Fitness: -10.2085\n",
      "    - New mask: 56 features, Fitness: -10.6105\n",
      "    - New mask: 54 features, Fitness: -9.2508\n",
      "    - New mask: 56 features, Fitness: -9.7839\n",
      "    - New mask: 54 features, Fitness: -7.9026\n",
      "    - New mask: 61 features, Fitness: -13.0492\n",
      "    - New mask: 59 features, Fitness: -11.6160\n",
      "    - New mask: 51 features, Fitness: -6.4151\n",
      "    - New mask: 59 features, Fitness: -12.3890\n",
      "    - New mask: 53 features, Fitness: -8.6107\n",
      "    - New mask: 57 features, Fitness: -11.0903\n",
      "    - New mask: 59 features, Fitness: -11.7154\n",
      "    - New mask: 53 features, Fitness: -8.5379\n",
      "    - New mask: 56 features, Fitness: -9.3973\n",
      "    - New mask: 56 features, Fitness: -10.4996\n",
      "    - New mask: 61 features, Fitness: -12.0240\n",
      "    - New mask: 57 features, Fitness: -11.5903\n",
      "    - New mask: 57 features, Fitness: -12.2581\n",
      "    - New mask: 59 features, Fitness: -11.7273\n",
      "    - New mask: 54 features, Fitness: -10.7175\n",
      "    - New mask: 60 features, Fitness: -17.4052\n",
      "    - New mask: 54 features, Fitness: -13.8388\n",
      "    - New mask: 54 features, Fitness: -12.7720\n",
      "    - New mask: 51 features, Fitness: -11.9958\n",
      "    - New mask: 59 features, Fitness: -17.3097\n",
      "    - New mask: 56 features, Fitness: -13.4851\n",
      "    - New mask: 59 features, Fitness: -17.5159\n",
      "    - New mask: 58 features, Fitness: -15.8760\n",
      "    - New mask: 55 features, Fitness: -14.3380\n",
      "    - New mask: 55 features, Fitness: -13.9568\n",
      "    - New mask: 56 features, Fitness: -14.7659\n",
      "    - New mask: 55 features, Fitness: -14.5233\n",
      "    - New mask: 59 features, Fitness: -18.8565\n",
      "    - New mask: 54 features, Fitness: -13.0336\n",
      "    - New mask: 57 features, Fitness: -14.4326\n",
      "    - New mask: 52 features, Fitness: -13.2694\n",
      "    - New mask: 54 features, Fitness: -13.3947\n",
      "    - New mask: 54 features, Fitness: -12.6210\n",
      "    - New mask: 60 features, Fitness: -18.6144\n",
      "    - New mask: 55 features, Fitness: -15.9471\n",
      "    - New mask: 55 features, Fitness: -16.5245\n",
      "    - New mask: 56 features, Fitness: -18.8729\n",
      "    - New mask: 56 features, Fitness: -17.0911\n",
      "    - New mask: 55 features, Fitness: -16.4125\n",
      "    - New mask: 57 features, Fitness: -17.4889\n",
      "    - New mask: 56 features, Fitness: -16.3657\n",
      "    - New mask: 57 features, Fitness: -18.7218\n",
      "    - New mask: 59 features, Fitness: -18.7474\n",
      "    - New mask: 57 features, Fitness: -17.7179\n",
      "    - New mask: 61 features, Fitness: -21.1471\n",
      "    - New mask: 59 features, Fitness: -19.0613\n",
      "    - New mask: 57 features, Fitness: -17.4052\n",
      "    - New mask: 55 features, Fitness: -15.3784\n",
      "    - New mask: 58 features, Fitness: -17.7396\n",
      "    - New mask: 59 features, Fitness: -19.2812\n",
      "    - New mask: 56 features, Fitness: -17.4538\n",
      "    - New mask: 59 features, Fitness: -18.8572\n",
      "    - New mask: 57 features, Fitness: -17.3104\n",
      "    - New mask: 59 features, Fitness: -19.2690\n",
      "    - New mask: 59 features, Fitness: -19.8348\n",
      "=== End of Round 10: Vote mask selects 64 features (rho: 0.48)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 11 ================\n",
      "  Adaptive rho for this round: 0.52\n",
      "    - New mask: 54 features, Fitness: -20.8115\n",
      "    - New mask: 59 features, Fitness: -26.3367\n",
      "    - New mask: 60 features, Fitness: -27.2865\n",
      "    - New mask: 57 features, Fitness: -25.2911\n",
      "    - New mask: 57 features, Fitness: -24.3666\n",
      "    - New mask: 62 features, Fitness: -29.8923\n",
      "    - New mask: 58 features, Fitness: -24.5010\n",
      "    - New mask: 56 features, Fitness: -22.3813\n",
      "    - New mask: 58 features, Fitness: -24.1394\n",
      "    - New mask: 62 features, Fitness: -29.7516\n",
      "    - New mask: 61 features, Fitness: -29.2058\n",
      "    - New mask: 61 features, Fitness: -28.5623\n",
      "    - New mask: 54 features, Fitness: -19.7689\n",
      "    - New mask: 56 features, Fitness: -22.7332\n",
      "    - New mask: 56 features, Fitness: -22.5017\n",
      "    - New mask: 61 features, Fitness: -28.5078\n",
      "    - New mask: 59 features, Fitness: -27.1044\n",
      "    - New mask: 54 features, Fitness: -22.5968\n",
      "    - New mask: 58 features, Fitness: -24.2460\n",
      "    - New mask: 59 features, Fitness: -26.4971\n",
      "    - New mask: 55 features, Fitness: -16.8705\n",
      "    - New mask: 58 features, Fitness: -18.3691\n",
      "    - New mask: 52 features, Fitness: -14.3654\n",
      "    - New mask: 60 features, Fitness: -20.4557\n",
      "    - New mask: 52 features, Fitness: -14.9825\n",
      "    - New mask: 58 features, Fitness: -19.6776\n",
      "    - New mask: 51 features, Fitness: -13.5937\n",
      "    - New mask: 54 features, Fitness: -16.1774\n",
      "    - New mask: 59 features, Fitness: -19.0121\n",
      "    - New mask: 56 features, Fitness: -17.8443\n",
      "    - New mask: 54 features, Fitness: -14.9678\n",
      "    - New mask: 57 features, Fitness: -17.7350\n",
      "    - New mask: 58 features, Fitness: -19.2845\n",
      "    - New mask: 55 features, Fitness: -16.1792\n",
      "    - New mask: 56 features, Fitness: -17.8733\n",
      "    - New mask: 59 features, Fitness: -20.4301\n",
      "    - New mask: 58 features, Fitness: -19.2032\n",
      "    - New mask: 56 features, Fitness: -17.3496\n",
      "    - New mask: 53 features, Fitness: -15.4602\n",
      "    - New mask: 52 features, Fitness: -15.4558\n",
      "    - New mask: 57 features, Fitness: -16.9047\n",
      "    - New mask: 59 features, Fitness: -17.5023\n",
      "    - New mask: 59 features, Fitness: -18.6428\n",
      "    - New mask: 52 features, Fitness: -12.3293\n",
      "    - New mask: 57 features, Fitness: -17.2338\n",
      "    - New mask: 56 features, Fitness: -14.7381\n",
      "    - New mask: 60 features, Fitness: -21.1336\n",
      "    - New mask: 56 features, Fitness: -16.0830\n",
      "    - New mask: 54 features, Fitness: -16.3100\n",
      "    - New mask: 51 features, Fitness: -11.6933\n",
      "    - New mask: 54 features, Fitness: -15.2575\n",
      "    - New mask: 57 features, Fitness: -17.1451\n",
      "    - New mask: 58 features, Fitness: -18.7636\n",
      "    - New mask: 56 features, Fitness: -15.7300\n",
      "    - New mask: 58 features, Fitness: -17.3663\n",
      "    - New mask: 55 features, Fitness: -14.9795\n",
      "    - New mask: 58 features, Fitness: -18.9956\n",
      "    - New mask: 55 features, Fitness: -13.7437\n",
      "    - New mask: 59 features, Fitness: -18.4898\n",
      "    - New mask: 58 features, Fitness: -18.0136\n",
      "    - New mask: 59 features, Fitness: -16.3056\n",
      "    - New mask: 58 features, Fitness: -14.0648\n",
      "    - New mask: 56 features, Fitness: -14.5382\n",
      "    - New mask: 53 features, Fitness: -11.8541\n",
      "    - New mask: 58 features, Fitness: -15.8632\n",
      "    - New mask: 53 features, Fitness: -12.0943\n",
      "    - New mask: 59 features, Fitness: -16.9077\n",
      "    - New mask: 56 features, Fitness: -13.7423\n",
      "    - New mask: 55 features, Fitness: -13.2444\n",
      "    - New mask: 57 features, Fitness: -13.9387\n",
      "    - New mask: 58 features, Fitness: -14.3321\n",
      "    - New mask: 56 features, Fitness: -13.4482\n",
      "    - New mask: 57 features, Fitness: -14.1796\n",
      "    - New mask: 55 features, Fitness: -14.8828\n",
      "    - New mask: 52 features, Fitness: -10.4811\n",
      "    - New mask: 49 features, Fitness: -9.0168\n",
      "    - New mask: 55 features, Fitness: -12.1808\n",
      "    - New mask: 56 features, Fitness: -12.3064\n",
      "    - New mask: 56 features, Fitness: -12.5425\n",
      "    - New mask: 54 features, Fitness: -11.9509\n",
      "    - New mask: 57 features, Fitness: -5.4181\n",
      "    - New mask: 55 features, Fitness: -3.3452\n",
      "    - New mask: 58 features, Fitness: -6.8305\n",
      "    - New mask: 59 features, Fitness: -6.5149\n",
      "    - New mask: 51 features, Fitness: -3.4530\n",
      "    - New mask: 55 features, Fitness: -5.1743\n",
      "    - New mask: 58 features, Fitness: -8.2289\n",
      "    - New mask: 56 features, Fitness: -5.1098\n",
      "    - New mask: 51 features, Fitness: -4.9202\n",
      "    - New mask: 55 features, Fitness: -5.5071\n",
      "    - New mask: 54 features, Fitness: -6.9732\n",
      "    - New mask: 57 features, Fitness: -6.3959\n",
      "    - New mask: 52 features, Fitness: -2.6779\n",
      "    - New mask: 63 features, Fitness: -10.3643\n",
      "    - New mask: 51 features, Fitness: -3.4568\n",
      "    - New mask: 52 features, Fitness: -4.7683\n",
      "    - New mask: 51 features, Fitness: -3.2878\n",
      "    - New mask: 54 features, Fitness: -3.3721\n",
      "    - New mask: 59 features, Fitness: -8.0235\n",
      "    - New mask: 56 features, Fitness: -4.1371\n",
      "    - New mask: 57 features, Fitness: -15.7860\n",
      "    - New mask: 54 features, Fitness: -14.3249\n",
      "    - New mask: 58 features, Fitness: -18.7902\n",
      "    - New mask: 58 features, Fitness: -17.8529\n",
      "    - New mask: 58 features, Fitness: -18.3014\n",
      "    - New mask: 58 features, Fitness: -16.9613\n",
      "    - New mask: 57 features, Fitness: -16.9529\n",
      "    - New mask: 56 features, Fitness: -15.8257\n",
      "    - New mask: 61 features, Fitness: -21.5936\n",
      "    - New mask: 58 features, Fitness: -16.9567\n",
      "    - New mask: 55 features, Fitness: -14.9509\n",
      "    - New mask: 57 features, Fitness: -16.5112\n",
      "    - New mask: 56 features, Fitness: -16.8659\n",
      "    - New mask: 57 features, Fitness: -18.7213\n",
      "    - New mask: 59 features, Fitness: -18.6446\n",
      "    - New mask: 57 features, Fitness: -15.7513\n",
      "    - New mask: 53 features, Fitness: -13.1016\n",
      "    - New mask: 54 features, Fitness: -14.1925\n",
      "    - New mask: 58 features, Fitness: -17.8145\n",
      "    - New mask: 55 features, Fitness: -15.1375\n",
      "    - New mask: 60 features, Fitness: -18.0917\n",
      "    - New mask: 51 features, Fitness: -11.4383\n",
      "    - New mask: 57 features, Fitness: -15.5711\n",
      "    - New mask: 60 features, Fitness: -19.2956\n",
      "    - New mask: 59 features, Fitness: -17.8578\n",
      "    - New mask: 57 features, Fitness: -15.7551\n",
      "    - New mask: 56 features, Fitness: -15.0910\n",
      "    - New mask: 55 features, Fitness: -14.3924\n",
      "    - New mask: 59 features, Fitness: -16.7847\n",
      "    - New mask: 53 features, Fitness: -12.7492\n",
      "    - New mask: 57 features, Fitness: -15.3200\n",
      "    - New mask: 58 features, Fitness: -16.0816\n",
      "    - New mask: 58 features, Fitness: -17.6696\n",
      "    - New mask: 58 features, Fitness: -16.3288\n",
      "    - New mask: 57 features, Fitness: -15.3526\n",
      "    - New mask: 62 features, Fitness: -19.7556\n",
      "    - New mask: 57 features, Fitness: -15.2822\n",
      "    - New mask: 56 features, Fitness: -15.4082\n",
      "    - New mask: 55 features, Fitness: -14.2720\n",
      "    - New mask: 56 features, Fitness: -15.2526\n",
      "    - New mask: 56 features, Fitness: -9.5753\n",
      "    - New mask: 51 features, Fitness: -7.1554\n",
      "    - New mask: 55 features, Fitness: -8.1450\n",
      "    - New mask: 53 features, Fitness: -7.4512\n",
      "    - New mask: 56 features, Fitness: -9.4766\n",
      "    - New mask: 57 features, Fitness: -9.9029\n",
      "    - New mask: 57 features, Fitness: -10.4373\n",
      "    - New mask: 56 features, Fitness: -9.3015\n",
      "    - New mask: 57 features, Fitness: -11.6790\n",
      "    - New mask: 54 features, Fitness: -9.1939\n",
      "    - New mask: 58 features, Fitness: -10.1892\n",
      "    - New mask: 60 features, Fitness: -12.9581\n",
      "    - New mask: 53 features, Fitness: -7.7274\n",
      "    - New mask: 58 features, Fitness: -10.7446\n",
      "    - New mask: 61 features, Fitness: -13.4848\n",
      "    - New mask: 57 features, Fitness: -10.0648\n",
      "    - New mask: 56 features, Fitness: -9.0684\n",
      "    - New mask: 58 features, Fitness: -11.9618\n",
      "    - New mask: 56 features, Fitness: -9.7603\n",
      "    - New mask: 49 features, Fitness: -7.8674\n",
      "    - New mask: 58 features, Fitness: -16.9147\n",
      "    - New mask: 55 features, Fitness: -15.0070\n",
      "    - New mask: 54 features, Fitness: -14.6698\n",
      "    - New mask: 55 features, Fitness: -15.2879\n",
      "    - New mask: 59 features, Fitness: -17.8630\n",
      "    - New mask: 55 features, Fitness: -14.8967\n",
      "    - New mask: 62 features, Fitness: -21.5189\n",
      "    - New mask: 57 features, Fitness: -16.0015\n",
      "    - New mask: 57 features, Fitness: -15.8342\n",
      "    - New mask: 60 features, Fitness: -18.7575\n",
      "    - New mask: 56 features, Fitness: -14.8425\n",
      "    - New mask: 57 features, Fitness: -16.7376\n",
      "    - New mask: 58 features, Fitness: -18.4314\n",
      "    - New mask: 54 features, Fitness: -14.0687\n",
      "    - New mask: 58 features, Fitness: -16.0187\n",
      "    - New mask: 61 features, Fitness: -19.8260\n",
      "    - New mask: 59 features, Fitness: -18.0400\n",
      "    - New mask: 55 features, Fitness: -13.9222\n",
      "    - New mask: 58 features, Fitness: -17.9974\n",
      "    - New mask: 55 features, Fitness: -14.5712\n",
      "    - New mask: 58 features, Fitness: -17.2938\n",
      "    - New mask: 57 features, Fitness: -17.2908\n",
      "    - New mask: 60 features, Fitness: -19.4385\n",
      "    - New mask: 54 features, Fitness: -15.7603\n",
      "    - New mask: 58 features, Fitness: -17.8158\n",
      "    - New mask: 59 features, Fitness: -19.3368\n",
      "    - New mask: 60 features, Fitness: -20.1795\n",
      "    - New mask: 59 features, Fitness: -18.4435\n",
      "    - New mask: 57 features, Fitness: -18.1170\n",
      "    - New mask: 60 features, Fitness: -19.6620\n",
      "    - New mask: 56 features, Fitness: -16.4591\n",
      "    - New mask: 55 features, Fitness: -14.6807\n",
      "    - New mask: 58 features, Fitness: -17.9714\n",
      "    - New mask: 59 features, Fitness: -19.6689\n",
      "    - New mask: 58 features, Fitness: -17.5730\n",
      "    - New mask: 56 features, Fitness: -18.0372\n",
      "    - New mask: 56 features, Fitness: -17.1860\n",
      "    - New mask: 57 features, Fitness: -16.6252\n",
      "    - New mask: 56 features, Fitness: -16.9528\n",
      "    - New mask: 59 features, Fitness: -20.3215\n",
      "=== End of Round 11: Vote mask selects 60 features (rho: 0.52)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 17, 19, 21, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 63, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 12 ================\n",
      "  Adaptive rho for this round: 0.55\n",
      "    - New mask: 52 features, Fitness: -18.6744\n",
      "    - New mask: 58 features, Fitness: -24.7853\n",
      "    - New mask: 58 features, Fitness: -25.2935\n",
      "    - New mask: 57 features, Fitness: -24.4492\n",
      "    - New mask: 58 features, Fitness: -26.2568\n",
      "    - New mask: 58 features, Fitness: -23.9014\n",
      "    - New mask: 54 features, Fitness: -21.3029\n",
      "    - New mask: 57 features, Fitness: -24.3709\n",
      "    - New mask: 53 features, Fitness: -20.3142\n",
      "    - New mask: 54 features, Fitness: -19.4931\n",
      "    - New mask: 59 features, Fitness: -28.0474\n",
      "    - New mask: 59 features, Fitness: -26.6921\n",
      "    - New mask: 56 features, Fitness: -22.4859\n",
      "    - New mask: 59 features, Fitness: -25.4915\n",
      "    - New mask: 53 features, Fitness: -20.1823\n",
      "    - New mask: 55 features, Fitness: -21.6846\n",
      "    - New mask: 60 features, Fitness: -27.0613\n",
      "    - New mask: 58 features, Fitness: -26.4521\n",
      "    - New mask: 58 features, Fitness: -24.7836\n",
      "    - New mask: 59 features, Fitness: -26.0017\n",
      "    - New mask: 56 features, Fitness: -16.4017\n",
      "    - New mask: 57 features, Fitness: -18.0566\n",
      "    - New mask: 53 features, Fitness: -16.9103\n",
      "    - New mask: 56 features, Fitness: -17.1767\n",
      "    - New mask: 60 features, Fitness: -21.9319\n",
      "    - New mask: 56 features, Fitness: -17.7109\n",
      "    - New mask: 53 features, Fitness: -14.7660\n",
      "    - New mask: 51 features, Fitness: -13.4446\n",
      "    - New mask: 56 features, Fitness: -18.1033\n",
      "    - New mask: 59 features, Fitness: -19.6619\n",
      "    - New mask: 52 features, Fitness: -14.1972\n",
      "    - New mask: 54 features, Fitness: -15.7747\n",
      "    - New mask: 53 features, Fitness: -14.9910\n",
      "    - New mask: 55 features, Fitness: -16.2815\n",
      "    - New mask: 55 features, Fitness: -17.0422\n",
      "    - New mask: 57 features, Fitness: -17.6897\n",
      "    - New mask: 58 features, Fitness: -18.5740\n",
      "    - New mask: 53 features, Fitness: -13.9972\n",
      "    - New mask: 56 features, Fitness: -17.3465\n",
      "    - New mask: 54 features, Fitness: -17.0902\n",
      "    - New mask: 56 features, Fitness: -15.0982\n",
      "    - New mask: 57 features, Fitness: -16.1054\n",
      "    - New mask: 57 features, Fitness: -18.0827\n",
      "    - New mask: 50 features, Fitness: -11.4899\n",
      "    - New mask: 56 features, Fitness: -16.1273\n",
      "    - New mask: 56 features, Fitness: -16.6528\n",
      "    - New mask: 57 features, Fitness: -17.8313\n",
      "    - New mask: 59 features, Fitness: -18.7772\n",
      "    - New mask: 52 features, Fitness: -15.0298\n",
      "    - New mask: 52 features, Fitness: -12.2701\n",
      "    - New mask: 53 features, Fitness: -13.6793\n",
      "    - New mask: 52 features, Fitness: -12.0307\n",
      "    - New mask: 57 features, Fitness: -18.8855\n",
      "    - New mask: 55 features, Fitness: -15.4258\n",
      "    - New mask: 56 features, Fitness: -16.8378\n",
      "    - New mask: 56 features, Fitness: -15.4734\n",
      "    - New mask: 60 features, Fitness: -19.8721\n",
      "    - New mask: 55 features, Fitness: -15.3010\n",
      "    - New mask: 58 features, Fitness: -17.0726\n",
      "    - New mask: 57 features, Fitness: -16.3553\n",
      "    - New mask: 55 features, Fitness: -13.4609\n",
      "    - New mask: 48 features, Fitness: -7.5419\n",
      "    - New mask: 51 features, Fitness: -10.3676\n",
      "    - New mask: 56 features, Fitness: -13.1090\n",
      "    - New mask: 56 features, Fitness: -14.2885\n",
      "    - New mask: 52 features, Fitness: -11.7062\n",
      "    - New mask: 58 features, Fitness: -15.3557\n",
      "    - New mask: 53 features, Fitness: -11.5907\n",
      "    - New mask: 56 features, Fitness: -13.3702\n",
      "    - New mask: 55 features, Fitness: -13.5850\n",
      "    - New mask: 60 features, Fitness: -15.2735\n",
      "    - New mask: 59 features, Fitness: -14.1491\n",
      "    - New mask: 53 features, Fitness: -11.3480\n",
      "    - New mask: 50 features, Fitness: -9.0906\n",
      "    - New mask: 54 features, Fitness: -12.4530\n",
      "    - New mask: 55 features, Fitness: -10.9518\n",
      "    - New mask: 54 features, Fitness: -11.3142\n",
      "    - New mask: 55 features, Fitness: -11.9323\n",
      "    - New mask: 55 features, Fitness: -12.5784\n",
      "    - New mask: 49 features, Fitness: -8.3591\n",
      "    - New mask: 54 features, Fitness: -3.2218\n",
      "    - New mask: 59 features, Fitness: -6.5279\n",
      "    - New mask: 58 features, Fitness: -6.9095\n",
      "    - New mask: 57 features, Fitness: -6.2377\n",
      "    - New mask: 56 features, Fitness: -6.0655\n",
      "    - New mask: 51 features, Fitness: -2.1974\n",
      "    - New mask: 56 features, Fitness: -5.7875\n",
      "    - New mask: 53 features, Fitness: -4.6410\n",
      "    - New mask: 55 features, Fitness: -5.3393\n",
      "    - New mask: 55 features, Fitness: -4.1998\n",
      "    - New mask: 54 features, Fitness: -4.9744\n",
      "    - New mask: 57 features, Fitness: -4.9123\n",
      "    - New mask: 54 features, Fitness: -3.6003\n",
      "    - New mask: 58 features, Fitness: -6.1984\n",
      "    - New mask: 55 features, Fitness: -4.1622\n",
      "    - New mask: 56 features, Fitness: -5.5748\n",
      "    - New mask: 53 features, Fitness: -5.0473\n",
      "    - New mask: 56 features, Fitness: -4.0872\n",
      "    - New mask: 56 features, Fitness: -5.9075\n",
      "    - New mask: 56 features, Fitness: -5.0626\n",
      "    - New mask: 52 features, Fitness: -12.9377\n",
      "    - New mask: 53 features, Fitness: -14.5930\n",
      "    - New mask: 56 features, Fitness: -15.3222\n",
      "    - New mask: 53 features, Fitness: -14.5243\n",
      "    - New mask: 60 features, Fitness: -20.4494\n",
      "    - New mask: 55 features, Fitness: -16.5155\n",
      "    - New mask: 54 features, Fitness: -13.3881\n",
      "    - New mask: 54 features, Fitness: -16.0196\n",
      "    - New mask: 54 features, Fitness: -13.9513\n",
      "    - New mask: 60 features, Fitness: -18.1433\n",
      "    - New mask: 54 features, Fitness: -14.4847\n",
      "    - New mask: 58 features, Fitness: -17.0848\n",
      "    - New mask: 54 features, Fitness: -14.0149\n",
      "    - New mask: 52 features, Fitness: -13.1476\n",
      "    - New mask: 59 features, Fitness: -17.5195\n",
      "    - New mask: 57 features, Fitness: -16.7890\n",
      "    - New mask: 56 features, Fitness: -16.4330\n",
      "    - New mask: 50 features, Fitness: -11.5468\n",
      "    - New mask: 56 features, Fitness: -16.7349\n",
      "    - New mask: 54 features, Fitness: -13.5095\n",
      "    - New mask: 54 features, Fitness: -13.7218\n",
      "    - New mask: 51 features, Fitness: -11.9684\n",
      "    - New mask: 58 features, Fitness: -17.2261\n",
      "    - New mask: 59 features, Fitness: -17.5755\n",
      "    - New mask: 58 features, Fitness: -16.6008\n",
      "    - New mask: 56 features, Fitness: -15.5781\n",
      "    - New mask: 55 features, Fitness: -14.4690\n",
      "    - New mask: 56 features, Fitness: -15.0311\n",
      "    - New mask: 53 features, Fitness: -14.2439\n",
      "    - New mask: 52 features, Fitness: -11.7715\n",
      "    - New mask: 55 features, Fitness: -14.7649\n",
      "    - New mask: 56 features, Fitness: -14.8487\n",
      "    - New mask: 53 features, Fitness: -14.3911\n",
      "    - New mask: 54 features, Fitness: -13.6447\n",
      "    - New mask: 52 features, Fitness: -12.1273\n",
      "    - New mask: 58 features, Fitness: -16.2298\n",
      "    - New mask: 56 features, Fitness: -14.7090\n",
      "    - New mask: 53 features, Fitness: -12.0654\n",
      "    - New mask: 54 features, Fitness: -14.6805\n",
      "    - New mask: 56 features, Fitness: -15.7830\n",
      "    - New mask: 54 features, Fitness: -9.2015\n",
      "    - New mask: 57 features, Fitness: -10.8674\n",
      "    - New mask: 56 features, Fitness: -9.7883\n",
      "    - New mask: 56 features, Fitness: -9.2354\n",
      "    - New mask: 58 features, Fitness: -11.3625\n",
      "    - New mask: 59 features, Fitness: -10.2642\n",
      "    - New mask: 56 features, Fitness: -9.8354\n",
      "    - New mask: 56 features, Fitness: -8.7864\n",
      "    - New mask: 54 features, Fitness: -10.2782\n",
      "    - New mask: 50 features, Fitness: -6.5513\n",
      "    - New mask: 59 features, Fitness: -11.2403\n",
      "    - New mask: 56 features, Fitness: -10.1719\n",
      "    - New mask: 55 features, Fitness: -8.5694\n",
      "    - New mask: 58 features, Fitness: -10.8145\n",
      "    - New mask: 59 features, Fitness: -11.6860\n",
      "    - New mask: 55 features, Fitness: -8.7907\n",
      "    - New mask: 60 features, Fitness: -11.6500\n",
      "    - New mask: 54 features, Fitness: -7.9118\n",
      "    - New mask: 57 features, Fitness: -9.4934\n",
      "    - New mask: 56 features, Fitness: -8.8397\n",
      "    - New mask: 53 features, Fitness: -11.6840\n",
      "    - New mask: 58 features, Fitness: -16.0817\n",
      "    - New mask: 55 features, Fitness: -15.6127\n",
      "    - New mask: 61 features, Fitness: -19.6207\n",
      "    - New mask: 59 features, Fitness: -16.4533\n",
      "    - New mask: 54 features, Fitness: -13.9022\n",
      "    - New mask: 56 features, Fitness: -16.1159\n",
      "    - New mask: 56 features, Fitness: -14.1543\n",
      "    - New mask: 57 features, Fitness: -15.5684\n",
      "    - New mask: 60 features, Fitness: -18.6973\n",
      "    - New mask: 56 features, Fitness: -14.2999\n",
      "    - New mask: 57 features, Fitness: -16.7351\n",
      "    - New mask: 54 features, Fitness: -12.8714\n",
      "    - New mask: 54 features, Fitness: -13.5126\n",
      "    - New mask: 55 features, Fitness: -14.0030\n",
      "    - New mask: 59 features, Fitness: -18.3135\n",
      "    - New mask: 55 features, Fitness: -15.2091\n",
      "    - New mask: 54 features, Fitness: -13.5267\n",
      "    - New mask: 59 features, Fitness: -18.4323\n",
      "    - New mask: 57 features, Fitness: -17.6501\n",
      "    - New mask: 57 features, Fitness: -17.1562\n",
      "    - New mask: 56 features, Fitness: -15.5253\n",
      "    - New mask: 59 features, Fitness: -18.4762\n",
      "    - New mask: 54 features, Fitness: -15.1470\n",
      "    - New mask: 59 features, Fitness: -17.9682\n",
      "    - New mask: 60 features, Fitness: -20.3014\n",
      "    - New mask: 58 features, Fitness: -18.0355\n",
      "    - New mask: 58 features, Fitness: -18.4193\n",
      "    - New mask: 55 features, Fitness: -14.5770\n",
      "    - New mask: 61 features, Fitness: -21.4102\n",
      "    - New mask: 51 features, Fitness: -12.2424\n",
      "    - New mask: 55 features, Fitness: -16.2292\n",
      "    - New mask: 57 features, Fitness: -17.2740\n",
      "    - New mask: 58 features, Fitness: -17.4802\n",
      "    - New mask: 56 features, Fitness: -16.5959\n",
      "    - New mask: 56 features, Fitness: -16.9224\n",
      "    - New mask: 51 features, Fitness: -11.9151\n",
      "    - New mask: 59 features, Fitness: -19.2614\n",
      "    - New mask: 56 features, Fitness: -16.3764\n",
      "    - New mask: 59 features, Fitness: -20.1601\n",
      "=== End of Round 12: Vote mask selects 57 features (rho: 0.55)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 17, 19, 21, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 13 ================\n",
      "  Adaptive rho for this round: 0.58\n",
      "    - New mask: 50 features, Fitness: -18.5220\n",
      "    - New mask: 55 features, Fitness: -22.7655\n",
      "    - New mask: 57 features, Fitness: -23.9352\n",
      "    - New mask: 57 features, Fitness: -22.9963\n",
      "    - New mask: 59 features, Fitness: -26.4413\n",
      "    - New mask: 54 features, Fitness: -21.3860\n",
      "    - New mask: 51 features, Fitness: -17.4591\n",
      "    - New mask: 51 features, Fitness: -18.8959\n",
      "    - New mask: 57 features, Fitness: -24.8010\n",
      "    - New mask: 52 features, Fitness: -17.3362\n",
      "    - New mask: 56 features, Fitness: -23.0092\n",
      "    - New mask: 58 features, Fitness: -27.1162\n",
      "    - New mask: 53 features, Fitness: -21.2217\n",
      "    - New mask: 55 features, Fitness: -23.2098\n",
      "    - New mask: 52 features, Fitness: -17.7384\n",
      "    - New mask: 52 features, Fitness: -18.2707\n",
      "    - New mask: 56 features, Fitness: -23.0302\n",
      "    - New mask: 55 features, Fitness: -21.8178\n",
      "    - New mask: 54 features, Fitness: -20.8685\n",
      "    - New mask: 57 features, Fitness: -24.2670\n",
      "    - New mask: 54 features, Fitness: -15.5462\n",
      "    - New mask: 55 features, Fitness: -15.9822\n",
      "    - New mask: 50 features, Fitness: -13.7676\n",
      "    - New mask: 54 features, Fitness: -15.0263\n",
      "    - New mask: 58 features, Fitness: -18.8155\n",
      "    - New mask: 56 features, Fitness: -17.0193\n",
      "    - New mask: 47 features, Fitness: -10.9196\n",
      "    - New mask: 51 features, Fitness: -13.4772\n",
      "    - New mask: 54 features, Fitness: -17.6164\n",
      "    - New mask: 56 features, Fitness: -17.1603\n",
      "    - New mask: 48 features, Fitness: -11.8315\n",
      "    - New mask: 54 features, Fitness: -15.3674\n",
      "    - New mask: 53 features, Fitness: -14.2297\n",
      "    - New mask: 54 features, Fitness: -15.1053\n",
      "    - New mask: 48 features, Fitness: -11.6800\n",
      "    - New mask: 54 features, Fitness: -16.0374\n",
      "    - New mask: 53 features, Fitness: -14.3457\n",
      "    - New mask: 56 features, Fitness: -17.1666\n",
      "    - New mask: 53 features, Fitness: -15.0074\n",
      "    - New mask: 50 features, Fitness: -13.9959\n",
      "    - New mask: 58 features, Fitness: -17.5256\n",
      "    - New mask: 52 features, Fitness: -12.8114\n",
      "    - New mask: 53 features, Fitness: -13.7193\n",
      "    - New mask: 52 features, Fitness: -12.9818\n",
      "    - New mask: 53 features, Fitness: -14.0085\n",
      "    - New mask: 54 features, Fitness: -14.6873\n",
      "    - New mask: 53 features, Fitness: -13.9590\n",
      "    - New mask: 55 features, Fitness: -15.9574\n",
      "    - New mask: 51 features, Fitness: -13.6285\n",
      "    - New mask: 55 features, Fitness: -14.4201\n",
      "    - New mask: 51 features, Fitness: -11.3071\n",
      "    - New mask: 49 features, Fitness: -9.9496\n",
      "    - New mask: 55 features, Fitness: -14.3610\n",
      "    - New mask: 58 features, Fitness: -17.7113\n",
      "    - New mask: 54 features, Fitness: -14.6960\n",
      "    - New mask: 52 features, Fitness: -12.3363\n",
      "    - New mask: 56 features, Fitness: -15.2687\n",
      "    - New mask: 49 features, Fitness: -11.7346\n",
      "    - New mask: 54 features, Fitness: -13.8789\n",
      "    - New mask: 52 features, Fitness: -12.9720\n",
      "    - New mask: 54 features, Fitness: -10.4833\n",
      "    - New mask: 52 features, Fitness: -10.9478\n",
      "    - New mask: 54 features, Fitness: -11.2447\n",
      "    - New mask: 55 features, Fitness: -11.7301\n",
      "    - New mask: 53 features, Fitness: -10.4248\n",
      "    - New mask: 52 features, Fitness: -10.7335\n",
      "    - New mask: 51 features, Fitness: -11.3576\n",
      "    - New mask: 55 features, Fitness: -12.4151\n",
      "    - New mask: 55 features, Fitness: -12.6869\n",
      "    - New mask: 53 features, Fitness: -11.1949\n",
      "    - New mask: 53 features, Fitness: -8.9828\n",
      "    - New mask: 53 features, Fitness: -10.5163\n",
      "    - New mask: 49 features, Fitness: -8.8700\n",
      "    - New mask: 51 features, Fitness: -9.3975\n",
      "    - New mask: 51 features, Fitness: -10.2343\n",
      "    - New mask: 50 features, Fitness: -8.3958\n",
      "    - New mask: 49 features, Fitness: -8.5743\n",
      "    - New mask: 56 features, Fitness: -13.5364\n",
      "    - New mask: 53 features, Fitness: -11.2037\n",
      "    - New mask: 51 features, Fitness: -8.9507\n",
      "    - New mask: 49 features, Fitness: -0.7067\n",
      "    - New mask: 58 features, Fitness: -6.3673\n",
      "    - New mask: 56 features, Fitness: -5.4426\n",
      "    - New mask: 56 features, Fitness: -5.4333\n",
      "    - New mask: 53 features, Fitness: -3.8749\n",
      "    - New mask: 53 features, Fitness: -3.4194\n",
      "    - New mask: 55 features, Fitness: -4.0571\n",
      "    - New mask: 49 features, Fitness: -3.2374\n",
      "    - New mask: 50 features, Fitness: -2.3492\n",
      "    - New mask: 52 features, Fitness: -3.7151\n",
      "    - New mask: 51 features, Fitness: -2.3396\n",
      "    - New mask: 56 features, Fitness: -4.4724\n",
      "    - New mask: 53 features, Fitness: -3.4785\n",
      "    - New mask: 58 features, Fitness: -7.3003\n",
      "    - New mask: 54 features, Fitness: -3.0840\n",
      "    - New mask: 57 features, Fitness: -5.1228\n",
      "    - New mask: 49 features, Fitness: -1.5346\n",
      "    - New mask: 56 features, Fitness: -4.9588\n",
      "    - New mask: 55 features, Fitness: -3.9464\n",
      "    - New mask: 51 features, Fitness: -3.3521\n",
      "    - New mask: 50 features, Fitness: -10.6419\n",
      "    - New mask: 51 features, Fitness: -12.2238\n",
      "    - New mask: 51 features, Fitness: -12.3667\n",
      "    - New mask: 57 features, Fitness: -16.6997\n",
      "    - New mask: 54 features, Fitness: -15.0336\n",
      "    - New mask: 53 features, Fitness: -13.5760\n",
      "    - New mask: 49 features, Fitness: -10.0111\n",
      "    - New mask: 53 features, Fitness: -15.1111\n",
      "    - New mask: 51 features, Fitness: -12.2894\n",
      "    - New mask: 55 features, Fitness: -15.2367\n",
      "    - New mask: 51 features, Fitness: -13.3008\n",
      "    - New mask: 57 features, Fitness: -16.0532\n",
      "    - New mask: 52 features, Fitness: -12.6845\n",
      "    - New mask: 53 features, Fitness: -13.7500\n",
      "    - New mask: 51 features, Fitness: -11.8124\n",
      "    - New mask: 55 features, Fitness: -15.7079\n",
      "    - New mask: 55 features, Fitness: -15.0807\n",
      "    - New mask: 51 features, Fitness: -11.5973\n",
      "    - New mask: 55 features, Fitness: -14.8593\n",
      "    - New mask: 54 features, Fitness: -14.2454\n",
      "    - New mask: 51 features, Fitness: -12.3586\n",
      "    - New mask: 53 features, Fitness: -13.8853\n",
      "    - New mask: 53 features, Fitness: -14.1364\n",
      "    - New mask: 55 features, Fitness: -13.5897\n",
      "    - New mask: 56 features, Fitness: -15.0004\n",
      "    - New mask: 57 features, Fitness: -16.0777\n",
      "    - New mask: 53 features, Fitness: -12.9623\n",
      "    - New mask: 52 features, Fitness: -13.8262\n",
      "    - New mask: 53 features, Fitness: -13.4451\n",
      "    - New mask: 53 features, Fitness: -13.1667\n",
      "    - New mask: 52 features, Fitness: -12.2428\n",
      "    - New mask: 58 features, Fitness: -15.5068\n",
      "    - New mask: 54 features, Fitness: -13.4863\n",
      "    - New mask: 54 features, Fitness: -14.7201\n",
      "    - New mask: 52 features, Fitness: -13.1423\n",
      "    - New mask: 53 features, Fitness: -13.1624\n",
      "    - New mask: 57 features, Fitness: -15.4948\n",
      "    - New mask: 52 features, Fitness: -11.8505\n",
      "    - New mask: 53 features, Fitness: -13.0829\n",
      "    - New mask: 53 features, Fitness: -13.1199\n",
      "    - New mask: 50 features, Fitness: -6.8705\n",
      "    - New mask: 52 features, Fitness: -7.3815\n",
      "    - New mask: 55 features, Fitness: -9.3017\n",
      "    - New mask: 54 features, Fitness: -8.9011\n",
      "    - New mask: 55 features, Fitness: -8.8449\n",
      "    - New mask: 57 features, Fitness: -10.7379\n",
      "    - New mask: 58 features, Fitness: -11.3057\n",
      "    - New mask: 56 features, Fitness: -8.6008\n",
      "    - New mask: 55 features, Fitness: -9.6469\n",
      "    - New mask: 50 features, Fitness: -5.7278\n",
      "    - New mask: 53 features, Fitness: -6.3756\n",
      "    - New mask: 53 features, Fitness: -8.3093\n",
      "    - New mask: 53 features, Fitness: -7.5335\n",
      "    - New mask: 51 features, Fitness: -6.5513\n",
      "    - New mask: 52 features, Fitness: -7.6886\n",
      "    - New mask: 54 features, Fitness: -7.9182\n",
      "    - New mask: 54 features, Fitness: -9.2367\n",
      "    - New mask: 50 features, Fitness: -6.0054\n",
      "    - New mask: 51 features, Fitness: -5.2882\n",
      "    - New mask: 53 features, Fitness: -7.8616\n",
      "    - New mask: 55 features, Fitness: -12.7247\n",
      "    - New mask: 50 features, Fitness: -10.4418\n",
      "    - New mask: 54 features, Fitness: -14.6155\n",
      "    - New mask: 56 features, Fitness: -15.0480\n",
      "    - New mask: 56 features, Fitness: -13.9629\n",
      "    - New mask: 54 features, Fitness: -12.2535\n",
      "    - New mask: 54 features, Fitness: -12.8891\n",
      "    - New mask: 52 features, Fitness: -12.4679\n",
      "    - New mask: 54 features, Fitness: -13.2375\n",
      "    - New mask: 57 features, Fitness: -16.1551\n",
      "    - New mask: 54 features, Fitness: -12.1327\n",
      "    - New mask: 51 features, Fitness: -12.0267\n",
      "    - New mask: 50 features, Fitness: -9.3175\n",
      "    - New mask: 54 features, Fitness: -14.8905\n",
      "    - New mask: 51 features, Fitness: -10.7838\n",
      "    - New mask: 58 features, Fitness: -15.8100\n",
      "    - New mask: 56 features, Fitness: -15.5670\n",
      "    - New mask: 51 features, Fitness: -11.1591\n",
      "    - New mask: 58 features, Fitness: -15.9093\n",
      "    - New mask: 53 features, Fitness: -12.3310\n",
      "    - New mask: 57 features, Fitness: -17.2285\n",
      "    - New mask: 57 features, Fitness: -16.0586\n",
      "    - New mask: 57 features, Fitness: -18.1894\n",
      "    - New mask: 50 features, Fitness: -11.7748\n",
      "    - New mask: 56 features, Fitness: -16.7204\n",
      "    - New mask: 54 features, Fitness: -14.6681\n",
      "    - New mask: 53 features, Fitness: -13.6217\n",
      "    - New mask: 57 features, Fitness: -17.7324\n",
      "    - New mask: 56 features, Fitness: -16.5488\n",
      "    - New mask: 57 features, Fitness: -18.6011\n",
      "    - New mask: 51 features, Fitness: -13.4721\n",
      "    - New mask: 54 features, Fitness: -15.8853\n",
      "    - New mask: 55 features, Fitness: -15.1736\n",
      "    - New mask: 55 features, Fitness: -16.9459\n",
      "    - New mask: 55 features, Fitness: -15.8939\n",
      "    - New mask: 57 features, Fitness: -17.7739\n",
      "    - New mask: 50 features, Fitness: -11.4576\n",
      "    - New mask: 57 features, Fitness: -17.2384\n",
      "    - New mask: 56 features, Fitness: -16.8287\n",
      "    - New mask: 56 features, Fitness: -17.0658\n",
      "=== End of Round 13: Vote mask selects 56 features (rho: 0.58)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 17, 19, 21, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 58, 59, 60, 61, 62, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 14 ================\n",
      "  Adaptive rho for this round: 0.61\n",
      "    - New mask: 50 features, Fitness: -19.6363\n",
      "    - New mask: 54 features, Fitness: -19.8764\n",
      "    - New mask: 53 features, Fitness: -21.7953\n",
      "    - New mask: 55 features, Fitness: -21.4960\n",
      "    - New mask: 52 features, Fitness: -20.6930\n",
      "    - New mask: 50 features, Fitness: -16.4326\n",
      "    - New mask: 53 features, Fitness: -18.7416\n",
      "    - New mask: 49 features, Fitness: -16.5628\n",
      "    - New mask: 55 features, Fitness: -22.9445\n",
      "    - New mask: 52 features, Fitness: -17.7010\n",
      "    - New mask: 55 features, Fitness: -22.0537\n",
      "    - New mask: 59 features, Fitness: -27.4284\n",
      "    - New mask: 49 features, Fitness: -16.7724\n",
      "    - New mask: 57 features, Fitness: -24.2198\n",
      "    - New mask: 53 features, Fitness: -18.5816\n",
      "    - New mask: 56 features, Fitness: -21.8215\n",
      "    - New mask: 52 features, Fitness: -18.4391\n",
      "    - New mask: 53 features, Fitness: -18.5278\n",
      "    - New mask: 54 features, Fitness: -21.4158\n",
      "    - New mask: 56 features, Fitness: -21.9612\n",
      "    - New mask: 52 features, Fitness: -13.9847\n",
      "    - New mask: 48 features, Fitness: -11.9108\n",
      "    - New mask: 54 features, Fitness: -15.5110\n",
      "    - New mask: 51 features, Fitness: -12.5847\n",
      "    - New mask: 53 features, Fitness: -14.9067\n",
      "    - New mask: 51 features, Fitness: -13.9539\n",
      "    - New mask: 49 features, Fitness: -11.7722\n",
      "    - New mask: 54 features, Fitness: -15.0232\n",
      "    - New mask: 51 features, Fitness: -15.0799\n",
      "    - New mask: 52 features, Fitness: -14.5167\n",
      "    - New mask: 43 features, Fitness: -8.2110\n",
      "    - New mask: 49 features, Fitness: -11.9695\n",
      "    - New mask: 50 features, Fitness: -11.9684\n",
      "    - New mask: 55 features, Fitness: -16.2236\n",
      "    - New mask: 49 features, Fitness: -11.3348\n",
      "    - New mask: 48 features, Fitness: -12.6942\n",
      "    - New mask: 46 features, Fitness: -10.5319\n",
      "    - New mask: 53 features, Fitness: -14.0381\n",
      "    - New mask: 48 features, Fitness: -11.7823\n",
      "    - New mask: 50 features, Fitness: -12.3444\n",
      "    - New mask: 54 features, Fitness: -13.9515\n",
      "    - New mask: 50 features, Fitness: -10.3585\n",
      "    - New mask: 56 features, Fitness: -15.5289\n",
      "    - New mask: 53 features, Fitness: -12.3683\n",
      "    - New mask: 56 features, Fitness: -16.0070\n",
      "    - New mask: 54 features, Fitness: -14.1527\n",
      "    - New mask: 50 features, Fitness: -11.7018\n",
      "    - New mask: 54 features, Fitness: -14.8789\n",
      "    - New mask: 49 features, Fitness: -11.6762\n",
      "    - New mask: 54 features, Fitness: -13.8977\n",
      "    - New mask: 51 features, Fitness: -10.9236\n",
      "    - New mask: 51 features, Fitness: -12.3112\n",
      "    - New mask: 52 features, Fitness: -12.2867\n",
      "    - New mask: 55 features, Fitness: -15.1846\n",
      "    - New mask: 49 features, Fitness: -10.4011\n",
      "    - New mask: 49 features, Fitness: -10.4047\n",
      "    - New mask: 52 features, Fitness: -11.9522\n",
      "    - New mask: 50 features, Fitness: -13.6459\n",
      "    - New mask: 51 features, Fitness: -10.4508\n",
      "    - New mask: 48 features, Fitness: -11.3994\n",
      "    - New mask: 53 features, Fitness: -9.6531\n",
      "    - New mask: 54 features, Fitness: -10.6367\n",
      "    - New mask: 53 features, Fitness: -10.4310\n",
      "    - New mask: 52 features, Fitness: -10.0249\n",
      "    - New mask: 51 features, Fitness: -9.2366\n",
      "    - New mask: 53 features, Fitness: -10.2022\n",
      "    - New mask: 52 features, Fitness: -9.8414\n",
      "    - New mask: 54 features, Fitness: -11.9863\n",
      "    - New mask: 54 features, Fitness: -10.9260\n",
      "    - New mask: 58 features, Fitness: -14.0869\n",
      "    - New mask: 51 features, Fitness: -8.6934\n",
      "    - New mask: 52 features, Fitness: -9.9310\n",
      "    - New mask: 53 features, Fitness: -11.3933\n",
      "    - New mask: 49 features, Fitness: -8.6105\n",
      "    - New mask: 53 features, Fitness: -10.3964\n",
      "    - New mask: 49 features, Fitness: -8.0840\n",
      "    - New mask: 51 features, Fitness: -10.2756\n",
      "    - New mask: 52 features, Fitness: -10.9065\n",
      "    - New mask: 51 features, Fitness: -8.7491\n",
      "    - New mask: 49 features, Fitness: -8.0818\n",
      "    - New mask: 47 features, Fitness: 0.3614\n",
      "    - New mask: 54 features, Fitness: -2.5280\n",
      "    - New mask: 53 features, Fitness: -5.3619\n",
      "    - New mask: 55 features, Fitness: -4.1105\n",
      "    - New mask: 52 features, Fitness: -2.5977\n",
      "    - New mask: 51 features, Fitness: -2.9905\n",
      "    - New mask: 50 features, Fitness: -2.9862\n",
      "    - New mask: 53 features, Fitness: -4.5386\n",
      "    - New mask: 48 features, Fitness: -0.5806\n",
      "    - New mask: 49 features, Fitness: -2.2995\n",
      "    - New mask: 52 features, Fitness: -1.5740\n",
      "    - New mask: 52 features, Fitness: -2.5390\n",
      "    - New mask: 51 features, Fitness: -3.1179\n",
      "    - New mask: 50 features, Fitness: -1.3953\n",
      "    - New mask: 53 features, Fitness: -2.9705\n",
      "    - New mask: 53 features, Fitness: -3.8550\n",
      "    - New mask: 53 features, Fitness: -3.4229\n",
      "    - New mask: 46 features, Fitness: 1.3316\n",
      "    - New mask: 52 features, Fitness: -2.1797\n",
      "    - New mask: 49 features, Fitness: -0.8238\n",
      "    - New mask: 57 features, Fitness: -16.0513\n",
      "    - New mask: 51 features, Fitness: -12.4025\n",
      "    - New mask: 50 features, Fitness: -10.3931\n",
      "    - New mask: 52 features, Fitness: -13.1554\n",
      "    - New mask: 50 features, Fitness: -11.8641\n",
      "    - New mask: 55 features, Fitness: -14.2438\n",
      "    - New mask: 50 features, Fitness: -10.6968\n",
      "    - New mask: 53 features, Fitness: -14.5526\n",
      "    - New mask: 51 features, Fitness: -11.7596\n",
      "    - New mask: 55 features, Fitness: -15.0971\n",
      "    - New mask: 56 features, Fitness: -16.4449\n",
      "    - New mask: 51 features, Fitness: -11.8616\n",
      "    - New mask: 53 features, Fitness: -12.6496\n",
      "    - New mask: 51 features, Fitness: -11.1789\n",
      "    - New mask: 51 features, Fitness: -10.9983\n",
      "    - New mask: 55 features, Fitness: -14.2236\n",
      "    - New mask: 58 features, Fitness: -16.5860\n",
      "    - New mask: 53 features, Fitness: -14.3380\n",
      "    - New mask: 56 features, Fitness: -15.4748\n",
      "    - New mask: 52 features, Fitness: -12.3858\n",
      "    - New mask: 47 features, Fitness: -10.5229\n",
      "    - New mask: 52 features, Fitness: -12.3543\n",
      "    - New mask: 52 features, Fitness: -13.2165\n",
      "    - New mask: 53 features, Fitness: -12.9159\n",
      "    - New mask: 54 features, Fitness: -13.5844\n",
      "    - New mask: 54 features, Fitness: -13.5111\n",
      "    - New mask: 55 features, Fitness: -14.2079\n",
      "    - New mask: 53 features, Fitness: -13.6678\n",
      "    - New mask: 55 features, Fitness: -15.0362\n",
      "    - New mask: 53 features, Fitness: -12.6364\n",
      "    - New mask: 53 features, Fitness: -13.5939\n",
      "    - New mask: 54 features, Fitness: -13.5558\n",
      "    - New mask: 48 features, Fitness: -10.0931\n",
      "    - New mask: 52 features, Fitness: -12.5115\n",
      "    - New mask: 51 features, Fitness: -11.8825\n",
      "    - New mask: 50 features, Fitness: -10.9699\n",
      "    - New mask: 51 features, Fitness: -11.7182\n",
      "    - New mask: 55 features, Fitness: -13.7826\n",
      "    - New mask: 54 features, Fitness: -13.8706\n",
      "    - New mask: 53 features, Fitness: -13.4215\n",
      "    - New mask: 52 features, Fitness: -6.8434\n",
      "    - New mask: 50 features, Fitness: -5.4636\n",
      "    - New mask: 51 features, Fitness: -6.0798\n",
      "    - New mask: 56 features, Fitness: -10.1726\n",
      "    - New mask: 56 features, Fitness: -8.3737\n",
      "    - New mask: 50 features, Fitness: -6.0215\n",
      "    - New mask: 58 features, Fitness: -10.5226\n",
      "    - New mask: 55 features, Fitness: -7.6666\n",
      "    - New mask: 56 features, Fitness: -10.3863\n",
      "    - New mask: 51 features, Fitness: -5.6826\n",
      "    - New mask: 54 features, Fitness: -7.7091\n",
      "    - New mask: 53 features, Fitness: -8.7782\n",
      "    - New mask: 52 features, Fitness: -6.6692\n",
      "    - New mask: 55 features, Fitness: -9.1622\n",
      "    - New mask: 53 features, Fitness: -7.4556\n",
      "    - New mask: 53 features, Fitness: -7.9549\n",
      "    - New mask: 51 features, Fitness: -5.2308\n",
      "    - New mask: 48 features, Fitness: -4.3087\n",
      "    - New mask: 53 features, Fitness: -6.1619\n",
      "    - New mask: 49 features, Fitness: -4.5902\n",
      "    - New mask: 55 features, Fitness: -12.8687\n",
      "    - New mask: 57 features, Fitness: -14.8170\n",
      "    - New mask: 55 features, Fitness: -14.7555\n",
      "    - New mask: 54 features, Fitness: -12.9429\n",
      "    - New mask: 54 features, Fitness: -12.6513\n",
      "    - New mask: 49 features, Fitness: -9.0700\n",
      "    - New mask: 49 features, Fitness: -9.3249\n",
      "    - New mask: 52 features, Fitness: -11.5024\n",
      "    - New mask: 54 features, Fitness: -11.9259\n",
      "    - New mask: 53 features, Fitness: -13.4610\n",
      "    - New mask: 51 features, Fitness: -9.9349\n",
      "    - New mask: 54 features, Fitness: -13.0064\n",
      "    - New mask: 52 features, Fitness: -10.4384\n",
      "    - New mask: 50 features, Fitness: -9.4164\n",
      "    - New mask: 50 features, Fitness: -10.0442\n",
      "    - New mask: 57 features, Fitness: -14.7227\n",
      "    - New mask: 55 features, Fitness: -14.9287\n",
      "    - New mask: 53 features, Fitness: -12.0525\n",
      "    - New mask: 54 features, Fitness: -12.3979\n",
      "    - New mask: 55 features, Fitness: -13.2964\n",
      "    - New mask: 48 features, Fitness: -11.2873\n",
      "    - New mask: 53 features, Fitness: -14.1172\n",
      "    - New mask: 60 features, Fitness: -21.8487\n",
      "    - New mask: 51 features, Fitness: -13.0914\n",
      "    - New mask: 53 features, Fitness: -13.9379\n",
      "    - New mask: 52 features, Fitness: -12.5804\n",
      "    - New mask: 50 features, Fitness: -12.2615\n",
      "    - New mask: 52 features, Fitness: -13.5802\n",
      "    - New mask: 54 features, Fitness: -14.6656\n",
      "    - New mask: 55 features, Fitness: -15.1027\n",
      "    - New mask: 52 features, Fitness: -12.7517\n",
      "    - New mask: 55 features, Fitness: -14.7141\n",
      "    - New mask: 53 features, Fitness: -14.8078\n",
      "    - New mask: 54 features, Fitness: -15.4164\n",
      "    - New mask: 53 features, Fitness: -14.3373\n",
      "    - New mask: 55 features, Fitness: -14.9888\n",
      "    - New mask: 51 features, Fitness: -12.5967\n",
      "    - New mask: 49 features, Fitness: -12.6734\n",
      "    - New mask: 52 features, Fitness: -15.2369\n",
      "    - New mask: 52 features, Fitness: -14.3698\n",
      "=== End of Round 14: Vote mask selects 52 features (rho: 0.61)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 14, 17, 19, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 58, 59, 60, 61, 62, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 15 ================\n",
      "  Adaptive rho for this round: 0.64\n",
      "    - New mask: 49 features, Fitness: -16.9629\n",
      "    - New mask: 51 features, Fitness: -17.3517\n",
      "    - New mask: 52 features, Fitness: -19.2906\n",
      "    - New mask: 53 features, Fitness: -21.0024\n",
      "    - New mask: 50 features, Fitness: -18.4981\n",
      "    - New mask: 50 features, Fitness: -19.5429\n",
      "    - New mask: 51 features, Fitness: -17.0582\n",
      "    - New mask: 49 features, Fitness: -16.4443\n",
      "    - New mask: 51 features, Fitness: -17.7692\n",
      "    - New mask: 53 features, Fitness: -19.4433\n",
      "    - New mask: 57 features, Fitness: -24.4628\n",
      "    - New mask: 57 features, Fitness: -25.2961\n",
      "    - New mask: 50 features, Fitness: -16.6250\n",
      "    - New mask: 48 features, Fitness: -15.3840\n",
      "    - New mask: 50 features, Fitness: -16.4224\n",
      "    - New mask: 53 features, Fitness: -19.3923\n",
      "    - New mask: 48 features, Fitness: -15.0312\n",
      "    - New mask: 52 features, Fitness: -18.3297\n",
      "    - New mask: 51 features, Fitness: -17.0214\n",
      "    - New mask: 53 features, Fitness: -20.2917\n",
      "    - New mask: 45 features, Fitness: -9.6819\n",
      "    - New mask: 45 features, Fitness: -9.5432\n",
      "    - New mask: 51 features, Fitness: -12.3322\n",
      "    - New mask: 49 features, Fitness: -11.2139\n",
      "    - New mask: 46 features, Fitness: -9.3541\n",
      "    - New mask: 52 features, Fitness: -14.1097\n",
      "    - New mask: 46 features, Fitness: -11.8831\n",
      "    - New mask: 49 features, Fitness: -11.2164\n",
      "    - New mask: 47 features, Fitness: -11.1484\n",
      "    - New mask: 47 features, Fitness: -10.1901\n",
      "    - New mask: 47 features, Fitness: -11.1109\n",
      "    - New mask: 47 features, Fitness: -11.0144\n",
      "    - New mask: 45 features, Fitness: -9.9543\n",
      "    - New mask: 49 features, Fitness: -12.2935\n",
      "    - New mask: 50 features, Fitness: -12.6618\n",
      "    - New mask: 48 features, Fitness: -10.9153\n",
      "    - New mask: 46 features, Fitness: -10.8977\n",
      "    - New mask: 49 features, Fitness: -11.5116\n",
      "    - New mask: 48 features, Fitness: -11.3269\n",
      "    - New mask: 46 features, Fitness: -10.7553\n",
      "    - New mask: 52 features, Fitness: -11.9762\n",
      "    - New mask: 50 features, Fitness: -10.8483\n",
      "    - New mask: 52 features, Fitness: -12.4090\n",
      "    - New mask: 47 features, Fitness: -8.1976\n",
      "    - New mask: 52 features, Fitness: -12.6909\n",
      "    - New mask: 52 features, Fitness: -11.7818\n",
      "    - New mask: 50 features, Fitness: -11.1702\n",
      "    - New mask: 52 features, Fitness: -12.2525\n",
      "    - New mask: 51 features, Fitness: -11.8923\n",
      "    - New mask: 51 features, Fitness: -10.6816\n",
      "    - New mask: 53 features, Fitness: -13.7507\n",
      "    - New mask: 48 features, Fitness: -10.4112\n",
      "    - New mask: 51 features, Fitness: -11.9685\n",
      "    - New mask: 50 features, Fitness: -10.7825\n",
      "    - New mask: 52 features, Fitness: -12.9928\n",
      "    - New mask: 49 features, Fitness: -9.9287\n",
      "    - New mask: 51 features, Fitness: -10.9657\n",
      "    - New mask: 50 features, Fitness: -12.7235\n",
      "    - New mask: 49 features, Fitness: -9.0151\n",
      "    - New mask: 47 features, Fitness: -9.0787\n",
      "    - New mask: 51 features, Fitness: -9.0274\n",
      "    - New mask: 52 features, Fitness: -9.5511\n",
      "    - New mask: 48 features, Fitness: -7.7502\n",
      "    - New mask: 50 features, Fitness: -8.2760\n",
      "    - New mask: 53 features, Fitness: -9.3580\n",
      "    - New mask: 52 features, Fitness: -10.6959\n",
      "    - New mask: 49 features, Fitness: -7.8773\n",
      "    - New mask: 51 features, Fitness: -10.4847\n",
      "    - New mask: 53 features, Fitness: -10.8330\n",
      "    - New mask: 50 features, Fitness: -8.5246\n",
      "    - New mask: 50 features, Fitness: -9.0506\n",
      "    - New mask: 52 features, Fitness: -10.1190\n",
      "    - New mask: 54 features, Fitness: -12.1837\n",
      "    - New mask: 51 features, Fitness: -8.3689\n",
      "    - New mask: 52 features, Fitness: -11.0618\n",
      "    - New mask: 49 features, Fitness: -8.0231\n",
      "    - New mask: 48 features, Fitness: -8.8712\n",
      "    - New mask: 51 features, Fitness: -9.3450\n",
      "    - New mask: 48 features, Fitness: -7.1940\n",
      "    - New mask: 50 features, Fitness: -9.6096\n",
      "    - New mask: 50 features, Fitness: -1.5606\n",
      "    - New mask: 48 features, Fitness: -0.4006\n",
      "    - New mask: 55 features, Fitness: -5.8642\n",
      "    - New mask: 51 features, Fitness: -2.5713\n",
      "    - New mask: 50 features, Fitness: -1.3001\n",
      "    - New mask: 51 features, Fitness: -3.0808\n",
      "    - New mask: 50 features, Fitness: -1.8893\n",
      "    - New mask: 51 features, Fitness: -2.0328\n",
      "    - New mask: 50 features, Fitness: -1.7863\n",
      "    - New mask: 47 features, Fitness: -1.1280\n",
      "    - New mask: 52 features, Fitness: -1.4382\n",
      "    - New mask: 44 features, Fitness: 0.9748\n",
      "    - New mask: 48 features, Fitness: -0.5039\n",
      "    - New mask: 50 features, Fitness: -2.0929\n",
      "    - New mask: 47 features, Fitness: -0.2784\n",
      "    - New mask: 51 features, Fitness: -2.5134\n",
      "    - New mask: 54 features, Fitness: -4.6053\n",
      "    - New mask: 48 features, Fitness: -0.7634\n",
      "    - New mask: 55 features, Fitness: -4.0203\n",
      "    - New mask: 47 features, Fitness: 0.3948\n",
      "    - New mask: 52 features, Fitness: -12.4435\n",
      "    - New mask: 52 features, Fitness: -12.2111\n",
      "    - New mask: 50 features, Fitness: -10.7903\n",
      "    - New mask: 53 features, Fitness: -12.8453\n",
      "    - New mask: 50 features, Fitness: -11.1813\n",
      "    - New mask: 51 features, Fitness: -10.5408\n",
      "    - New mask: 50 features, Fitness: -10.5936\n",
      "    - New mask: 56 features, Fitness: -14.5285\n",
      "    - New mask: 48 features, Fitness: -9.4110\n",
      "    - New mask: 52 features, Fitness: -12.5256\n",
      "    - New mask: 52 features, Fitness: -12.5209\n",
      "    - New mask: 52 features, Fitness: -11.8955\n",
      "    - New mask: 52 features, Fitness: -12.3244\n",
      "    - New mask: 47 features, Fitness: -9.7314\n",
      "    - New mask: 46 features, Fitness: -8.6100\n",
      "    - New mask: 52 features, Fitness: -12.0333\n",
      "    - New mask: 53 features, Fitness: -13.0500\n",
      "    - New mask: 50 features, Fitness: -11.7401\n",
      "    - New mask: 50 features, Fitness: -11.8207\n",
      "    - New mask: 50 features, Fitness: -10.6388\n",
      "    - New mask: 50 features, Fitness: -10.2839\n",
      "    - New mask: 50 features, Fitness: -11.4293\n",
      "    - New mask: 49 features, Fitness: -10.6665\n",
      "    - New mask: 52 features, Fitness: -12.5517\n",
      "    - New mask: 50 features, Fitness: -10.4413\n",
      "    - New mask: 50 features, Fitness: -11.3154\n",
      "    - New mask: 52 features, Fitness: -11.7153\n",
      "    - New mask: 53 features, Fitness: -13.6587\n",
      "    - New mask: 54 features, Fitness: -13.5768\n",
      "    - New mask: 53 features, Fitness: -12.8844\n",
      "    - New mask: 50 features, Fitness: -10.0431\n",
      "    - New mask: 53 features, Fitness: -11.9338\n",
      "    - New mask: 47 features, Fitness: -9.9691\n",
      "    - New mask: 48 features, Fitness: -9.8996\n",
      "    - New mask: 52 features, Fitness: -11.9338\n",
      "    - New mask: 51 features, Fitness: -11.9515\n",
      "    - New mask: 50 features, Fitness: -12.0312\n",
      "    - New mask: 49 features, Fitness: -11.0806\n",
      "    - New mask: 52 features, Fitness: -13.1467\n",
      "    - New mask: 48 features, Fitness: -9.3864\n",
      "    - New mask: 52 features, Fitness: -6.7123\n",
      "    - New mask: 48 features, Fitness: -4.6102\n",
      "    - New mask: 46 features, Fitness: -4.4746\n",
      "    - New mask: 50 features, Fitness: -6.7563\n",
      "    - New mask: 51 features, Fitness: -4.6515\n",
      "    - New mask: 50 features, Fitness: -5.3695\n",
      "    - New mask: 52 features, Fitness: -6.1022\n",
      "    - New mask: 49 features, Fitness: -5.1499\n",
      "    - New mask: 50 features, Fitness: -5.5399\n",
      "    - New mask: 49 features, Fitness: -4.4793\n",
      "    - New mask: 49 features, Fitness: -5.6108\n",
      "    - New mask: 51 features, Fitness: -7.9729\n",
      "    - New mask: 53 features, Fitness: -6.4626\n",
      "    - New mask: 51 features, Fitness: -7.1331\n",
      "    - New mask: 53 features, Fitness: -8.0005\n",
      "    - New mask: 52 features, Fitness: -6.4679\n",
      "    - New mask: 50 features, Fitness: -5.0191\n",
      "    - New mask: 49 features, Fitness: -5.2849\n",
      "    - New mask: 50 features, Fitness: -5.0238\n",
      "    - New mask: 45 features, Fitness: -3.0806\n",
      "    - New mask: 51 features, Fitness: -11.2827\n",
      "    - New mask: 56 features, Fitness: -15.0359\n",
      "    - New mask: 54 features, Fitness: -12.7210\n",
      "    - New mask: 50 features, Fitness: -9.3530\n",
      "    - New mask: 51 features, Fitness: -11.1138\n",
      "    - New mask: 48 features, Fitness: -8.7105\n",
      "    - New mask: 48 features, Fitness: -8.1922\n",
      "    - New mask: 47 features, Fitness: -7.8343\n",
      "    - New mask: 49 features, Fitness: -10.4479\n",
      "    - New mask: 51 features, Fitness: -11.0430\n",
      "    - New mask: 52 features, Fitness: -10.5900\n",
      "    - New mask: 51 features, Fitness: -9.7783\n",
      "    - New mask: 45 features, Fitness: -6.4165\n",
      "    - New mask: 52 features, Fitness: -11.7997\n",
      "    - New mask: 50 features, Fitness: -10.5410\n",
      "    - New mask: 53 features, Fitness: -12.0924\n",
      "    - New mask: 47 features, Fitness: -7.4284\n",
      "    - New mask: 50 features, Fitness: -9.8199\n",
      "    - New mask: 53 features, Fitness: -11.8798\n",
      "    - New mask: 51 features, Fitness: -11.3169\n",
      "    - New mask: 49 features, Fitness: -11.0738\n",
      "    - New mask: 51 features, Fitness: -13.4312\n",
      "    - New mask: 55 features, Fitness: -17.8221\n",
      "    - New mask: 47 features, Fitness: -10.8127\n",
      "    - New mask: 44 features, Fitness: -8.8075\n",
      "    - New mask: 49 features, Fitness: -11.5818\n",
      "    - New mask: 52 features, Fitness: -12.8520\n",
      "    - New mask: 50 features, Fitness: -13.0048\n",
      "    - New mask: 51 features, Fitness: -13.2978\n",
      "    - New mask: 55 features, Fitness: -14.5618\n",
      "    - New mask: 50 features, Fitness: -12.0089\n",
      "    - New mask: 53 features, Fitness: -14.1986\n",
      "    - New mask: 51 features, Fitness: -13.2688\n",
      "    - New mask: 54 features, Fitness: -15.1574\n",
      "    - New mask: 50 features, Fitness: -12.5988\n",
      "    - New mask: 55 features, Fitness: -15.8048\n",
      "    - New mask: 47 features, Fitness: -10.1328\n",
      "    - New mask: 47 features, Fitness: -10.1829\n",
      "    - New mask: 53 features, Fitness: -14.5966\n",
      "    - New mask: 51 features, Fitness: -12.9586\n",
      "=== End of Round 15: Vote mask selects 49 features (rho: 0.64)\n",
      "    Indices: [0, 1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 17, 19, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 38, 39, 40, 41, 42, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 58, 59, 60, 61, 62, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 16 ================\n",
      "  Adaptive rho for this round: 0.67\n",
      "    - New mask: 47 features, Fitness: -14.1475\n",
      "    - New mask: 52 features, Fitness: -18.9041\n",
      "    - New mask: 49 features, Fitness: -15.0359\n",
      "    - New mask: 49 features, Fitness: -17.8853\n",
      "    - New mask: 51 features, Fitness: -18.0355\n",
      "    - New mask: 52 features, Fitness: -19.0038\n",
      "    - New mask: 49 features, Fitness: -15.3247\n",
      "    - New mask: 47 features, Fitness: -14.9028\n",
      "    - New mask: 50 features, Fitness: -17.1617\n",
      "    - New mask: 46 features, Fitness: -12.3948\n",
      "    - New mask: 54 features, Fitness: -21.1623\n",
      "    - New mask: 55 features, Fitness: -22.9201\n",
      "    - New mask: 48 features, Fitness: -16.4548\n",
      "    - New mask: 47 features, Fitness: -15.6570\n",
      "    - New mask: 49 features, Fitness: -17.0722\n",
      "    - New mask: 46 features, Fitness: -13.0039\n",
      "    - New mask: 49 features, Fitness: -15.9861\n",
      "    - New mask: 51 features, Fitness: -17.9687\n",
      "    - New mask: 47 features, Fitness: -14.2038\n",
      "    - New mask: 57 features, Fitness: -23.5355\n",
      "    - New mask: 46 features, Fitness: -10.1481\n",
      "    - New mask: 49 features, Fitness: -11.4450\n",
      "    - New mask: 48 features, Fitness: -9.4979\n",
      "    - New mask: 52 features, Fitness: -13.2631\n",
      "    - New mask: 43 features, Fitness: -7.6544\n",
      "    - New mask: 50 features, Fitness: -13.0843\n",
      "    - New mask: 47 features, Fitness: -9.9429\n",
      "    - New mask: 48 features, Fitness: -10.5384\n",
      "    - New mask: 43 features, Fitness: -7.3849\n",
      "    - New mask: 46 features, Fitness: -9.8547\n",
      "    - New mask: 47 features, Fitness: -9.9273\n",
      "    - New mask: 44 features, Fitness: -9.5854\n",
      "    - New mask: 43 features, Fitness: -8.2839\n",
      "    - New mask: 48 features, Fitness: -10.6525\n",
      "    - New mask: 50 features, Fitness: -12.2945\n",
      "    - New mask: 48 features, Fitness: -10.3082\n",
      "    - New mask: 45 features, Fitness: -9.5153\n",
      "    - New mask: 46 features, Fitness: -9.3191\n",
      "    - New mask: 42 features, Fitness: -6.5296\n",
      "    - New mask: 46 features, Fitness: -9.7321\n",
      "    - New mask: 49 features, Fitness: -9.6564\n",
      "    - New mask: 47 features, Fitness: -8.6747\n",
      "    - New mask: 52 features, Fitness: -11.9013\n",
      "    - New mask: 49 features, Fitness: -9.6810\n",
      "    - New mask: 51 features, Fitness: -11.2126\n",
      "    - New mask: 46 features, Fitness: -7.0289\n",
      "    - New mask: 45 features, Fitness: -7.3431\n",
      "    - New mask: 47 features, Fitness: -9.7604\n",
      "    - New mask: 52 features, Fitness: -12.2027\n",
      "    - New mask: 47 features, Fitness: -8.3238\n",
      "    - New mask: 49 features, Fitness: -9.6578\n",
      "    - New mask: 48 features, Fitness: -9.0311\n",
      "    - New mask: 49 features, Fitness: -8.4683\n",
      "    - New mask: 48 features, Fitness: -8.8264\n",
      "    - New mask: 51 features, Fitness: -11.5152\n",
      "    - New mask: 47 features, Fitness: -8.3795\n",
      "    - New mask: 44 features, Fitness: -7.2956\n",
      "    - New mask: 48 features, Fitness: -8.5936\n",
      "    - New mask: 51 features, Fitness: -10.8063\n",
      "    - New mask: 48 features, Fitness: -8.8571\n",
      "    - New mask: 47 features, Fitness: -8.0436\n",
      "    - New mask: 48 features, Fitness: -6.9133\n",
      "    - New mask: 50 features, Fitness: -9.5625\n",
      "    - New mask: 49 features, Fitness: -7.7224\n",
      "    - New mask: 53 features, Fitness: -10.1158\n",
      "    - New mask: 51 features, Fitness: -8.8326\n",
      "    - New mask: 48 features, Fitness: -7.1980\n",
      "    - New mask: 46 features, Fitness: -6.3364\n",
      "    - New mask: 53 features, Fitness: -10.4098\n",
      "    - New mask: 48 features, Fitness: -6.6050\n",
      "    - New mask: 50 features, Fitness: -9.3194\n",
      "    - New mask: 47 features, Fitness: -6.8636\n",
      "    - New mask: 52 features, Fitness: -9.9928\n",
      "    - New mask: 46 features, Fitness: -5.1534\n",
      "    - New mask: 47 features, Fitness: -6.6775\n",
      "    - New mask: 50 features, Fitness: -8.0311\n",
      "    - New mask: 51 features, Fitness: -8.8400\n",
      "    - New mask: 51 features, Fitness: -9.9712\n",
      "    - New mask: 46 features, Fitness: -5.4226\n",
      "    - New mask: 48 features, Fitness: -7.5010\n",
      "    - New mask: 46 features, Fitness: 0.5944\n",
      "    - New mask: 44 features, Fitness: 0.7991\n",
      "    - New mask: 44 features, Fitness: -1.3746\n",
      "    - New mask: 46 features, Fitness: -0.3363\n",
      "    - New mask: 48 features, Fitness: -0.8354\n",
      "    - New mask: 50 features, Fitness: -0.9338\n",
      "    - New mask: 43 features, Fitness: 1.3494\n",
      "    - New mask: 53 features, Fitness: -3.5165\n",
      "    - New mask: 48 features, Fitness: -1.9118\n",
      "    - New mask: 50 features, Fitness: -1.7863\n",
      "    - New mask: 42 features, Fitness: 1.5132\n",
      "    - New mask: 45 features, Fitness: 0.8132\n",
      "    - New mask: 44 features, Fitness: 1.1094\n",
      "    - New mask: 48 features, Fitness: -1.4586\n",
      "    - New mask: 44 features, Fitness: -0.0336\n",
      "    - New mask: 47 features, Fitness: -0.0565\n",
      "    - New mask: 48 features, Fitness: -1.2521\n",
      "    - New mask: 48 features, Fitness: -1.2078\n",
      "    - New mask: 48 features, Fitness: -1.9747\n",
      "    - New mask: 48 features, Fitness: -1.1276\n",
      "    - New mask: 49 features, Fitness: -10.5578\n",
      "    - New mask: 48 features, Fitness: -9.2570\n",
      "    - New mask: 48 features, Fitness: -10.1828\n",
      "    - New mask: 52 features, Fitness: -12.1197\n",
      "    - New mask: 45 features, Fitness: -8.7936\n",
      "    - New mask: 52 features, Fitness: -12.2558\n",
      "    - New mask: 49 features, Fitness: -10.5760\n",
      "    - New mask: 49 features, Fitness: -10.3131\n",
      "    - New mask: 46 features, Fitness: -8.9037\n",
      "    - New mask: 51 features, Fitness: -11.8516\n",
      "    - New mask: 51 features, Fitness: -11.3213\n",
      "    - New mask: 49 features, Fitness: -10.4916\n",
      "    - New mask: 50 features, Fitness: -11.4009\n",
      "    - New mask: 48 features, Fitness: -8.9034\n",
      "    - New mask: 43 features, Fitness: -7.3335\n",
      "    - New mask: 46 features, Fitness: -9.1571\n",
      "    - New mask: 49 features, Fitness: -10.3611\n",
      "    - New mask: 49 features, Fitness: -11.6343\n",
      "    - New mask: 49 features, Fitness: -10.5896\n",
      "    - New mask: 41 features, Fitness: -5.4979\n",
      "    - New mask: 49 features, Fitness: -9.8569\n",
      "    - New mask: 49 features, Fitness: -10.3636\n",
      "    - New mask: 50 features, Fitness: -10.2400\n",
      "    - New mask: 49 features, Fitness: -10.0155\n",
      "    - New mask: 49 features, Fitness: -9.8595\n",
      "    - New mask: 49 features, Fitness: -11.0440\n",
      "    - New mask: 51 features, Fitness: -10.8025\n",
      "    - New mask: 46 features, Fitness: -8.8258\n",
      "    - New mask: 49 features, Fitness: -10.6786\n",
      "    - New mask: 51 features, Fitness: -10.9707\n",
      "    - New mask: 51 features, Fitness: -11.0284\n",
      "    - New mask: 47 features, Fitness: -8.5279\n",
      "    - New mask: 46 features, Fitness: -8.5283\n",
      "    - New mask: 49 features, Fitness: -9.7920\n",
      "    - New mask: 48 features, Fitness: -9.8700\n",
      "    - New mask: 50 features, Fitness: -11.2305\n",
      "    - New mask: 48 features, Fitness: -9.5124\n",
      "    - New mask: 47 features, Fitness: -9.4957\n",
      "    - New mask: 49 features, Fitness: -11.0720\n",
      "    - New mask: 48 features, Fitness: -9.4020\n",
      "    - New mask: 46 features, Fitness: -2.8111\n",
      "    - New mask: 44 features, Fitness: -1.8461\n",
      "    - New mask: 46 features, Fitness: -4.3008\n",
      "    - New mask: 47 features, Fitness: -4.7580\n",
      "    - New mask: 47 features, Fitness: -3.9383\n",
      "    - New mask: 48 features, Fitness: -3.6305\n",
      "    - New mask: 51 features, Fitness: -5.9305\n",
      "    - New mask: 47 features, Fitness: -3.5807\n",
      "    - New mask: 49 features, Fitness: -5.0486\n",
      "    - New mask: 46 features, Fitness: -3.6515\n",
      "    - New mask: 47 features, Fitness: -4.7866\n",
      "    - New mask: 48 features, Fitness: -4.4001\n",
      "    - New mask: 47 features, Fitness: -4.4962\n",
      "    - New mask: 48 features, Fitness: -4.7802\n",
      "    - New mask: 47 features, Fitness: -4.3392\n",
      "    - New mask: 47 features, Fitness: -3.5395\n",
      "    - New mask: 51 features, Fitness: -5.7056\n",
      "    - New mask: 46 features, Fitness: -3.8087\n",
      "    - New mask: 48 features, Fitness: -4.0728\n",
      "    - New mask: 47 features, Fitness: -3.2006\n",
      "    - New mask: 45 features, Fitness: -7.0379\n",
      "    - New mask: 49 features, Fitness: -9.3006\n",
      "    - New mask: 49 features, Fitness: -9.1950\n",
      "    - New mask: 50 features, Fitness: -9.8502\n",
      "    - New mask: 47 features, Fitness: -7.4996\n",
      "    - New mask: 49 features, Fitness: -8.3824\n",
      "    - New mask: 50 features, Fitness: -9.5158\n",
      "    - New mask: 48 features, Fitness: -8.5096\n",
      "    - New mask: 46 features, Fitness: -6.9709\n",
      "    - New mask: 52 features, Fitness: -11.8568\n",
      "    - New mask: 47 features, Fitness: -7.3978\n",
      "    - New mask: 49 features, Fitness: -9.6627\n",
      "    - New mask: 45 features, Fitness: -6.3823\n",
      "    - New mask: 49 features, Fitness: -9.0891\n",
      "    - New mask: 48 features, Fitness: -8.5353\n",
      "    - New mask: 51 features, Fitness: -11.0156\n",
      "    - New mask: 45 features, Fitness: -6.5223\n",
      "    - New mask: 48 features, Fitness: -8.6054\n",
      "    - New mask: 47 features, Fitness: -7.9328\n",
      "    - New mask: 49 features, Fitness: -9.0224\n",
      "    - New mask: 43 features, Fitness: -8.5093\n",
      "    - New mask: 49 features, Fitness: -11.7840\n",
      "    - New mask: 52 features, Fitness: -14.4259\n",
      "    - New mask: 48 features, Fitness: -10.6787\n",
      "    - New mask: 48 features, Fitness: -10.2447\n",
      "    - New mask: 46 features, Fitness: -9.6535\n",
      "    - New mask: 47 features, Fitness: -10.1654\n",
      "    - New mask: 51 features, Fitness: -12.9023\n",
      "    - New mask: 51 features, Fitness: -13.1939\n",
      "    - New mask: 51 features, Fitness: -12.6379\n",
      "    - New mask: 48 features, Fitness: -11.1046\n",
      "    - New mask: 49 features, Fitness: -11.8235\n",
      "    - New mask: 48 features, Fitness: -10.6412\n",
      "    - New mask: 49 features, Fitness: -11.0870\n",
      "    - New mask: 48 features, Fitness: -11.3694\n",
      "    - New mask: 51 features, Fitness: -13.6833\n",
      "    - New mask: 44 features, Fitness: -9.0778\n",
      "    - New mask: 45 features, Fitness: -8.9237\n",
      "    - New mask: 45 features, Fitness: -8.7083\n",
      "    - New mask: 47 features, Fitness: -9.4678\n",
      "=== End of Round 16: Vote mask selects 49 features (rho: 0.67)\n",
      "    Indices: [0, 1, 2, 3, 4, 6, 7, 8, 10, 12, 13, 14, 17, 19, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 38, 39, 40, 41, 42, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 58, 59, 60, 61, 62, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 17 ================\n",
      "  Adaptive rho for this round: 0.71\n",
      "    - New mask: 47 features, Fitness: -14.0747\n",
      "    - New mask: 51 features, Fitness: -17.2903\n",
      "    - New mask: 48 features, Fitness: -13.8975\n",
      "    - New mask: 46 features, Fitness: -14.4588\n",
      "    - New mask: 48 features, Fitness: -14.5769\n",
      "    - New mask: 51 features, Fitness: -17.7743\n",
      "    - New mask: 47 features, Fitness: -14.8264\n",
      "    - New mask: 47 features, Fitness: -13.3409\n",
      "    - New mask: 48 features, Fitness: -13.9830\n",
      "    - New mask: 47 features, Fitness: -14.0860\n",
      "    - New mask: 50 features, Fitness: -16.5169\n",
      "    - New mask: 52 features, Fitness: -18.0213\n",
      "    - New mask: 50 features, Fitness: -17.1867\n",
      "    - New mask: 45 features, Fitness: -12.6718\n",
      "    - New mask: 45 features, Fitness: -12.8710\n",
      "    - New mask: 46 features, Fitness: -13.7123\n",
      "    - New mask: 52 features, Fitness: -18.9002\n",
      "    - New mask: 49 features, Fitness: -15.7184\n",
      "    - New mask: 47 features, Fitness: -14.0960\n",
      "    - New mask: 49 features, Fitness: -16.4614\n",
      "    - New mask: 52 features, Fitness: -13.5441\n",
      "    - New mask: 45 features, Fitness: -9.2509\n",
      "    - New mask: 43 features, Fitness: -7.2853\n",
      "    - New mask: 47 features, Fitness: -9.3872\n",
      "    - New mask: 45 features, Fitness: -8.5753\n",
      "    - New mask: 44 features, Fitness: -7.7778\n",
      "    - New mask: 46 features, Fitness: -8.7711\n",
      "    - New mask: 47 features, Fitness: -9.8934\n",
      "    - New mask: 46 features, Fitness: -9.2931\n",
      "    - New mask: 39 features, Fitness: -5.6706\n",
      "    - New mask: 46 features, Fitness: -9.1442\n",
      "    - New mask: 47 features, Fitness: -10.0754\n",
      "    - New mask: 44 features, Fitness: -8.1299\n",
      "    - New mask: 48 features, Fitness: -9.9327\n",
      "    - New mask: 46 features, Fitness: -9.1396\n",
      "    - New mask: 50 features, Fitness: -11.7926\n",
      "    - New mask: 46 features, Fitness: -10.7983\n",
      "    - New mask: 45 features, Fitness: -9.1186\n",
      "    - New mask: 44 features, Fitness: -8.2940\n",
      "    - New mask: 42 features, Fitness: -6.5299\n",
      "    - New mask: 48 features, Fitness: -8.9540\n",
      "    - New mask: 44 features, Fitness: -7.1124\n",
      "    - New mask: 50 features, Fitness: -10.8865\n",
      "    - New mask: 49 features, Fitness: -9.8073\n",
      "    - New mask: 46 features, Fitness: -6.6296\n",
      "    - New mask: 46 features, Fitness: -7.2283\n",
      "    - New mask: 46 features, Fitness: -7.6754\n",
      "    - New mask: 43 features, Fitness: -6.0088\n",
      "    - New mask: 48 features, Fitness: -9.8005\n",
      "    - New mask: 46 features, Fitness: -6.8400\n",
      "    - New mask: 49 features, Fitness: -8.7680\n",
      "    - New mask: 45 features, Fitness: -6.7297\n",
      "    - New mask: 48 features, Fitness: -9.3283\n",
      "    - New mask: 44 features, Fitness: -5.8444\n",
      "    - New mask: 47 features, Fitness: -8.1661\n",
      "    - New mask: 50 features, Fitness: -10.1572\n",
      "    - New mask: 48 features, Fitness: -10.6558\n",
      "    - New mask: 47 features, Fitness: -8.6764\n",
      "    - New mask: 45 features, Fitness: -6.7639\n",
      "    - New mask: 47 features, Fitness: -7.2355\n",
      "    - New mask: 51 features, Fitness: -8.6681\n",
      "    - New mask: 51 features, Fitness: -9.0829\n",
      "    - New mask: 46 features, Fitness: -6.5685\n",
      "    - New mask: 49 features, Fitness: -7.4269\n",
      "    - New mask: 46 features, Fitness: -5.5737\n",
      "    - New mask: 45 features, Fitness: -4.7329\n",
      "    - New mask: 47 features, Fitness: -5.5494\n",
      "    - New mask: 48 features, Fitness: -7.4627\n",
      "    - New mask: 48 features, Fitness: -7.4653\n",
      "    - New mask: 45 features, Fitness: -5.4475\n",
      "    - New mask: 43 features, Fitness: -3.6691\n",
      "    - New mask: 46 features, Fitness: -5.2692\n",
      "    - New mask: 51 features, Fitness: -8.4309\n",
      "    - New mask: 45 features, Fitness: -4.2814\n",
      "    - New mask: 50 features, Fitness: -7.7332\n",
      "    - New mask: 49 features, Fitness: -7.2059\n",
      "    - New mask: 48 features, Fitness: -7.0763\n",
      "    - New mask: 47 features, Fitness: -6.1989\n",
      "    - New mask: 51 features, Fitness: -8.5968\n",
      "    - New mask: 47 features, Fitness: -5.7461\n",
      "    - New mask: 44 features, Fitness: 1.0737\n",
      "    - New mask: 47 features, Fitness: -1.0568\n",
      "    - New mask: 44 features, Fitness: 1.1006\n",
      "    - New mask: 42 features, Fitness: -0.4859\n",
      "    - New mask: 49 features, Fitness: -0.2968\n",
      "    - New mask: 46 features, Fitness: 0.8328\n",
      "    - New mask: 44 features, Fitness: 1.1884\n",
      "    - New mask: 42 features, Fitness: 0.9335\n",
      "    - New mask: 45 features, Fitness: 0.2335\n",
      "    - New mask: 45 features, Fitness: -1.7931\n",
      "    - New mask: 46 features, Fitness: -0.4376\n",
      "    - New mask: 50 features, Fitness: -1.3875\n",
      "    - New mask: 44 features, Fitness: 1.0031\n",
      "    - New mask: 48 features, Fitness: -1.1260\n",
      "    - New mask: 48 features, Fitness: 0.5048\n",
      "    - New mask: 44 features, Fitness: 0.3208\n",
      "    - New mask: 39 features, Fitness: 1.6745\n",
      "    - New mask: 46 features, Fitness: -0.3518\n",
      "    - New mask: 42 features, Fitness: 0.5750\n",
      "    - New mask: 43 features, Fitness: 0.2283\n",
      "    - New mask: 48 features, Fitness: -10.4752\n",
      "    - New mask: 46 features, Fitness: -9.3448\n",
      "    - New mask: 48 features, Fitness: -10.2410\n",
      "    - New mask: 48 features, Fitness: -9.0424\n",
      "    - New mask: 45 features, Fitness: -8.5094\n",
      "    - New mask: 47 features, Fitness: -9.5077\n",
      "    - New mask: 44 features, Fitness: -8.2863\n",
      "    - New mask: 42 features, Fitness: -7.3081\n",
      "    - New mask: 47 features, Fitness: -8.9822\n",
      "    - New mask: 47 features, Fitness: -8.6801\n",
      "    - New mask: 47 features, Fitness: -9.2566\n",
      "    - New mask: 47 features, Fitness: -8.0091\n",
      "    - New mask: 44 features, Fitness: -8.6345\n",
      "    - New mask: 47 features, Fitness: -8.6913\n",
      "    - New mask: 47 features, Fitness: -9.6529\n",
      "    - New mask: 43 features, Fitness: -6.8231\n",
      "    - New mask: 47 features, Fitness: -8.6835\n",
      "    - New mask: 46 features, Fitness: -9.0618\n",
      "    - New mask: 43 features, Fitness: -7.4745\n",
      "    - New mask: 42 features, Fitness: -6.6032\n",
      "    - New mask: 48 features, Fitness: -9.1512\n",
      "    - New mask: 51 features, Fitness: -10.9182\n",
      "    - New mask: 47 features, Fitness: -8.3217\n",
      "    - New mask: 48 features, Fitness: -9.1512\n",
      "    - New mask: 47 features, Fitness: -8.7427\n",
      "    - New mask: 43 features, Fitness: -6.2254\n",
      "    - New mask: 48 features, Fitness: -9.4055\n",
      "    - New mask: 49 features, Fitness: -9.8912\n",
      "    - New mask: 47 features, Fitness: -9.4258\n",
      "    - New mask: 47 features, Fitness: -8.3756\n",
      "    - New mask: 49 features, Fitness: -10.0173\n",
      "    - New mask: 42 features, Fitness: -6.6347\n",
      "    - New mask: 46 features, Fitness: -8.9177\n",
      "    - New mask: 48 features, Fitness: -9.3340\n",
      "    - New mask: 46 features, Fitness: -8.9984\n",
      "    - New mask: 50 features, Fitness: -10.4158\n",
      "    - New mask: 46 features, Fitness: -8.3150\n",
      "    - New mask: 48 features, Fitness: -9.5152\n",
      "    - New mask: 46 features, Fitness: -8.0918\n",
      "    - New mask: 45 features, Fitness: -7.3191\n",
      "    - New mask: 45 features, Fitness: -2.0553\n",
      "    - New mask: 45 features, Fitness: -2.3548\n",
      "    - New mask: 44 features, Fitness: -2.8383\n",
      "    - New mask: 47 features, Fitness: -4.0438\n",
      "    - New mask: 44 features, Fitness: -3.2293\n",
      "    - New mask: 47 features, Fitness: -2.9264\n",
      "    - New mask: 48 features, Fitness: -4.2375\n",
      "    - New mask: 44 features, Fitness: -2.3925\n",
      "    - New mask: 46 features, Fitness: -3.4455\n",
      "    - New mask: 43 features, Fitness: -2.1082\n",
      "    - New mask: 48 features, Fitness: -3.9375\n",
      "    - New mask: 45 features, Fitness: -3.7751\n",
      "    - New mask: 43 features, Fitness: -1.8843\n",
      "    - New mask: 48 features, Fitness: -4.4785\n",
      "    - New mask: 47 features, Fitness: -3.5644\n",
      "    - New mask: 45 features, Fitness: -2.9033\n",
      "    - New mask: 48 features, Fitness: -3.4158\n",
      "    - New mask: 47 features, Fitness: -3.6998\n",
      "    - New mask: 49 features, Fitness: -3.6447\n",
      "    - New mask: 45 features, Fitness: -3.6130\n",
      "    - New mask: 47 features, Fitness: -7.5093\n",
      "    - New mask: 49 features, Fitness: -9.2981\n",
      "    - New mask: 48 features, Fitness: -8.4592\n",
      "    - New mask: 44 features, Fitness: -6.1350\n",
      "    - New mask: 49 features, Fitness: -9.1238\n",
      "    - New mask: 46 features, Fitness: -7.3903\n",
      "    - New mask: 48 features, Fitness: -7.6550\n",
      "    - New mask: 48 features, Fitness: -8.5367\n",
      "    - New mask: 42 features, Fitness: -4.4359\n",
      "    - New mask: 48 features, Fitness: -8.2463\n",
      "    - New mask: 44 features, Fitness: -6.1760\n",
      "    - New mask: 48 features, Fitness: -8.9907\n",
      "    - New mask: 46 features, Fitness: -6.4645\n",
      "    - New mask: 46 features, Fitness: -7.1032\n",
      "    - New mask: 45 features, Fitness: -6.9676\n",
      "    - New mask: 48 features, Fitness: -7.9567\n",
      "    - New mask: 44 features, Fitness: -5.9289\n",
      "    - New mask: 46 features, Fitness: -7.2081\n",
      "    - New mask: 45 features, Fitness: -7.1865\n",
      "    - New mask: 45 features, Fitness: -6.7194\n",
      "    - New mask: 43 features, Fitness: -7.9687\n",
      "    - New mask: 45 features, Fitness: -9.0488\n",
      "    - New mask: 48 features, Fitness: -11.7816\n",
      "    - New mask: 44 features, Fitness: -8.7293\n",
      "    - New mask: 47 features, Fitness: -10.0826\n",
      "    - New mask: 45 features, Fitness: -8.5075\n",
      "    - New mask: 45 features, Fitness: -9.2558\n",
      "    - New mask: 49 features, Fitness: -11.2183\n",
      "    - New mask: 48 features, Fitness: -11.1695\n",
      "    - New mask: 44 features, Fitness: -8.2850\n",
      "    - New mask: 49 features, Fitness: -10.6653\n",
      "    - New mask: 44 features, Fitness: -8.4690\n",
      "    - New mask: 46 features, Fitness: -9.7360\n",
      "    - New mask: 47 features, Fitness: -11.1562\n",
      "    - New mask: 49 features, Fitness: -10.7977\n",
      "    - New mask: 45 features, Fitness: -9.3830\n",
      "    - New mask: 45 features, Fitness: -10.0758\n",
      "    - New mask: 43 features, Fitness: -8.7850\n",
      "    - New mask: 41 features, Fitness: -8.3269\n",
      "    - New mask: 42 features, Fitness: -7.8378\n",
      "=== End of Round 17: Vote mask selects 35 features (rho: 0.71)\n",
      "    Indices: [1, 2, 3, 4, 6, 8, 12, 13, 14, 19, 24, 26, 28, 29, 30, 32, 33, 34, 40, 41, 42, 45, 46, 47, 48, 50, 52, 54, 58, 59, 60, 62, 64, 65, 68]\n",
      "\n",
      "================ Federated BFA Round 18 ================\n",
      "  Adaptive rho for this round: 0.74\n",
      "    - New mask: 45 features, Fitness: -12.2660\n",
      "    - New mask: 45 features, Fitness: -12.3671\n",
      "    - New mask: 42 features, Fitness: -9.7005\n",
      "    - New mask: 43 features, Fitness: -13.3560\n",
      "    - New mask: 42 features, Fitness: -10.2832\n",
      "    - New mask: 44 features, Fitness: -14.6500\n",
      "    - New mask: 42 features, Fitness: -10.6673\n",
      "    - New mask: 43 features, Fitness: -10.1954\n",
      "    - New mask: 38 features, Fitness: -7.6158\n",
      "    - New mask: 45 features, Fitness: -13.2684\n",
      "    - New mask: 41 features, Fitness: -10.0996\n",
      "    - New mask: 45 features, Fitness: -12.6126\n",
      "    - New mask: 48 features, Fitness: -15.7775\n",
      "    - New mask: 42 features, Fitness: -12.6546\n",
      "    - New mask: 41 features, Fitness: -10.7522\n",
      "    - New mask: 45 features, Fitness: -12.3952\n",
      "    - New mask: 46 features, Fitness: -14.3138\n",
      "    - New mask: 41 features, Fitness: -10.1409\n",
      "    - New mask: 42 features, Fitness: -10.4419\n",
      "    - New mask: 41 features, Fitness: -11.3863\n",
      "    - New mask: 39 features, Fitness: -4.9295\n",
      "    - New mask: 42 features, Fitness: -7.3691\n",
      "    - New mask: 37 features, Fitness: -4.4461\n",
      "    - New mask: 45 features, Fitness: -8.7852\n",
      "    - New mask: 37 features, Fitness: -4.1051\n",
      "    - New mask: 41 features, Fitness: -6.0765\n",
      "    - New mask: 42 features, Fitness: -6.2892\n",
      "    - New mask: 40 features, Fitness: -5.6526\n",
      "    - New mask: 40 features, Fitness: -5.6610\n",
      "    - New mask: 36 features, Fitness: -3.8744\n",
      "    - New mask: 40 features, Fitness: -5.5344\n",
      "    - New mask: 41 features, Fitness: -6.2323\n",
      "    - New mask: 37 features, Fitness: -4.3387\n",
      "    - New mask: 40 features, Fitness: -5.8527\n",
      "    - New mask: 42 features, Fitness: -7.2015\n",
      "    - New mask: 39 features, Fitness: -5.6421\n",
      "    - New mask: 50 features, Fitness: -11.0989\n",
      "    - New mask: 42 features, Fitness: -6.2179\n",
      "    - New mask: 40 features, Fitness: -5.5906\n",
      "    - New mask: 37 features, Fitness: -4.1909\n",
      "    - New mask: 44 features, Fitness: -5.8858\n",
      "    - New mask: 44 features, Fitness: -6.8838\n",
      "    - New mask: 46 features, Fitness: -7.4997\n",
      "    - New mask: 43 features, Fitness: -5.8392\n",
      "    - New mask: 41 features, Fitness: -4.1516\n",
      "    - New mask: 43 features, Fitness: -5.9457\n",
      "    - New mask: 43 features, Fitness: -5.2664\n",
      "    - New mask: 37 features, Fitness: -2.8302\n",
      "    - New mask: 42 features, Fitness: -5.3718\n",
      "    - New mask: 38 features, Fitness: -4.0245\n",
      "    - New mask: 42 features, Fitness: -4.6780\n",
      "    - New mask: 40 features, Fitness: -3.4629\n",
      "    - New mask: 42 features, Fitness: -5.7354\n",
      "    - New mask: 42 features, Fitness: -5.3308\n",
      "    - New mask: 45 features, Fitness: -6.6230\n",
      "    - New mask: 43 features, Fitness: -5.0422\n",
      "    - New mask: 40 features, Fitness: -4.5895\n",
      "    - New mask: 43 features, Fitness: -5.3013\n",
      "    - New mask: 44 features, Fitness: -5.7248\n",
      "    - New mask: 40 features, Fitness: -3.0644\n",
      "    - New mask: 44 features, Fitness: -4.6146\n",
      "    - New mask: 39 features, Fitness: -1.9142\n",
      "    - New mask: 43 features, Fitness: -4.4700\n",
      "    - New mask: 46 features, Fitness: -5.2598\n",
      "    - New mask: 43 features, Fitness: -3.6811\n",
      "    - New mask: 40 features, Fitness: -2.6111\n",
      "    - New mask: 43 features, Fitness: -3.4264\n",
      "    - New mask: 40 features, Fitness: -3.4902\n",
      "    - New mask: 40 features, Fitness: -2.9587\n",
      "    - New mask: 39 features, Fitness: -2.1757\n",
      "    - New mask: 43 features, Fitness: -3.3445\n",
      "    - New mask: 42 features, Fitness: -3.1194\n",
      "    - New mask: 45 features, Fitness: -5.0762\n",
      "    - New mask: 43 features, Fitness: -4.0247\n",
      "    - New mask: 41 features, Fitness: -3.5698\n",
      "    - New mask: 39 features, Fitness: -2.2507\n",
      "    - New mask: 39 features, Fitness: -2.3445\n",
      "    - New mask: 42 features, Fitness: -3.4899\n",
      "    - New mask: 42 features, Fitness: -3.6754\n",
      "    - New mask: 42 features, Fitness: -3.5224\n",
      "    - New mask: 36 features, Fitness: 2.1369\n",
      "    - New mask: 42 features, Fitness: 0.0033\n",
      "    - New mask: 39 features, Fitness: 2.0703\n",
      "    - New mask: 42 features, Fitness: -0.1183\n",
      "    - New mask: 42 features, Fitness: 2.2028\n",
      "    - New mask: 42 features, Fitness: 2.0343\n",
      "    - New mask: 42 features, Fitness: -0.4687\n",
      "    - New mask: 33 features, Fitness: 3.6381\n",
      "    - New mask: 36 features, Fitness: 2.4063\n",
      "    - New mask: 40 features, Fitness: 1.6164\n",
      "    - New mask: 42 features, Fitness: 0.0715\n",
      "    - New mask: 42 features, Fitness: 0.1025\n",
      "    - New mask: 42 features, Fitness: 1.6296\n",
      "    - New mask: 37 features, Fitness: 2.8207\n",
      "    - New mask: 43 features, Fitness: 0.5720\n",
      "    - New mask: 33 features, Fitness: 2.9329\n",
      "    - New mask: 38 features, Fitness: 1.3698\n",
      "    - New mask: 40 features, Fitness: 0.6444\n",
      "    - New mask: 41 features, Fitness: 0.4902\n",
      "    - New mask: 41 features, Fitness: 0.0617\n",
      "    - New mask: 44 features, Fitness: -7.7800\n",
      "    - New mask: 41 features, Fitness: -5.7503\n",
      "    - New mask: 42 features, Fitness: -6.2385\n",
      "    - New mask: 45 features, Fitness: -7.0394\n",
      "    - New mask: 45 features, Fitness: -7.5996\n",
      "    - New mask: 40 features, Fitness: -6.4572\n",
      "    - New mask: 42 features, Fitness: -5.8480\n",
      "    - New mask: 34 features, Fitness: -2.6000\n",
      "    - New mask: 41 features, Fitness: -6.2678\n",
      "    - New mask: 39 features, Fitness: -3.8089\n",
      "    - New mask: 40 features, Fitness: -4.9637\n",
      "    - New mask: 40 features, Fitness: -5.2208\n",
      "    - New mask: 41 features, Fitness: -5.6729\n",
      "    - New mask: 40 features, Fitness: -5.4083\n",
      "    - New mask: 45 features, Fitness: -8.8085\n",
      "    - New mask: 43 features, Fitness: -7.6952\n",
      "    - New mask: 41 features, Fitness: -6.0362\n",
      "    - New mask: 37 features, Fitness: -4.7505\n",
      "    - New mask: 41 features, Fitness: -6.7283\n",
      "    - New mask: 41 features, Fitness: -5.7691\n",
      "    - New mask: 38 features, Fitness: -4.3311\n",
      "    - New mask: 44 features, Fitness: -7.2917\n",
      "    - New mask: 40 features, Fitness: -4.9788\n",
      "    - New mask: 41 features, Fitness: -5.1029\n",
      "    - New mask: 42 features, Fitness: -6.3379\n",
      "    - New mask: 44 features, Fitness: -6.5929\n",
      "    - New mask: 41 features, Fitness: -5.3805\n",
      "    - New mask: 47 features, Fitness: -8.6505\n",
      "    - New mask: 42 features, Fitness: -5.3862\n",
      "    - New mask: 44 features, Fitness: -6.8950\n",
      "    - New mask: 42 features, Fitness: -6.2031\n",
      "    - New mask: 39 features, Fitness: -5.7619\n",
      "    - New mask: 40 features, Fitness: -5.4713\n",
      "    - New mask: 40 features, Fitness: -4.1969\n",
      "    - New mask: 45 features, Fitness: -7.5806\n",
      "    - New mask: 41 features, Fitness: -5.6249\n",
      "    - New mask: 41 features, Fitness: -4.7340\n",
      "    - New mask: 40 features, Fitness: -5.4015\n",
      "    - New mask: 46 features, Fitness: -7.8713\n",
      "    - New mask: 41 features, Fitness: -5.1982\n",
      "    - New mask: 38 features, Fitness: -1.0181\n",
      "    - New mask: 38 features, Fitness: -0.1887\n",
      "    - New mask: 40 features, Fitness: -1.4204\n",
      "    - New mask: 43 features, Fitness: -2.3964\n",
      "    - New mask: 42 features, Fitness: -2.8356\n",
      "    - New mask: 40 features, Fitness: -0.3092\n",
      "    - New mask: 40 features, Fitness: -1.7872\n",
      "    - New mask: 37 features, Fitness: -0.7200\n",
      "    - New mask: 44 features, Fitness: -2.6268\n",
      "    - New mask: 38 features, Fitness: 0.2030\n",
      "    - New mask: 43 features, Fitness: -1.6488\n",
      "    - New mask: 43 features, Fitness: -3.6312\n",
      "    - New mask: 38 features, Fitness: -0.1718\n",
      "    - New mask: 48 features, Fitness: -4.2808\n",
      "    - New mask: 43 features, Fitness: -1.6762\n",
      "    - New mask: 41 features, Fitness: -1.7212\n",
      "    - New mask: 38 features, Fitness: -1.2647\n",
      "    - New mask: 43 features, Fitness: -2.1297\n",
      "    - New mask: 41 features, Fitness: -1.8633\n",
      "    - New mask: 40 features, Fitness: -1.4923\n",
      "    - New mask: 41 features, Fitness: -3.8448\n",
      "    - New mask: 42 features, Fitness: -4.8915\n",
      "    - New mask: 43 features, Fitness: -5.2202\n",
      "    - New mask: 39 features, Fitness: -3.0516\n",
      "    - New mask: 41 features, Fitness: -4.4269\n",
      "    - New mask: 40 features, Fitness: -4.8303\n",
      "    - New mask: 44 features, Fitness: -5.2846\n",
      "    - New mask: 44 features, Fitness: -5.4831\n",
      "    - New mask: 42 features, Fitness: -4.2054\n",
      "    - New mask: 44 features, Fitness: -6.0120\n",
      "    - New mask: 40 features, Fitness: -4.3473\n",
      "    - New mask: 40 features, Fitness: -4.3951\n",
      "    - New mask: 40 features, Fitness: -4.2935\n",
      "    - New mask: 37 features, Fitness: -2.8218\n",
      "    - New mask: 43 features, Fitness: -6.3261\n",
      "    - New mask: 45 features, Fitness: -5.9121\n",
      "    - New mask: 40 features, Fitness: -3.8725\n",
      "    - New mask: 43 features, Fitness: -5.0714\n",
      "    - New mask: 40 features, Fitness: -3.7332\n",
      "    - New mask: 41 features, Fitness: -4.4588\n",
      "    - New mask: 42 features, Fitness: -7.8513\n",
      "    - New mask: 40 features, Fitness: -5.7800\n",
      "    - New mask: 42 features, Fitness: -6.7128\n",
      "    - New mask: 40 features, Fitness: -6.3858\n",
      "    - New mask: 40 features, Fitness: -6.8620\n",
      "    - New mask: 45 features, Fitness: -9.1712\n",
      "    - New mask: 39 features, Fitness: -5.5771\n",
      "    - New mask: 41 features, Fitness: -7.9901\n",
      "    - New mask: 40 features, Fitness: -5.2778\n",
      "    - New mask: 36 features, Fitness: -4.1384\n",
      "    - New mask: 42 features, Fitness: -7.5186\n",
      "    - New mask: 40 features, Fitness: -6.5430\n",
      "    - New mask: 47 features, Fitness: -10.0268\n",
      "    - New mask: 40 features, Fitness: -6.6725\n",
      "    - New mask: 40 features, Fitness: -7.0893\n",
      "    - New mask: 39 features, Fitness: -6.1284\n",
      "    - New mask: 43 features, Fitness: -7.8739\n",
      "    - New mask: 40 features, Fitness: -6.2038\n",
      "    - New mask: 39 features, Fitness: -5.7061\n",
      "    - New mask: 39 features, Fitness: -6.1313\n",
      "=== End of Round 18: Vote mask selects 31 features (rho: 0.74)\n",
      "    Indices: [2, 3, 4, 6, 8, 12, 13, 14, 19, 24, 26, 28, 29, 30, 33, 34, 40, 41, 42, 45, 46, 47, 48, 50, 54, 58, 59, 60, 64, 65, 68]\n",
      "\n",
      "================ Federated BFA Round 19 ================\n",
      "  Adaptive rho for this round: 0.77\n",
      "    - New mask: 36 features, Fitness: -6.2222\n",
      "    - New mask: 37 features, Fitness: -6.3578\n",
      "    - New mask: 39 features, Fitness: -8.0670\n",
      "    - New mask: 40 features, Fitness: -10.3164\n",
      "    - New mask: 37 features, Fitness: -7.8269\n",
      "    - New mask: 39 features, Fitness: -9.5830\n",
      "    - New mask: 41 features, Fitness: -9.8568\n",
      "    - New mask: 39 features, Fitness: -9.2667\n",
      "    - New mask: 31 features, Fitness: -4.9918\n",
      "    - New mask: 38 features, Fitness: -8.3180\n",
      "    - New mask: 35 features, Fitness: -7.0972\n",
      "    - New mask: 37 features, Fitness: -6.4935\n",
      "    - New mask: 39 features, Fitness: -8.6618\n",
      "    - New mask: 38 features, Fitness: -9.0755\n",
      "    - New mask: 37 features, Fitness: -7.6112\n",
      "    - New mask: 40 features, Fitness: -8.4031\n",
      "    - New mask: 38 features, Fitness: -8.0501\n",
      "    - New mask: 33 features, Fitness: -5.3487\n",
      "    - New mask: 39 features, Fitness: -7.7518\n",
      "    - New mask: 40 features, Fitness: -10.0801\n",
      "    - New mask: 32 features, Fitness: -2.2459\n",
      "    - New mask: 34 features, Fitness: -3.1143\n",
      "    - New mask: 34 features, Fitness: -3.1785\n",
      "    - New mask: 38 features, Fitness: -4.4641\n",
      "    - New mask: 33 features, Fitness: -3.5849\n",
      "    - New mask: 38 features, Fitness: -4.2508\n",
      "    - New mask: 40 features, Fitness: -5.6771\n",
      "    - New mask: 35 features, Fitness: -3.3539\n",
      "    - New mask: 34 features, Fitness: -2.9779\n",
      "    - New mask: 35 features, Fitness: -3.6033\n",
      "    - New mask: 34 features, Fitness: -2.8503\n",
      "    - New mask: 37 features, Fitness: -4.7026\n",
      "    - New mask: 35 features, Fitness: -3.6062\n",
      "    - New mask: 32 features, Fitness: -2.6160\n",
      "    - New mask: 42 features, Fitness: -6.1162\n",
      "    - New mask: 36 features, Fitness: -3.6123\n",
      "    - New mask: 39 features, Fitness: -5.4609\n",
      "    - New mask: 35 features, Fitness: -3.0445\n",
      "    - New mask: 41 features, Fitness: -6.3339\n",
      "    - New mask: 32 features, Fitness: -2.2945\n",
      "    - New mask: 39 features, Fitness: -2.7116\n",
      "    - New mask: 36 features, Fitness: -4.8577\n",
      "    - New mask: 34 features, Fitness: -2.3874\n",
      "    - New mask: 36 features, Fitness: -2.1678\n",
      "    - New mask: 33 features, Fitness: -1.7207\n",
      "    - New mask: 35 features, Fitness: -2.8593\n",
      "    - New mask: 38 features, Fitness: -2.9188\n",
      "    - New mask: 38 features, Fitness: -4.1848\n",
      "    - New mask: 38 features, Fitness: -3.4711\n",
      "    - New mask: 33 features, Fitness: -1.5774\n",
      "    - New mask: 41 features, Fitness: -4.8241\n",
      "    - New mask: 38 features, Fitness: -2.9299\n",
      "    - New mask: 35 features, Fitness: -4.0467\n",
      "    - New mask: 35 features, Fitness: -2.3888\n",
      "    - New mask: 39 features, Fitness: -3.9318\n",
      "    - New mask: 34 features, Fitness: -2.8913\n",
      "    - New mask: 40 features, Fitness: -4.9546\n",
      "    - New mask: 36 features, Fitness: -2.5636\n",
      "    - New mask: 33 features, Fitness: -2.0088\n",
      "    - New mask: 38 features, Fitness: -2.6614\n",
      "    - New mask: 37 features, Fitness: -2.0123\n",
      "    - New mask: 36 features, Fitness: -0.6483\n",
      "    - New mask: 39 features, Fitness: -2.3410\n",
      "    - New mask: 38 features, Fitness: -1.2401\n",
      "    - New mask: 38 features, Fitness: -1.6737\n",
      "    - New mask: 37 features, Fitness: -1.1989\n",
      "    - New mask: 42 features, Fitness: -4.0939\n",
      "    - New mask: 35 features, Fitness: -0.5810\n",
      "    - New mask: 36 features, Fitness: -1.1958\n",
      "    - New mask: 37 features, Fitness: -1.8054\n",
      "    - New mask: 39 features, Fitness: -2.3888\n",
      "    - New mask: 35 features, Fitness: -0.5159\n",
      "    - New mask: 44 features, Fitness: -4.7306\n",
      "    - New mask: 35 features, Fitness: -1.2416\n",
      "    - New mask: 36 features, Fitness: -1.9307\n",
      "    - New mask: 33 features, Fitness: -0.0680\n",
      "    - New mask: 36 features, Fitness: -1.1238\n",
      "    - New mask: 37 features, Fitness: -1.6570\n",
      "    - New mask: 34 features, Fitness: -0.7002\n",
      "    - New mask: 34 features, Fitness: -0.5819\n",
      "    - New mask: 38 features, Fitness: 2.1299\n",
      "    - New mask: 37 features, Fitness: 1.7332\n",
      "    - New mask: 34 features, Fitness: 2.2405\n",
      "    - New mask: 36 features, Fitness: 2.8470\n",
      "    - New mask: 32 features, Fitness: 3.5974\n",
      "    - New mask: 35 features, Fitness: 2.5208\n",
      "    - New mask: 42 features, Fitness: 0.9806\n",
      "    - New mask: 31 features, Fitness: 3.9748\n",
      "    - New mask: 30 features, Fitness: 4.0564\n",
      "    - New mask: 34 features, Fitness: 2.8859\n",
      "    - New mask: 35 features, Fitness: 2.9733\n",
      "    - New mask: 31 features, Fitness: 4.3069\n",
      "    - New mask: 36 features, Fitness: 2.2636\n",
      "    - New mask: 31 features, Fitness: 3.8609\n",
      "    - New mask: 40 features, Fitness: 1.3501\n",
      "    - New mask: 32 features, Fitness: 3.5147\n",
      "    - New mask: 36 features, Fitness: 1.7668\n",
      "    - New mask: 31 features, Fitness: 3.0064\n",
      "    - New mask: 34 features, Fitness: 3.3677\n",
      "    - New mask: 36 features, Fitness: 3.4185\n",
      "    - New mask: 42 features, Fitness: -6.7744\n",
      "    - New mask: 36 features, Fitness: -3.7590\n",
      "    - New mask: 38 features, Fitness: -4.3593\n",
      "    - New mask: 37 features, Fitness: -3.8403\n",
      "    - New mask: 38 features, Fitness: -4.6389\n",
      "    - New mask: 32 features, Fitness: -2.2127\n",
      "    - New mask: 34 features, Fitness: -2.7978\n",
      "    - New mask: 33 features, Fitness: -2.0334\n",
      "    - New mask: 38 features, Fitness: -4.2546\n",
      "    - New mask: 33 features, Fitness: -1.7007\n",
      "    - New mask: 33 features, Fitness: -1.7675\n",
      "    - New mask: 35 features, Fitness: -3.7589\n",
      "    - New mask: 34 features, Fitness: -1.9707\n",
      "    - New mask: 36 features, Fitness: -3.4082\n",
      "    - New mask: 35 features, Fitness: -3.5348\n",
      "    - New mask: 34 features, Fitness: -3.9475\n",
      "    - New mask: 36 features, Fitness: -4.2306\n",
      "    - New mask: 32 features, Fitness: -2.6692\n",
      "    - New mask: 39 features, Fitness: -4.8115\n",
      "    - New mask: 36 features, Fitness: -4.1638\n",
      "    - New mask: 38 features, Fitness: -4.0550\n",
      "    - New mask: 40 features, Fitness: -4.9396\n",
      "    - New mask: 38 features, Fitness: -3.3555\n",
      "    - New mask: 40 features, Fitness: -4.3868\n",
      "    - New mask: 34 features, Fitness: -2.4537\n",
      "    - New mask: 39 features, Fitness: -3.9126\n",
      "    - New mask: 34 features, Fitness: -2.2358\n",
      "    - New mask: 41 features, Fitness: -4.4370\n",
      "    - New mask: 37 features, Fitness: -3.9464\n",
      "    - New mask: 36 features, Fitness: -2.5899\n",
      "    - New mask: 38 features, Fitness: -4.1543\n",
      "    - New mask: 36 features, Fitness: -3.0394\n",
      "    - New mask: 38 features, Fitness: -4.1512\n",
      "    - New mask: 37 features, Fitness: -3.2288\n",
      "    - New mask: 37 features, Fitness: -3.1991\n",
      "    - New mask: 36 features, Fitness: -3.2873\n",
      "    - New mask: 39 features, Fitness: -4.0305\n",
      "    - New mask: 36 features, Fitness: -3.0608\n",
      "    - New mask: 39 features, Fitness: -4.0845\n",
      "    - New mask: 39 features, Fitness: -3.5824\n",
      "    - New mask: 38 features, Fitness: -0.7448\n",
      "    - New mask: 36 features, Fitness: 0.5923\n",
      "    - New mask: 35 features, Fitness: 0.6393\n",
      "    - New mask: 37 features, Fitness: 0.5045\n",
      "    - New mask: 35 features, Fitness: -0.5248\n",
      "    - New mask: 36 features, Fitness: -0.1377\n",
      "    - New mask: 35 features, Fitness: -0.2976\n",
      "    - New mask: 35 features, Fitness: 0.2719\n",
      "    - New mask: 35 features, Fitness: -0.3566\n",
      "    - New mask: 35 features, Fitness: 1.4702\n",
      "    - New mask: 34 features, Fitness: -0.0259\n",
      "    - New mask: 40 features, Fitness: -1.7587\n",
      "    - New mask: 40 features, Fitness: -0.0349\n",
      "    - New mask: 40 features, Fitness: -0.5602\n",
      "    - New mask: 39 features, Fitness: -0.7430\n",
      "    - New mask: 37 features, Fitness: 0.5038\n",
      "    - New mask: 34 features, Fitness: 0.3193\n",
      "    - New mask: 36 features, Fitness: -0.5084\n",
      "    - New mask: 36 features, Fitness: -0.0482\n",
      "    - New mask: 34 features, Fitness: 0.2785\n",
      "    - New mask: 32 features, Fitness: -0.8653\n",
      "    - New mask: 39 features, Fitness: -4.0531\n",
      "    - New mask: 39 features, Fitness: -4.5921\n",
      "    - New mask: 39 features, Fitness: -3.4375\n",
      "    - New mask: 38 features, Fitness: -3.0652\n",
      "    - New mask: 33 features, Fitness: -1.3788\n",
      "    - New mask: 40 features, Fitness: -3.1518\n",
      "    - New mask: 38 features, Fitness: -3.4645\n",
      "    - New mask: 38 features, Fitness: -2.7755\n",
      "    - New mask: 38 features, Fitness: -2.8447\n",
      "    - New mask: 34 features, Fitness: -2.1949\n",
      "    - New mask: 38 features, Fitness: -3.9271\n",
      "    - New mask: 38 features, Fitness: -2.5407\n",
      "    - New mask: 36 features, Fitness: -2.5668\n",
      "    - New mask: 35 features, Fitness: -1.5890\n",
      "    - New mask: 42 features, Fitness: -5.3014\n",
      "    - New mask: 39 features, Fitness: -4.3206\n",
      "    - New mask: 40 features, Fitness: -3.6686\n",
      "    - New mask: 39 features, Fitness: -3.0990\n",
      "    - New mask: 35 features, Fitness: -1.8188\n",
      "    - New mask: 36 features, Fitness: -3.6471\n",
      "    - New mask: 35 features, Fitness: -3.1170\n",
      "    - New mask: 37 features, Fitness: -4.3094\n",
      "    - New mask: 36 features, Fitness: -4.0932\n",
      "    - New mask: 38 features, Fitness: -4.5710\n",
      "    - New mask: 41 features, Fitness: -5.5163\n",
      "    - New mask: 34 features, Fitness: -3.2376\n",
      "    - New mask: 41 features, Fitness: -5.8629\n",
      "    - New mask: 37 features, Fitness: -4.3567\n",
      "    - New mask: 35 features, Fitness: -4.0500\n",
      "    - New mask: 36 features, Fitness: -4.0288\n",
      "    - New mask: 33 features, Fitness: -3.0976\n",
      "    - New mask: 38 features, Fitness: -4.2240\n",
      "    - New mask: 36 features, Fitness: -3.9272\n",
      "    - New mask: 38 features, Fitness: -4.8612\n",
      "    - New mask: 41 features, Fitness: -7.3117\n",
      "    - New mask: 36 features, Fitness: -4.2054\n",
      "    - New mask: 42 features, Fitness: -6.0568\n",
      "    - New mask: 38 features, Fitness: -4.3603\n",
      "    - New mask: 36 features, Fitness: -4.4317\n",
      "=== End of Round 19: Vote mask selects 30 features (rho: 0.77)\n",
      "    Indices: [2, 3, 4, 6, 8, 12, 13, 14, 19, 24, 26, 28, 29, 30, 33, 34, 40, 41, 42, 45, 46, 47, 48, 50, 54, 59, 60, 64, 65, 68]\n",
      "\n",
      "================ Federated BFA Round 20 ================\n",
      "  Adaptive rho for this round: 0.80\n",
      "    - New mask: 32 features, Fitness: -4.6486\n",
      "    - New mask: 34 features, Fitness: -5.8213\n",
      "    - New mask: 35 features, Fitness: -6.4968\n",
      "    - New mask: 34 features, Fitness: -6.4503\n",
      "    - New mask: 33 features, Fitness: -5.6067\n",
      "    - New mask: 39 features, Fitness: -9.0294\n",
      "    - New mask: 35 features, Fitness: -5.9816\n",
      "    - New mask: 33 features, Fitness: -5.0167\n",
      "    - New mask: 31 features, Fitness: -4.6056\n",
      "    - New mask: 30 features, Fitness: -3.4736\n",
      "    - New mask: 32 features, Fitness: -5.2967\n",
      "    - New mask: 31 features, Fitness: -4.8681\n",
      "    - New mask: 31 features, Fitness: -3.8118\n",
      "    - New mask: 35 features, Fitness: -6.6276\n",
      "    - New mask: 38 features, Fitness: -7.8911\n",
      "    - New mask: 34 features, Fitness: -6.0079\n",
      "    - New mask: 34 features, Fitness: -6.6637\n",
      "    - New mask: 30 features, Fitness: -4.4290\n",
      "    - New mask: 36 features, Fitness: -7.0177\n",
      "    - New mask: 33 features, Fitness: -6.0943\n",
      "    - New mask: 30 features, Fitness: -1.5155\n",
      "    - New mask: 34 features, Fitness: -2.8667\n",
      "    - New mask: 28 features, Fitness: -1.8194\n",
      "    - New mask: 32 features, Fitness: -1.9881\n",
      "    - New mask: 29 features, Fitness: -2.3423\n",
      "    - New mask: 38 features, Fitness: -4.1382\n",
      "    - New mask: 35 features, Fitness: -3.4241\n",
      "    - New mask: 34 features, Fitness: -3.1138\n",
      "    - New mask: 30 features, Fitness: -2.8808\n",
      "    - New mask: 35 features, Fitness: -4.4548\n",
      "    - New mask: 32 features, Fitness: -2.1574\n",
      "    - New mask: 32 features, Fitness: -2.3628\n",
      "    - New mask: 34 features, Fitness: -3.1111\n",
      "    - New mask: 33 features, Fitness: -2.6400\n",
      "    - New mask: 33 features, Fitness: -2.9052\n",
      "    - New mask: 35 features, Fitness: -3.4253\n",
      "    - New mask: 37 features, Fitness: -4.0574\n",
      "    - New mask: 32 features, Fitness: -2.0754\n",
      "    - New mask: 33 features, Fitness: -2.4488\n",
      "    - New mask: 35 features, Fitness: -3.2072\n",
      "    - New mask: 31 features, Fitness: -2.4242\n",
      "    - New mask: 34 features, Fitness: -2.9367\n",
      "    - New mask: 33 features, Fitness: -2.5341\n",
      "    - New mask: 37 features, Fitness: -2.4210\n",
      "    - New mask: 34 features, Fitness: -2.1454\n",
      "    - New mask: 33 features, Fitness: -1.8027\n",
      "    - New mask: 34 features, Fitness: -2.4193\n",
      "    - New mask: 30 features, Fitness: -1.4877\n",
      "    - New mask: 34 features, Fitness: -1.7883\n",
      "    - New mask: 33 features, Fitness: -1.9869\n",
      "    - New mask: 36 features, Fitness: -4.0664\n",
      "    - New mask: 35 features, Fitness: -2.7487\n",
      "    - New mask: 33 features, Fitness: -2.7603\n",
      "    - New mask: 33 features, Fitness: -1.7824\n",
      "    - New mask: 34 features, Fitness: -3.7514\n",
      "    - New mask: 36 features, Fitness: -2.8719\n",
      "    - New mask: 34 features, Fitness: -2.4987\n",
      "    - New mask: 37 features, Fitness: -2.7502\n",
      "    - New mask: 33 features, Fitness: -3.1317\n",
      "    - New mask: 31 features, Fitness: -0.9001\n",
      "    - New mask: 32 features, Fitness: 0.2795\n",
      "    - New mask: 31 features, Fitness: 0.1922\n",
      "    - New mask: 35 features, Fitness: -0.9010\n",
      "    - New mask: 35 features, Fitness: -0.7126\n",
      "    - New mask: 31 features, Fitness: 0.4418\n",
      "    - New mask: 34 features, Fitness: -0.5255\n",
      "    - New mask: 36 features, Fitness: -2.0669\n",
      "    - New mask: 34 features, Fitness: -0.1487\n",
      "    - New mask: 35 features, Fitness: -0.6388\n",
      "    - New mask: 31 features, Fitness: 0.1927\n",
      "    - New mask: 33 features, Fitness: -0.1721\n",
      "    - New mask: 34 features, Fitness: -0.5636\n",
      "    - New mask: 34 features, Fitness: -0.8249\n",
      "    - New mask: 31 features, Fitness: 0.3890\n",
      "    - New mask: 28 features, Fitness: 1.0209\n",
      "    - New mask: 32 features, Fitness: -0.0265\n",
      "    - New mask: 33 features, Fitness: -0.6564\n",
      "    - New mask: 36 features, Fitness: -1.3309\n",
      "    - New mask: 32 features, Fitness: -0.0562\n",
      "    - New mask: 30 features, Fitness: 0.5234\n",
      "    - New mask: 37 features, Fitness: 2.0552\n",
      "    - New mask: 36 features, Fitness: 1.3982\n",
      "    - New mask: 32 features, Fitness: 3.4041\n",
      "    - New mask: 31 features, Fitness: 3.2719\n",
      "    - New mask: 30 features, Fitness: 3.2083\n",
      "    - New mask: 36 features, Fitness: 3.2844\n",
      "    - New mask: 36 features, Fitness: 2.2921\n",
      "    - New mask: 38 features, Fitness: 2.7204\n",
      "    - New mask: 28 features, Fitness: 3.6675\n",
      "    - New mask: 35 features, Fitness: 3.3761\n",
      "    - New mask: 27 features, Fitness: 4.1745\n",
      "    - New mask: 35 features, Fitness: 3.2266\n",
      "    - New mask: 35 features, Fitness: 3.1767\n",
      "    - New mask: 32 features, Fitness: 4.3280\n",
      "    - New mask: 36 features, Fitness: 2.8127\n",
      "    - New mask: 31 features, Fitness: 3.8945\n",
      "    - New mask: 28 features, Fitness: 2.8084\n",
      "    - New mask: 33 features, Fitness: 3.5273\n",
      "    - New mask: 28 features, Fitness: 3.8276\n",
      "    - New mask: 36 features, Fitness: 3.7034\n",
      "    - New mask: 32 features, Fitness: -1.4622\n",
      "    - New mask: 31 features, Fitness: -1.9749\n",
      "    - New mask: 33 features, Fitness: -2.2767\n",
      "    - New mask: 31 features, Fitness: -1.1407\n",
      "    - New mask: 36 features, Fitness: -3.0191\n",
      "    - New mask: 36 features, Fitness: -3.6109\n",
      "    - New mask: 38 features, Fitness: -4.3424\n",
      "    - New mask: 28 features, Fitness: -0.9943\n",
      "    - New mask: 31 features, Fitness: -1.1835\n",
      "    - New mask: 34 features, Fitness: -1.9165\n",
      "    - New mask: 30 features, Fitness: -1.6123\n",
      "    - New mask: 33 features, Fitness: -2.5975\n",
      "    - New mask: 29 features, Fitness: -0.4946\n",
      "    - New mask: 29 features, Fitness: -1.8501\n",
      "    - New mask: 31 features, Fitness: -1.9373\n",
      "    - New mask: 34 features, Fitness: -2.5005\n",
      "    - New mask: 37 features, Fitness: -3.8917\n",
      "    - New mask: 32 features, Fitness: -2.2958\n",
      "    - New mask: 31 features, Fitness: -1.7865\n",
      "    - New mask: 35 features, Fitness: -2.9743\n",
      "    - New mask: 36 features, Fitness: -3.0150\n",
      "    - New mask: 37 features, Fitness: -3.5768\n",
      "    - New mask: 36 features, Fitness: -2.8109\n",
      "    - New mask: 34 features, Fitness: -2.2548\n",
      "    - New mask: 33 features, Fitness: -1.6270\n",
      "    - New mask: 34 features, Fitness: -2.2563\n",
      "    - New mask: 32 features, Fitness: -1.5332\n",
      "    - New mask: 38 features, Fitness: -3.6973\n",
      "    - New mask: 33 features, Fitness: -2.2628\n",
      "    - New mask: 34 features, Fitness: -2.7751\n",
      "    - New mask: 33 features, Fitness: -2.2984\n",
      "    - New mask: 34 features, Fitness: -2.7281\n",
      "    - New mask: 36 features, Fitness: -3.1175\n",
      "    - New mask: 36 features, Fitness: -2.5551\n",
      "    - New mask: 34 features, Fitness: -2.1113\n",
      "    - New mask: 31 features, Fitness: -1.4349\n",
      "    - New mask: 33 features, Fitness: -1.5404\n",
      "    - New mask: 34 features, Fitness: -2.1367\n",
      "    - New mask: 36 features, Fitness: -3.5877\n",
      "    - New mask: 34 features, Fitness: -2.0142\n",
      "    - New mask: 33 features, Fitness: 0.3996\n",
      "    - New mask: 37 features, Fitness: 1.2154\n",
      "    - New mask: 37 features, Fitness: 0.9753\n",
      "    - New mask: 38 features, Fitness: 1.1676\n",
      "    - New mask: 33 features, Fitness: -0.0368\n",
      "    - New mask: 33 features, Fitness: 0.4544\n",
      "    - New mask: 32 features, Fitness: 0.9354\n",
      "    - New mask: 32 features, Fitness: 1.1875\n",
      "    - New mask: 34 features, Fitness: 0.2428\n",
      "    - New mask: 39 features, Fitness: -0.1193\n",
      "    - New mask: 35 features, Fitness: 0.0722\n",
      "    - New mask: 34 features, Fitness: 0.5945\n",
      "    - New mask: 39 features, Fitness: 0.5678\n",
      "    - New mask: 35 features, Fitness: 1.6431\n",
      "    - New mask: 34 features, Fitness: 1.3451\n",
      "    - New mask: 38 features, Fitness: 0.3202\n",
      "    - New mask: 34 features, Fitness: 0.6154\n",
      "    - New mask: 33 features, Fitness: 0.6954\n",
      "    - New mask: 32 features, Fitness: 1.4110\n",
      "    - New mask: 30 features, Fitness: 2.0145\n",
      "    - New mask: 33 features, Fitness: -0.9625\n",
      "    - New mask: 36 features, Fitness: -2.4286\n",
      "    - New mask: 36 features, Fitness: -2.0997\n",
      "    - New mask: 37 features, Fitness: -2.5344\n",
      "    - New mask: 33 features, Fitness: -1.7051\n",
      "    - New mask: 32 features, Fitness: -1.4302\n",
      "    - New mask: 36 features, Fitness: -2.0722\n",
      "    - New mask: 33 features, Fitness: -1.3922\n",
      "    - New mask: 33 features, Fitness: -1.4627\n",
      "    - New mask: 32 features, Fitness: -0.3270\n",
      "    - New mask: 34 features, Fitness: -1.6124\n",
      "    - New mask: 34 features, Fitness: -1.8176\n",
      "    - New mask: 34 features, Fitness: -1.7983\n",
      "    - New mask: 33 features, Fitness: -1.0224\n",
      "    - New mask: 31 features, Fitness: -0.1680\n",
      "    - New mask: 34 features, Fitness: -1.4144\n",
      "    - New mask: 36 features, Fitness: -2.4897\n",
      "    - New mask: 34 features, Fitness: -1.2835\n",
      "    - New mask: 34 features, Fitness: -1.3242\n",
      "    - New mask: 31 features, Fitness: -0.3369\n",
      "    - New mask: 31 features, Fitness: -2.5081\n",
      "    - New mask: 32 features, Fitness: -1.8578\n",
      "    - New mask: 30 features, Fitness: -1.7236\n",
      "    - New mask: 32 features, Fitness: -2.8911\n",
      "    - New mask: 34 features, Fitness: -2.8418\n",
      "    - New mask: 40 features, Fitness: -5.0597\n",
      "    - New mask: 32 features, Fitness: -2.3264\n",
      "    - New mask: 34 features, Fitness: -2.7276\n",
      "    - New mask: 36 features, Fitness: -3.7559\n",
      "    - New mask: 35 features, Fitness: -2.9747\n",
      "    - New mask: 34 features, Fitness: -2.8872\n",
      "    - New mask: 36 features, Fitness: -4.3939\n",
      "    - New mask: 38 features, Fitness: -4.5184\n",
      "    - New mask: 32 features, Fitness: -2.2762\n",
      "    - New mask: 32 features, Fitness: -3.4091\n",
      "    - New mask: 36 features, Fitness: -4.2293\n",
      "    - New mask: 35 features, Fitness: -3.8600\n",
      "    - New mask: 36 features, Fitness: -3.7287\n",
      "    - New mask: 36 features, Fitness: -3.9111\n",
      "    - New mask: 30 features, Fitness: -1.5374\n",
      "=== End of Round 20: Vote mask selects 28 features (rho: 0.80)\n",
      "    Indices: [2, 4, 6, 8, 12, 13, 14, 19, 24, 26, 28, 29, 30, 33, 34, 40, 41, 42, 45, 46, 47, 48, 54, 59, 60, 64, 65, 68]\n",
      "\n",
      "Final federated feature count: 28\n",
      "Selected feature names: ['Total Bwd packets', 'Total Length of Bwd Packet', 'Fwd Packet Length Min', 'Fwd Packet Length Std', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Fwd IAT Total', 'Bwd IAT Total', 'Bwd IAT Std', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd Header Length', 'Bwd Header Length', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWR Flag Count', 'Fwd Bytes/Bulk Avg', 'Bwd Bulk Rate Avg', 'Subflow Fwd Packets', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Active Mean']\n"
     ]
    }
   ],
   "source": [
    "n_feat_select_rounds = 20\n",
    "n_fireflies = 20           # Number of fireflies per client\n",
    "n_features = X.shape[1]\n",
    "num_clients = len(client_data_np)\n",
    "rho_start, rho_end = 0.2, 0.8\n",
    "penalty_lambda = 0.9\n",
    "\n",
    "# Precompute Fisher scores and correlation matrix for each client\n",
    "client_fisher_scores = []\n",
    "client_corr_matrix = []\n",
    "for Xc, yc in client_data_np:\n",
    "    fisher_scores = compute_fisher_scores(Xc, yc)\n",
    "    corr_matrix = compute_corr_matrix(Xc)\n",
    "    client_fisher_scores.append(fisher_scores)\n",
    "    client_corr_matrix.append(corr_matrix)\n",
    "\n",
    "# Initialize fireflies for each client at round 1\n",
    "client_fireflies = []\n",
    "client_local_bests = []\n",
    "for cid in range(num_clients):\n",
    "    fireflies = []\n",
    "    for _ in range(n_fireflies):\n",
    "        mask = np.random.choice([0, 1], size=n_features)\n",
    "        if np.sum(mask) == 0:\n",
    "            mask[np.random.randint(n_features)] = 1  # Ensure at least one feature is selected\n",
    "        fireflies.append(mask)\n",
    "    # Evaluate and store best\n",
    "    best_fitness = -np.inf\n",
    "    best_mask = None\n",
    "    for mask in fireflies:\n",
    "        sel = np.where(mask)[0]\n",
    "        fit = evaluate_feature_subset(sel, client_fisher_scores[cid], client_corr_matrix[cid], penalty_lambda)\n",
    "        if fit > best_fitness or best_mask is None:\n",
    "            best_fitness = fit\n",
    "            best_mask = mask.copy()\n",
    "    # Fallback: all features if somehow none was found\n",
    "    if best_mask is None:\n",
    "        best_mask = np.ones(n_features, dtype=int)\n",
    "    client_fireflies.append(fireflies)\n",
    "    client_local_bests.append(best_mask.copy())\n",
    "\n",
    "# Start with all features selected in global mask\n",
    "global_mask = np.ones(n_features, dtype=int)\n",
    "\n",
    "for round_fs in range(n_feat_select_rounds):\n",
    "    print(f\"\\n================ Federated BFA Round {round_fs+1} ================\")\n",
    "    # Linear schedule for rho\n",
    "    rho = rho_start + (rho_end - rho_start) * (round_fs / (n_feat_select_rounds - 1))\n",
    "    print(f\"  Adaptive rho for this round: {rho:.2f}\")\n",
    "\n",
    "    client_best_masks = []\n",
    "    # For each client, update fireflies and find new local best\n",
    "    for cid in range(num_clients):\n",
    "        fireflies = client_fireflies[cid]\n",
    "        fisher_scores = client_fisher_scores[cid]\n",
    "        corr_matrix = client_corr_matrix[cid]\n",
    "        local_best = client_local_bests[cid]\n",
    "        new_fireflies = []\n",
    "        best_fitness = -np.inf\n",
    "        best_mask = None\n",
    "        for f in range(n_fireflies):\n",
    "            new_mask = one_step_binary_firefly(\n",
    "                fireflies[f],\n",
    "                global_mask,\n",
    "                local_best,\n",
    "                fisher_scores,\n",
    "                corr_matrix,\n",
    "                penalty_lambda=penalty_lambda,\n",
    "                verbose=True\n",
    "            )\n",
    "            # Ensure at least one feature\n",
    "            if np.sum(new_mask) == 0:\n",
    "                new_mask[np.random.randint(n_features)] = 1\n",
    "            new_fireflies.append(new_mask)\n",
    "            sel = np.where(new_mask)[0]\n",
    "            fit = evaluate_feature_subset(sel, fisher_scores, corr_matrix, penalty_lambda)\n",
    "            if fit > best_fitness or best_mask is None:\n",
    "                best_fitness = fit\n",
    "                best_mask = new_mask.copy()\n",
    "        # Fallback: all features if somehow none was found\n",
    "        if best_mask is None:\n",
    "            best_mask = np.ones(n_features, dtype=int)\n",
    "        # Update client's fireflies and local best\n",
    "        client_fireflies[cid] = new_fireflies\n",
    "        client_local_bests[cid] = best_mask.copy()\n",
    "        client_best_masks.append(best_mask.copy())\n",
    "    client_best_masks = np.array(client_best_masks)\n",
    "    vote_counts = np.sum(client_best_masks, axis=0)\n",
    "    vote_mask = (vote_counts >= (rho * num_clients)).astype(int)\n",
    "    print(f\"=== End of Round {round_fs+1}: Vote mask selects {vote_mask.sum()} features (rho: {rho:.2f})\\n\"\n",
    "          f\"    Indices: {np.where(vote_mask)[0].tolist()}\")\n",
    "    global_mask = vote_mask.copy()\n",
    "\n",
    "selected_indices = np.where(global_mask == 1)[0]\n",
    "print(f\"\\nFinal federated feature count: {len(selected_indices)}\")\n",
    "selected_feature_names = [feature_cols[i] for i in selected_indices]\n",
    "print(\"Selected feature names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final federated feature count: 28\n",
      "Selected feature names: ['Total Bwd packets', 'Total Length of Bwd Packet', 'Fwd Packet Length Min', 'Fwd Packet Length Std', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Fwd IAT Total', 'Bwd IAT Total', 'Bwd IAT Std', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd Header Length', 'Bwd Header Length', 'Packet Length Std', 'Packet Length Variance', 'FIN Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'CWR Flag Count', 'Fwd Bytes/Bulk Avg', 'Bwd Bulk Rate Avg', 'Subflow Fwd Packets', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Active Mean']\n"
     ]
    }
   ],
   "source": [
    "selected_indices = np.where(global_mask == 1)[0]\n",
    "print(f\"\\nFinal federated feature count: {len(selected_indices)}\")\n",
    "selected_feature_names = [feature_cols[i] for i in selected_indices]\n",
    "print(\"Selected feature names:\", selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel = X[:, selected_indices]\n",
    "input_dim = X_sel.shape[1]\n",
    "full_dataset = TabularDataset(X_sel, y)\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "client_loaders = []\n",
    "for i in range(num_clients):\n",
    "    idxs = client_indices[i]\n",
    "    client_subset = Subset(train_dataset, idxs)\n",
    "    client_loader = DataLoader(client_subset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    client_loaders.append(client_loader)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=2):\n",
    "        super(TabularMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        features = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(features)\n",
    "        out = self.fc3(x)\n",
    "        if return_features:\n",
    "            return out, features\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Federated Round 1 (Local Epochs: 10)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before:  4.20% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 61.92% | Acc After: 98.14%\n",
      "  Client  3 | Samples: 278944 | Acc Before:  4.05% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 22.80% | Acc After: 97.46%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 11.43% | Acc After: 98.38%\n",
      "  Client  6 | Samples: 14413 | Acc Before:  4.76% | Acc After: 99.28%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 42.22% | Acc After: 96.97%\n",
      "  Client  8 | Samples: 123392 | Acc Before:  7.47% | Acc After: 98.98%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 30.62% | Acc After: 96.87%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 62.55% | Acc After: 98.15%\n",
      "\n",
      "[Round 1] Global Test Accuracy: 96.13%\n",
      "Client Acc BEFORE (mean ± std): 25.20% ± 22.09%\n",
      "Client Acc AFTER  (mean ± std): 98.42% ± 1.07%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 2 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 98.38% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.40% | Acc After: 98.13%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 98.40% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 95.74% | Acc After: 97.61%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 97.34% | Acc After: 98.39%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 98.20% | Acc After: 99.66%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 92.92% | Acc After: 97.11%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.95% | Acc After: 98.94%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 94.63% | Acc After: 96.92%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.99% | Acc After: 98.17%\n",
      "\n",
      "[Round 2] Global Test Accuracy: 96.62%\n",
      "Client Acc BEFORE (mean ± std): 95.40% ± 3.12%\n",
      "Client Acc AFTER  (mean ± std): 98.49% ± 1.06%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 3 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.25% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.11% | Acc After: 98.28%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.25% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.26% | Acc After: 97.69%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.02% | Acc After: 98.57%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.12% | Acc After: 99.74%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.04% | Acc After: 97.23%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 98.72% | Acc After: 99.10%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 94.99% | Acc After: 97.45%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.75% | Acc After: 97.63%\n",
      "\n",
      "[Round 3] Global Test Accuracy: 96.83%\n",
      "Client Acc BEFORE (mean ± std): 95.85% ± 3.55%\n",
      "Client Acc AFTER  (mean ± std): 98.57% ± 1.02%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 4 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.52% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.23% | Acc After: 98.26%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.53% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.43% | Acc After: 97.68%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.30% | Acc After: 98.49%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.42% | Acc After: 99.75%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.18% | Acc After: 97.33%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 98.99% | Acc After: 99.08%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.14% | Acc After: 97.21%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.78% | Acc After: 98.30%\n",
      "\n",
      "[Round 4] Global Test Accuracy: 97.01%\n",
      "Client Acc BEFORE (mean ± std): 96.05% ± 3.63%\n",
      "Client Acc AFTER  (mean ± std): 98.61% ± 1.00%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 5 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.57% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.66% | Acc After: 98.17%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.60% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.65% | Acc After: 97.70%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.40% | Acc After: 98.59%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.44% | Acc After: 99.74%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.59% | Acc After: 97.26%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.06% | Acc After: 99.09%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.43% | Acc After: 97.31%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.33% | Acc After: 98.28%\n",
      "\n",
      "[Round 5] Global Test Accuracy: 97.04%\n",
      "Client Acc BEFORE (mean ± std): 96.27% ± 3.45%\n",
      "Client Acc AFTER  (mean ± std): 98.61% ± 1.00%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 6 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.58% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.69% | Acc After: 98.32%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.61% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.69% | Acc After: 97.75%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.42% | Acc After: 98.60%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.48% | Acc After: 99.73%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.62% | Acc After: 97.37%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.09% | Acc After: 99.04%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.49% | Acc After: 97.35%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.45% | Acc After: 98.27%\n",
      "\n",
      "[Round 6] Global Test Accuracy: 97.04%\n",
      "Client Acc BEFORE (mean ± std): 96.31% ± 3.43%\n",
      "Client Acc AFTER  (mean ± std): 98.64% ± 0.96%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 7 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.57% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.68% | Acc After: 98.36%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.60% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.68% | Acc After: 97.74%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.41% | Acc After: 98.63%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.46% | Acc After: 99.78%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.68% | Acc After: 97.37%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.07% | Acc After: 99.18%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.46% | Acc After: 97.19%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.36% | Acc After: 96.79%\n",
      "\n",
      "[Round 7] Global Test Accuracy: 96.95%\n",
      "Client Acc BEFORE (mean ± std): 96.30% ± 3.44%\n",
      "Client Acc AFTER  (mean ± std): 98.50% ± 1.14%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 8 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.60% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.23% | Acc After: 98.32%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.63% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.54% | Acc After: 97.82%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.39% | Acc After: 98.69%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.48% | Acc After: 99.78%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.27% | Acc After: 97.33%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.07% | Acc After: 99.14%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.24% | Acc After: 97.53%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.91% | Acc After: 98.24%\n",
      "\n",
      "[Round 8] Global Test Accuracy: 97.09%\n",
      "Client Acc BEFORE (mean ± std): 96.14% ± 3.63%\n",
      "Client Acc AFTER  (mean ± std): 98.68% ± 0.95%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 9 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.58% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.84% | Acc After: 98.33%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.60% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.74% | Acc After: 97.82%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.45% | Acc After: 98.65%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.49% | Acc After: 99.74%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.74% | Acc After: 97.40%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.10% | Acc After: 99.19%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.54% | Acc After: 97.42%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.58% | Acc After: 98.28%\n",
      "\n",
      "[Round 9] Global Test Accuracy: 97.10%\n",
      "Client Acc BEFORE (mean ± std): 96.37% ± 3.38%\n",
      "Client Acc AFTER  (mean ± std): 98.68% ± 0.95%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 10 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.57% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.89% | Acc After: 98.31%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.60% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.75% | Acc After: 97.79%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.44% | Acc After: 98.77%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.47% | Acc After: 99.79%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.77% | Acc After: 97.48%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.10% | Acc After: 99.10%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.53% | Acc After: 97.43%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.61% | Acc After: 98.29%\n",
      "\n",
      "[Round 10] Global Test Accuracy: 97.00%\n",
      "Client Acc BEFORE (mean ± std): 96.37% ± 3.36%\n",
      "Client Acc AFTER  (mean ± std): 98.69% ± 0.94%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 11 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.63% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.41% | Acc After: 98.33%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.65% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.64% | Acc After: 97.83%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.44% | Acc After: 98.75%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.53% | Acc After: 99.77%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.46% | Acc After: 97.46%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.11% | Acc After: 99.22%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.37% | Acc After: 97.46%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.07% | Acc After: 98.28%\n",
      "\n",
      "[Round 11] Global Test Accuracy: 97.02%\n",
      "Client Acc BEFORE (mean ± std): 96.23% ± 3.57%\n",
      "Client Acc AFTER  (mean ± std): 98.71% ± 0.94%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 12 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.69% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.30% | Acc After: 98.25%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.71% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.65% | Acc After: 97.84%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.48% | Acc After: 98.63%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.58% | Acc After: 99.78%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.44% | Acc After: 97.43%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.16% | Acc After: 99.20%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.34% | Acc After: 97.35%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.95% | Acc After: 98.17%\n",
      "\n",
      "[Round 12] Global Test Accuracy: 96.91%\n",
      "Client Acc BEFORE (mean ± std): 96.23% ± 3.64%\n",
      "Client Acc AFTER  (mean ± std): 98.66% ± 0.97%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 13 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.69% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 89.93% | Acc After: 98.32%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.71% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.54% | Acc After: 97.72%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.43% | Acc After: 98.68%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.56% | Acc After: 99.72%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.19% | Acc After: 97.51%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.14% | Acc After: 99.17%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.18% | Acc After: 97.51%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.65% | Acc After: 98.17%\n",
      "\n",
      "[Round 13] Global Test Accuracy: 96.88%\n",
      "Client Acc BEFORE (mean ± std): 96.10% ± 3.77%\n",
      "Client Acc AFTER  (mean ± std): 98.68% ± 0.94%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 14 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.71% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 89.79% | Acc After: 98.33%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.72% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.49% | Acc After: 97.86%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.41% | Acc After: 98.67%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.55% | Acc After: 99.80%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.13% | Acc After: 97.50%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.15% | Acc After: 99.19%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.12% | Acc After: 97.29%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.48% | Acc After: 98.28%\n",
      "\n",
      "[Round 14] Global Test Accuracy: 96.85%\n",
      "Client Acc BEFORE (mean ± std): 96.05% ± 3.82%\n",
      "Client Acc AFTER  (mean ± std): 98.69% ± 0.96%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 15 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.74% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 89.57% | Acc After: 98.32%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.76% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.46% | Acc After: 97.79%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.43% | Acc After: 98.73%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.64% | Acc After: 99.71%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 92.99% | Acc After: 97.49%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.16% | Acc After: 99.18%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.05% | Acc After: 97.44%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.31% | Acc After: 98.29%\n",
      "\n",
      "[Round 15] Global Test Accuracy: 96.99%\n",
      "Client Acc BEFORE (mean ± std): 96.01% ± 3.92%\n",
      "Client Acc AFTER  (mean ± std): 98.69% ± 0.93%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 16 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.75% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.14% | Acc After: 98.32%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.77% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.63% | Acc After: 97.85%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.48% | Acc After: 98.73%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.66% | Acc After: 99.71%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.31% | Acc After: 97.46%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.20% | Acc After: 99.18%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.29% | Acc After: 97.45%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 89.81% | Acc After: 98.28%\n",
      "\n",
      "[Round 16] Global Test Accuracy: 97.09%\n",
      "Client Acc BEFORE (mean ± std): 96.20% ± 3.72%\n",
      "Client Acc AFTER  (mean ± std): 98.69% ± 0.93%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 17 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.73% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.44% | Acc After: 98.31%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.75% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.72% | Acc After: 97.76%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.52% | Acc After: 98.66%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.62% | Acc After: 99.80%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.52% | Acc After: 97.41%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.21% | Acc After: 99.16%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.42% | Acc After: 97.44%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.12% | Acc After: 98.24%\n",
      "\n",
      "[Round 17] Global Test Accuracy: 97.10%\n",
      "Client Acc BEFORE (mean ± std): 96.30% ± 3.59%\n",
      "Client Acc AFTER  (mean ± std): 98.67% ± 0.96%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 18 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.75% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.46% | Acc After: 98.29%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.76% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.73% | Acc After: 97.83%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.51% | Acc After: 98.69%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.67% | Acc After: 99.68%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.54% | Acc After: 97.43%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.21% | Acc After: 99.20%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.44% | Acc After: 97.45%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.18% | Acc After: 96.78%\n",
      "\n",
      "[Round 18] Global Test Accuracy: 97.02%\n",
      "Client Acc BEFORE (mean ± std): 96.32% ± 3.59%\n",
      "Client Acc AFTER  (mean ± std): 98.53% ± 1.09%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 19 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.59% | Acc After: 99.96%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.61% | Acc After: 98.25%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.61% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.64% | Acc After: 97.95%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.42% | Acc After: 98.71%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.46% | Acc After: 99.71%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.57% | Acc After: 97.48%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.10% | Acc After: 99.21%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.35% | Acc After: 97.34%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.20% | Acc After: 98.28%\n",
      "\n",
      "[Round 19] Global Test Accuracy: 97.10%\n",
      "Client Acc BEFORE (mean ± std): 96.26% ± 3.49%\n",
      "Client Acc AFTER  (mean ± std): 98.69% ± 0.94%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 20 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.71% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 90.47% | Acc After: 98.27%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.71% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 96.74% | Acc After: 97.90%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.50% | Acc After: 98.72%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.60% | Acc After: 99.67%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 93.55% | Acc After: 97.46%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.18% | Acc After: 99.20%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 95.45% | Acc After: 97.41%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 90.23% | Acc After: 98.27%\n",
      "\n",
      "[Round 20] Global Test Accuracy: 97.15%\n",
      "Client Acc BEFORE (mean ± std): 96.31% ± 3.56%\n",
      "Client Acc AFTER  (mean ± std): 98.68% ± 0.93%\n",
      "Client sample count (min, max): 14413, 278944\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_one_client(model, loader, epochs=1, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.cpu()\n",
    "\n",
    "def evaluate_local(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def average_weights(weight_list):\n",
    "    avg_weights = {}\n",
    "    for key in weight_list[0].keys():\n",
    "        avg_weights[key] = sum([w[key] for w in weight_list]) / len(weight_list)\n",
    "    return avg_weights\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "global_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "global_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "num_rounds = 20\n",
    "for rnd in range(1, num_rounds + 1):\n",
    "    adaptive_epochs = max(1, int(10 - 9 * (rnd-1) / (num_rounds-1)))\n",
    "    print(f\"\\n{'='*30}\\nFederated Round {rnd} (Local Epochs: {adaptive_epochs})\\n{'='*30}\")\n",
    "    local_weights = []\n",
    "    client_accuracies_before = []\n",
    "    client_accuracies_after = []\n",
    "    client_sample_counts = []\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        num_samples = len(client_loaders[client_id].dataset)\n",
    "        acc_before = evaluate_local(global_model, client_loaders[client_id])\n",
    "        local_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_model = train_one_client(local_model, client_loaders[client_id], epochs=adaptive_epochs)\n",
    "        acc_after = evaluate_local(local_model, client_loaders[client_id])\n",
    "        local_weights.append(local_model.state_dict())\n",
    "        client_sample_counts.append(num_samples)\n",
    "        client_accuracies_before.append(acc_before)\n",
    "        client_accuracies_after.append(acc_after)\n",
    "        print(f\"  Client {client_id+1:2d} | Samples: {num_samples:4d} | Acc Before: {acc_before:5.2f}% | Acc After: {acc_after:5.2f}%\")\n",
    "\n",
    "    global_model.load_state_dict(average_weights(local_weights))\n",
    "    acc_global = test_model(global_model, test_loader)\n",
    "    print(f\"\\n[Round {rnd}] Global Test Accuracy: {acc_global:.2f}%\")\n",
    "    print(f\"Client Acc BEFORE (mean ± std): {np.mean(client_accuracies_before):.2f}% ± {np.std(client_accuracies_before):.2f}%\")\n",
    "    print(f\"Client Acc AFTER  (mean ± std): {np.mean(client_accuracies_after):.2f}% ± {np.std(client_accuracies_after):.2f}%\")\n",
    "    print(f\"Client sample count (min, max): {min(client_sample_counts)}, {max(client_sample_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.15%\n",
      "Confusion Matrix:\n",
      " [[179956    441]\n",
      " [  6559  58942]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test_modelv2(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    acc = 100. * correct / total\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    return acc, cm\n",
    "\n",
    "# Usage\n",
    "acc, cm = test_modelv2(global_model, test_loader)\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def frhc_local_feature_selection(X, max_clusters=None, comp_feat=1):\n",
    "    \"\"\"\n",
    "    Local representative feature selection by hierarchical clustering of features.\n",
    "    \n",
    "    Parameters:\n",
    "        X: [n_samples, n_features] numpy array (client's local data)\n",
    "        max_clusters: int or None, maximum clusters to try for optimal selection\n",
    "        comp_feat: int, number of compensation features to add\n",
    "\n",
    "    Returns:\n",
    "        selected_feature_indices: list of selected feature indices\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    # Step 1: Compute absolute correlation distance between features\n",
    "    corr_matrix = np.corrcoef(X, rowvar=False)\n",
    "    dist_matrix = 1 - np.abs(corr_matrix)\n",
    "    # Ensure distance matrix is valid\n",
    "    np.fill_diagonal(dist_matrix, 0)\n",
    "    # Convert to condensed form for linkage\n",
    "    condensed = squareform(dist_matrix, checks=False)\n",
    "    # Step 2: Hierarchical clustering\n",
    "    Z = linkage(condensed, method='average')\n",
    "    # Step 3: Optimal number of clusters (can be determined by a method, here use max_clusters or sqrt rule)\n",
    "    if max_clusters is None:\n",
    "        K = int(np.sqrt(n_features))\n",
    "    else:\n",
    "        K = min(max_clusters, n_features)\n",
    "    clusters = fcluster(Z, K, criterion='maxclust')\n",
    "    # Step 4: Find the two largest clusters\n",
    "    cluster_sizes = [(c, np.sum(clusters == c)) for c in np.unique(clusters)]\n",
    "    cluster_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    selected_features = []\n",
    "    for i in range(min(2, len(cluster_sizes))):\n",
    "        c = cluster_sizes[i][0]\n",
    "        selected_features.extend(np.where(clusters == c)[0].tolist())\n",
    "    # Step 5: Optionally add compensation feature(s)\n",
    "    if comp_feat > 0:\n",
    "        feature_counts = [(c, np.sum(clusters == c)) for c in np.unique(clusters)]\n",
    "        cluster_sorted = sorted(feature_counts, key=lambda x: x[1], reverse=True)\n",
    "        # Add features from next largest clusters if needed\n",
    "        for i in range(2, min(2 + comp_feat, len(cluster_sorted))):\n",
    "            c = cluster_sorted[i][0]\n",
    "            selected_features.append(np.where(clusters == c)[0][0])\n",
    "    # Remove duplicates\n",
    "    selected_features = list(sorted(set(selected_features)))\n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frhc_global_intersection(selected_lists):\n",
    "    \"\"\"\n",
    "    Compute global overlapping federated features as intersection of local sets.\n",
    "    Parameters:\n",
    "        selected_lists: list of list of feature indices (from each client)\n",
    "    Returns:\n",
    "        final_indices: list of feature indices present in all clients\n",
    "    \"\"\"\n",
    "    # Convert all to set for intersection\n",
    "    final_indices = set(selected_lists[0])\n",
    "    for feat_set in selected_lists[1:]:\n",
    "        final_indices &= set(feat_set)\n",
    "    return sorted(list(final_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 27\n",
      "Global federated feature indices (FRHC): [0, 1, 6, 9, 11, 12, 13, 15, 16, 17, 19, 20, 22, 23, 30, 36, 37, 38, 39, 46, 49, 51, 57, 62, 66, 67, 69]\n",
      "Selected feature names: ['Flow Duration', 'Total Fwd Packet', 'Fwd Packet Length Min', 'Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd PSH Flags', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'ACK Flag Count', 'ECE Flag Count', 'Average Packet Size', 'Bwd Bytes/Bulk Avg', 'Subflow Bwd Packets', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Std']\n"
     ]
    }
   ],
   "source": [
    "# Suppose client_data_np is a list of (X_local, y_local) for all clients\n",
    "selected_lists = []\n",
    "for Xc, yc in client_data_np:\n",
    "    feats = frhc_local_feature_selection(Xc,max_clusters=12,comp_feat=9)\n",
    "    selected_lists.append(feats)\n",
    "\n",
    "# Global intersection at the server\n",
    "global_frhc_indices = frhc_global_intersection(selected_lists)\n",
    "print(\"Count:\",len(global_frhc_indices))\n",
    "print(\"Global federated feature indices (FRHC):\", global_frhc_indices)\n",
    "print(\"Selected feature names:\", [feature_cols[i] for i in global_frhc_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices=global_frhc_indices\n",
    "X_sel = X[:, selected_indices]\n",
    "input_dim = X_sel.shape[1]\n",
    "full_dataset = TabularDataset(X_sel, y)\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "client_loaders = []\n",
    "for i in range(num_clients):\n",
    "    idxs = client_indices[i]\n",
    "    client_subset = Subset(train_dataset, idxs)\n",
    "    client_loader = DataLoader(client_subset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    client_loaders.append(client_loader)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=2):\n",
    "        super(TabularMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        features = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(features)\n",
    "        out = self.fc3(x)\n",
    "        if return_features:\n",
    "            return out, features\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Federated Round 1 (Local Epochs: 10)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before:  3.69% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 65.23% | Acc After: 96.43%\n",
      "  Client  3 | Samples: 278944 | Acc Before:  3.54% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 23.87% | Acc After: 93.29%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 11.63% | Acc After: 95.07%\n",
      "  Client  6 | Samples: 14413 | Acc Before:  4.39% | Acc After: 99.14%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 43.87% | Acc After: 87.71%\n",
      "  Client  8 | Samples: 123392 | Acc Before:  7.08% | Acc After: 97.58%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 32.15% | Acc After: 91.32%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 66.04% | Acc After: 97.18%\n",
      "\n",
      "[Round 1] Global Test Accuracy: 84.40%\n",
      "Client Acc BEFORE (mean ± std): 26.15% ± 23.53%\n",
      "Client Acc AFTER  (mean ± std): 95.76% ± 3.79%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 2 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.54% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 45.37% | Acc After: 96.45%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.61% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 81.67% | Acc After: 93.26%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 92.51% | Acc After: 95.97%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 98.98% | Acc After: 99.14%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 63.87% | Acc After: 89.56%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 96.43% | Acc After: 97.83%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 74.39% | Acc After: 91.50%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 44.45% | Acc After: 97.19%\n",
      "\n",
      "[Round 2] Global Test Accuracy: 89.84%\n",
      "Client Acc BEFORE (mean ± std): 79.68% ± 20.76%\n",
      "Client Acc AFTER  (mean ± std): 96.08% ± 3.40%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 3 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.27% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 65.46% | Acc After: 96.49%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.29% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 88.18% | Acc After: 93.34%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 94.79% | Acc After: 94.92%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 98.91% | Acc After: 99.25%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 77.17% | Acc After: 89.11%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.31% | Acc After: 97.55%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 83.64% | Acc After: 92.58%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 64.84% | Acc After: 97.20%\n",
      "\n",
      "[Round 3] Global Test Accuracy: 90.73%\n",
      "Client Acc BEFORE (mean ± std): 86.89% ± 12.94%\n",
      "Client Acc AFTER  (mean ± std): 96.04% ± 3.37%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 4 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.37% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 68.29% | Acc After: 96.45%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.41% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 89.15% | Acc After: 94.27%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 95.27% | Acc After: 96.21%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.14% | Acc After: 99.37%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 78.92% | Acc After: 90.70%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.60% | Acc After: 98.10%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 84.99% | Acc After: 92.27%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 67.53% | Acc After: 97.22%\n",
      "\n",
      "[Round 4] Global Test Accuracy: 91.43%\n",
      "Client Acc BEFORE (mean ± std): 87.97% ± 11.95%\n",
      "Client Acc AFTER  (mean ± std): 96.45% ± 3.03%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 5 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.39% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 70.66% | Acc After: 96.45%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.42% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 90.07% | Acc After: 94.37%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 95.59% | Acc After: 96.04%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.14% | Acc After: 99.27%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 80.61% | Acc After: 90.63%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.77% | Acc After: 97.80%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 86.10% | Acc After: 92.66%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 70.17% | Acc After: 97.20%\n",
      "\n",
      "[Round 5] Global Test Accuracy: 89.82%\n",
      "Client Acc BEFORE (mean ± std): 88.89% ± 11.00%\n",
      "Client Acc AFTER  (mean ± std): 96.44% ± 2.96%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 6 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.55% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 64.77% | Acc After: 96.48%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.61% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 88.15% | Acc After: 94.13%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 94.98% | Acc After: 96.51%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.23% | Acc After: 99.24%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 76.54% | Acc After: 89.76%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.57% | Acc After: 98.13%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 83.44% | Acc After: 91.60%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 63.85% | Acc After: 97.23%\n",
      "\n",
      "[Round 6] Global Test Accuracy: 90.84%\n",
      "Client Acc BEFORE (mean ± std): 86.77% ± 13.39%\n",
      "Client Acc AFTER  (mean ± std): 96.30% ± 3.31%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 7 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.56% | Acc After: 99.96%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 68.38% | Acc After: 96.45%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.61% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 89.34% | Acc After: 94.44%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 95.44% | Acc After: 96.69%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.24% | Acc After: 99.31%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 78.97% | Acc After: 90.92%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.79% | Acc After: 97.81%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 85.01% | Acc After: 92.26%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 67.65% | Acc After: 97.20%\n",
      "\n",
      "[Round 7] Global Test Accuracy: 90.24%\n",
      "Client Acc BEFORE (mean ± std): 88.10% ± 11.99%\n",
      "Client Acc AFTER  (mean ± std): 96.50% ± 2.96%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 8 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.60% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 66.13% | Acc After: 96.45%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.67% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 88.56% | Acc After: 94.19%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 95.22% | Acc After: 96.76%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.26% | Acc After: 99.41%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 77.34% | Acc After: 91.26%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.70% | Acc After: 98.29%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 84.04% | Acc After: 91.89%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 65.23% | Acc After: 97.23%\n",
      "\n",
      "[Round 8] Global Test Accuracy: 90.50%\n",
      "Client Acc BEFORE (mean ± std): 87.27% ± 12.90%\n",
      "Client Acc AFTER  (mean ± std): 96.54% ± 3.01%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 9 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.63% | Acc After: 99.96%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 66.97% | Acc After: 96.45%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.71% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 88.92% | Acc After: 94.23%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 95.38% | Acc After: 96.49%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.31% | Acc After: 99.44%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 78.00% | Acc After: 90.81%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.79% | Acc After: 98.32%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 84.42% | Acc After: 92.68%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 66.12% | Acc After: 97.26%\n",
      "\n",
      "[Round 9] Global Test Accuracy: 90.11%\n",
      "Client Acc BEFORE (mean ± std): 87.63% ± 12.58%\n",
      "Client Acc AFTER  (mean ± std): 96.56% ± 2.98%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 10 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.67% | Acc After: 99.96%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 65.43% | Acc After: 96.46%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.71% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 88.40% | Acc After: 94.24%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 95.13% | Acc After: 96.08%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.30% | Acc After: 99.33%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 77.07% | Acc After: 89.02%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.73% | Acc After: 98.33%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 83.73% | Acc After: 92.40%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 64.66% | Acc After: 97.21%\n",
      "\n",
      "[Round 10] Global Test Accuracy: 87.30%\n",
      "Client Acc BEFORE (mean ± std): 87.08% ± 13.15%\n",
      "Client Acc AFTER  (mean ± std): 96.30% ± 3.37%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 11 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.78% | Acc After: 99.96%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 55.30% | Acc After: 96.46%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.79% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 85.06% | Acc After: 94.23%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 93.91% | Acc After: 96.32%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.27% | Acc After: 99.37%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 70.53% | Acc After: 91.20%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.16% | Acc After: 97.07%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 79.17% | Acc After: 92.69%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 54.16% | Acc After: 97.20%\n",
      "\n",
      "[Round 11] Global Test Accuracy: 87.96%\n",
      "Client Acc BEFORE (mean ± std): 83.41% ± 17.09%\n",
      "Client Acc AFTER  (mean ± std): 96.45% ± 2.85%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 12 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.77% | Acc After: 99.96%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 57.52% | Acc After: 96.46%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.79% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 85.81% | Acc After: 93.94%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 94.18% | Acc After: 96.74%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.30% | Acc After: 99.43%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 71.97% | Acc After: 90.98%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.29% | Acc After: 97.65%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 80.22% | Acc After: 88.79%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 56.61% | Acc After: 97.24%\n",
      "\n",
      "[Round 12] Global Test Accuracy: 87.23%\n",
      "Client Acc BEFORE (mean ± std): 84.25% ± 16.20%\n",
      "Client Acc AFTER  (mean ± std): 96.12% ± 3.60%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 13 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.79% | Acc After: 99.96%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 54.95% | Acc After: 96.51%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.81% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 84.98% | Acc After: 93.23%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 93.87% | Acc After: 96.78%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.28% | Acc After: 99.40%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 70.30% | Acc After: 90.92%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.15% | Acc After: 98.08%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 78.97% | Acc After: 92.72%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 53.90% | Acc After: 97.20%\n",
      "\n",
      "[Round 13] Global Test Accuracy: 87.04%\n",
      "Client Acc BEFORE (mean ± std): 83.30% ± 17.21%\n",
      "Client Acc AFTER  (mean ± std): 96.48% ± 3.03%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 14 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.77% | Acc After: 99.96%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 54.37% | Acc After: 96.45%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.81% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 84.79% | Acc After: 94.44%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 93.80% | Acc After: 96.54%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.27% | Acc After: 99.47%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 69.93% | Acc After: 89.34%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.13% | Acc After: 97.94%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 78.65% | Acc After: 92.88%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 53.36% | Acc After: 97.20%\n",
      "\n",
      "[Round 14] Global Test Accuracy: 87.11%\n",
      "Client Acc BEFORE (mean ± std): 83.09% ± 17.42%\n",
      "Client Acc AFTER  (mean ± std): 96.42% ± 3.23%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 15 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.80% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 54.52% | Acc After: 96.46%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.82% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 84.85% | Acc After: 94.59%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 93.82% | Acc After: 96.38%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.27% | Acc After: 99.28%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 69.99% | Acc After: 90.89%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.14% | Acc After: 98.08%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 78.79% | Acc After: 92.64%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 53.53% | Acc After: 97.22%\n",
      "\n",
      "[Round 15] Global Test Accuracy: 87.25%\n",
      "Client Acc BEFORE (mean ± std): 83.15% ± 17.37%\n",
      "Client Acc AFTER  (mean ± std): 96.55% ± 2.91%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 16 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.78% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 55.07% | Acc After: 96.48%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.80% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 85.02% | Acc After: 94.21%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 93.89% | Acc After: 96.82%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.27% | Acc After: 99.43%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 70.37% | Acc After: 91.35%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.17% | Acc After: 98.14%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 79.06% | Acc After: 92.99%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 54.10% | Acc After: 97.23%\n",
      "\n",
      "[Round 16] Global Test Accuracy: 87.79%\n",
      "Client Acc BEFORE (mean ± std): 83.35% ± 17.15%\n",
      "Client Acc AFTER  (mean ± std): 96.66% ± 2.83%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 17 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.77% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 56.92% | Acc After: 96.47%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.80% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 85.66% | Acc After: 93.98%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 94.14% | Acc After: 96.81%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.29% | Acc After: 99.39%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 71.64% | Acc After: 90.92%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.26% | Acc After: 98.16%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 79.96% | Acc After: 60.23%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 56.14% | Acc After: 97.22%\n",
      "\n",
      "[Round 17] Global Test Accuracy: 87.12%\n",
      "Client Acc BEFORE (mean ± std): 84.06% ± 16.41%\n",
      "Client Acc AFTER  (mean ± std): 93.31% ± 11.35%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 18 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.80% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 54.55% | Acc After: 96.46%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.82% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 84.87% | Acc After: 93.92%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 93.84% | Acc After: 96.68%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.27% | Acc After: 99.34%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 70.09% | Acc After: 88.59%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.17% | Acc After: 97.89%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 78.84% | Acc After: 92.65%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 53.59% | Acc After: 97.21%\n",
      "\n",
      "[Round 18] Global Test Accuracy: 87.42%\n",
      "Client Acc BEFORE (mean ± std): 83.18% ± 17.35%\n",
      "Client Acc AFTER  (mean ± std): 96.27% ± 3.44%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 19 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.78% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 55.51% | Acc After: 96.49%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.81% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 85.20% | Acc After: 94.04%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 93.96% | Acc After: 96.80%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.24% | Acc After: 99.29%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 70.77% | Acc After: 91.17%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.20% | Acc After: 98.10%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 79.34% | Acc After: 92.49%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 54.72% | Acc After: 97.20%\n",
      "\n",
      "[Round 19] Global Test Accuracy: 88.23%\n",
      "Client Acc BEFORE (mean ± std): 83.55% ± 16.94%\n",
      "Client Acc AFTER  (mean ± std): 96.55% ± 2.92%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 20 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.73% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 58.42% | Acc After: 96.46%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.79% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 86.12% | Acc After: 93.21%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 94.35% | Acc After: 96.67%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.31% | Acc After: 99.45%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 72.45% | Acc After: 91.13%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 97.34% | Acc After: 98.00%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 80.59% | Acc After: 92.67%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 57.56% | Acc After: 97.21%\n",
      "\n",
      "[Round 20] Global Test Accuracy: 87.78%\n",
      "Client Acc BEFORE (mean ± std): 84.57% ± 15.86%\n",
      "Client Acc AFTER  (mean ± std): 96.47% ± 3.00%\n",
      "Client sample count (min, max): 14413, 278944\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_one_client(model, loader, epochs=1, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.cpu()\n",
    "\n",
    "def evaluate_local(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def average_weights(weight_list):\n",
    "    avg_weights = {}\n",
    "    for key in weight_list[0].keys():\n",
    "        avg_weights[key] = sum([w[key] for w in weight_list]) / len(weight_list)\n",
    "    return avg_weights\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "global_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "global_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "num_rounds = 20\n",
    "for rnd in range(1, num_rounds + 1):\n",
    "    adaptive_epochs = max(1, int(10 - 9 * (rnd-1) / (num_rounds-1)))\n",
    "    print(f\"\\n{'='*30}\\nFederated Round {rnd} (Local Epochs: {adaptive_epochs})\\n{'='*30}\")\n",
    "    local_weights = []\n",
    "    client_accuracies_before = []\n",
    "    client_accuracies_after = []\n",
    "    client_sample_counts = []\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        num_samples = len(client_loaders[client_id].dataset)\n",
    "        acc_before = evaluate_local(global_model, client_loaders[client_id])\n",
    "        local_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_model = train_one_client(local_model, client_loaders[client_id], epochs=adaptive_epochs)\n",
    "        acc_after = evaluate_local(local_model, client_loaders[client_id])\n",
    "        local_weights.append(local_model.state_dict())\n",
    "        client_sample_counts.append(num_samples)\n",
    "        client_accuracies_before.append(acc_before)\n",
    "        client_accuracies_after.append(acc_after)\n",
    "        print(f\"  Client {client_id+1:2d} | Samples: {num_samples:4d} | Acc Before: {acc_before:5.2f}% | Acc After: {acc_after:5.2f}%\")\n",
    "\n",
    "    global_model.load_state_dict(average_weights(local_weights))\n",
    "    acc_global = test_model(global_model, test_loader)\n",
    "    print(f\"\\n[Round {rnd}] Global Test Accuracy: {acc_global:.2f}%\")\n",
    "    print(f\"Client Acc BEFORE (mean ± std): {np.mean(client_accuracies_before):.2f}% ± {np.std(client_accuracies_before):.2f}%\")\n",
    "    print(f\"Client Acc AFTER  (mean ± std): {np.mean(client_accuracies_after):.2f}% ± {np.std(client_accuracies_after):.2f}%\")\n",
    "    print(f\"Client sample count (min, max): {min(client_sample_counts)}, {max(client_sample_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 87.78%\n",
      "Confusion Matrix:\n",
      " [[180074    323]\n",
      " [ 29730  35771]]\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "acc, cm = test_modelv2(global_model, test_loader)\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel = X\n",
    "input_dim = X_sel.shape[1]\n",
    "full_dataset = TabularDataset(X_sel, y)\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "client_loaders = []\n",
    "for i in range(num_clients):\n",
    "    idxs = client_indices[i]\n",
    "    client_subset = Subset(train_dataset, idxs)\n",
    "    client_loader = DataLoader(client_subset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    client_loaders.append(client_loader)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=2):\n",
    "        super(TabularMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        features = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(features)\n",
    "        out = self.fc3(x)\n",
    "        if return_features:\n",
    "            return out, features\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Federated Round 1 (Local Epochs: 10)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 97.53% | Acc After: 99.95%\n",
      "  Client  2 | Samples: 40075 | Acc Before:  8.10% | Acc After: 98.43%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 97.64% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 68.42% | Acc After: 97.95%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 86.01% | Acc After: 98.77%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 96.55% | Acc After: 99.55%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 38.98% | Acc After: 97.38%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 92.56% | Acc After: 99.11%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 56.40% | Acc After: 97.34%\n",
      "  Client 10 | Samples: 79317 | Acc Before:  7.25% | Acc After: 98.49%\n",
      "\n",
      "[Round 1] Global Test Accuracy: 93.43%\n",
      "Client Acc BEFORE (mean ± std): 64.95% ± 34.14%\n",
      "Client Acc AFTER  (mean ± std): 98.70% ± 0.92%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 2 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.74% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 77.23% | Acc After: 98.45%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.75% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 92.40% | Acc After: 97.97%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 96.80% | Acc After: 98.85%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.48% | Acc After: 99.73%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 84.84% | Acc After: 97.56%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 98.39% | Acc After: 99.18%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 89.18% | Acc After: 97.51%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 76.85% | Acc After: 98.52%\n",
      "\n",
      "[Round 2] Global Test Accuracy: 97.55%\n",
      "Client Acc BEFORE (mean ± std): 91.46% ± 8.62%\n",
      "Client Acc AFTER  (mean ± std): 98.77% ± 0.89%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 3 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.56% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 92.66% | Acc After: 98.47%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.55% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.31% | Acc After: 98.05%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.67% | Acc After: 98.86%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.46% | Acc After: 99.78%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.00% | Acc After: 97.52%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.16% | Acc After: 99.28%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 96.33% | Acc After: 97.54%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 92.49% | Acc After: 98.51%\n",
      "\n",
      "[Round 3] Global Test Accuracy: 97.78%\n",
      "Client Acc BEFORE (mean ± std): 97.02% ± 2.65%\n",
      "Client Acc AFTER  (mean ± std): 98.80% ± 0.89%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 4 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.59% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 93.43% | Acc After: 98.46%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.58% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.61% | Acc After: 98.14%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.77% | Acc After: 98.90%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.46% | Acc After: 99.81%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.47% | Acc After: 97.55%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.23% | Acc After: 99.30%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 96.74% | Acc After: 97.71%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 93.23% | Acc After: 98.50%\n",
      "\n",
      "[Round 4] Global Test Accuracy: 97.88%\n",
      "Client Acc BEFORE (mean ± std): 97.31% ± 2.37%\n",
      "Client Acc AFTER  (mean ± std): 98.84% ± 0.86%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 5 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.59% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 93.69% | Acc After: 98.49%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.59% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.71% | Acc After: 98.01%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.84% | Acc After: 98.97%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.48% | Acc After: 99.84%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.62% | Acc After: 97.69%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.24% | Acc After: 99.31%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 96.85% | Acc After: 97.70%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 93.55% | Acc After: 98.49%\n",
      "\n",
      "[Round 5] Global Test Accuracy: 97.89%\n",
      "Client Acc BEFORE (mean ± std): 97.42% ± 2.27%\n",
      "Client Acc AFTER  (mean ± std): 98.85% ± 0.86%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 6 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.62% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 93.71% | Acc After: 98.44%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.62% | Acc After: 100.00%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.73% | Acc After: 98.12%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.88% | Acc After: 99.00%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.50% | Acc After: 99.84%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.63% | Acc After: 97.70%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.27% | Acc After: 99.33%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 96.87% | Acc After: 97.69%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 93.52% | Acc After: 98.49%\n",
      "\n",
      "[Round 6] Global Test Accuracy: 97.97%\n",
      "Client Acc BEFORE (mean ± std): 97.43% ± 2.28%\n",
      "Client Acc AFTER  (mean ± std): 98.86% ± 0.86%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 7 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.61% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 93.98% | Acc After: 98.51%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.61% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.80% | Acc After: 98.16%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.90% | Acc After: 99.00%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.49% | Acc After: 99.83%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.83% | Acc After: 97.65%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.29% | Acc After: 99.35%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 96.99% | Acc After: 97.72%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 93.80% | Acc After: 98.50%\n",
      "\n",
      "[Round 7] Global Test Accuracy: 97.90%\n",
      "Client Acc BEFORE (mean ± std): 97.53% ± 2.17%\n",
      "Client Acc AFTER  (mean ± std): 98.87% ± 0.85%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 8 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.64% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 93.73% | Acc After: 98.52%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.64% | Acc After: 100.00%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.73% | Acc After: 98.19%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.88% | Acc After: 99.02%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.53% | Acc After: 99.90%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.66% | Acc After: 97.78%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.29% | Acc After: 99.35%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 96.86% | Acc After: 97.73%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 93.47% | Acc After: 99.39%\n",
      "\n",
      "[Round 8] Global Test Accuracy: 98.03%\n",
      "Client Acc BEFORE (mean ± std): 97.44% ± 2.29%\n",
      "Client Acc AFTER  (mean ± std): 98.99% ± 0.84%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 9 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.60% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.21% | Acc After: 98.54%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.61% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.85% | Acc After: 98.17%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.92% | Acc After: 98.99%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.48% | Acc After: 99.88%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.98% | Acc After: 97.74%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.29% | Acc After: 99.38%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.09% | Acc After: 97.73%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.05% | Acc After: 98.54%\n",
      "\n",
      "[Round 9] Global Test Accuracy: 98.01%\n",
      "Client Acc BEFORE (mean ± std): 97.61% ± 2.08%\n",
      "Client Acc AFTER  (mean ± std): 98.90% ± 0.84%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 10 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.61% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.14% | Acc After: 98.55%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.62% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.84% | Acc After: 98.20%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.93% | Acc After: 99.04%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.48% | Acc After: 99.87%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.93% | Acc After: 97.72%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.31% | Acc After: 99.37%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.07% | Acc After: 97.76%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 93.95% | Acc After: 98.54%\n",
      "\n",
      "[Round 10] Global Test Accuracy: 97.98%\n",
      "Client Acc BEFORE (mean ± std): 97.59% ± 2.12%\n",
      "Client Acc AFTER  (mean ± std): 98.90% ± 0.83%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 11 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.64% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.02% | Acc After: 98.49%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.65% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.80% | Acc After: 98.15%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.92% | Acc After: 99.05%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.53% | Acc After: 99.88%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.86% | Acc After: 97.80%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.31% | Acc After: 99.39%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.01% | Acc After: 98.21%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 93.78% | Acc After: 98.53%\n",
      "\n",
      "[Round 11] Global Test Accuracy: 98.12%\n",
      "Client Acc BEFORE (mean ± std): 97.55% ± 2.18%\n",
      "Client Acc AFTER  (mean ± std): 98.95% ± 0.78%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 12 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.61% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.47% | Acc After: 98.57%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.62% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.95% | Acc After: 98.23%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.97% | Acc After: 98.98%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.52% | Acc After: 99.87%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 96.13% | Acc After: 97.80%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.33% | Acc After: 99.40%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.24% | Acc After: 97.76%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.28% | Acc After: 98.51%\n",
      "\n",
      "[Round 12] Global Test Accuracy: 98.08%\n",
      "Client Acc BEFORE (mean ± std): 97.71% ± 1.99%\n",
      "Client Acc AFTER  (mean ± std): 98.91% ± 0.82%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 13 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.61% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.35% | Acc After: 98.50%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.63% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.90% | Acc After: 98.22%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.96% | Acc After: 99.04%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.53% | Acc After: 99.90%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 96.05% | Acc After: 97.82%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.32% | Acc After: 99.37%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.16% | Acc After: 97.82%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.11% | Acc After: 99.20%\n",
      "\n",
      "[Round 13] Global Test Accuracy: 98.05%\n",
      "Client Acc BEFORE (mean ± std): 97.66% ± 2.05%\n",
      "Client Acc AFTER  (mean ± std): 98.98% ± 0.81%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 14 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.68% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.05% | Acc After: 98.53%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.70% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.85% | Acc After: 98.14%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.97% | Acc After: 99.06%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.61% | Acc After: 99.87%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.87% | Acc After: 97.78%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.37% | Acc After: 99.40%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.07% | Acc After: 97.71%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 93.85% | Acc After: 98.62%\n",
      "\n",
      "[Round 14] Global Test Accuracy: 98.10%\n",
      "Client Acc BEFORE (mean ± std): 97.60% ± 2.18%\n",
      "Client Acc AFTER  (mean ± std): 98.91% ± 0.84%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 15 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.66% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.31% | Acc After: 98.53%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.67% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.91% | Acc After: 98.22%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 99.00% | Acc After: 99.10%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.55% | Acc After: 99.89%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.99% | Acc After: 97.70%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.37% | Acc After: 99.33%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.15% | Acc After: 97.77%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.07% | Acc After: 98.53%\n",
      "\n",
      "[Round 15] Global Test Accuracy: 98.10%\n",
      "Client Acc BEFORE (mean ± std): 97.67% ± 2.09%\n",
      "Client Acc AFTER  (mean ± std): 98.90% ± 0.84%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 16 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.67% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.24% | Acc After: 98.50%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.68% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.90% | Acc After: 98.28%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 98.99% | Acc After: 99.01%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.54% | Acc After: 99.87%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 95.95% | Acc After: 97.76%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.37% | Acc After: 99.41%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.12% | Acc After: 97.74%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.00% | Acc After: 98.54%\n",
      "\n",
      "[Round 16] Global Test Accuracy: 98.16%\n",
      "Client Acc BEFORE (mean ± std): 97.65% ± 2.11%\n",
      "Client Acc AFTER  (mean ± std): 98.91% ± 0.83%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 17 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.62% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.60% | Acc After: 98.47%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.64% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.98% | Acc After: 98.22%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 99.00% | Acc After: 99.05%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.53% | Acc After: 99.87%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 96.23% | Acc After: 97.69%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.33% | Acc After: 99.33%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.29% | Acc After: 97.85%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.43% | Acc After: 98.52%\n",
      "\n",
      "[Round 17] Global Test Accuracy: 98.17%\n",
      "Client Acc BEFORE (mean ± std): 97.77% ± 1.94%\n",
      "Client Acc AFTER  (mean ± std): 98.90% ± 0.83%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 18 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.65% | Acc After: 99.97%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.54% | Acc After: 98.46%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.65% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.96% | Acc After: 98.25%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 99.02% | Acc After: 98.98%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.57% | Acc After: 99.80%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 96.21% | Acc After: 97.80%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.36% | Acc After: 99.41%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.26% | Acc After: 97.76%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.37% | Acc After: 98.50%\n",
      "\n",
      "[Round 18] Global Test Accuracy: 98.19%\n",
      "Client Acc BEFORE (mean ± std): 97.76% ± 1.98%\n",
      "Client Acc AFTER  (mean ± std): 98.89% ± 0.82%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 19 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.61% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.76% | Acc After: 98.51%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.61% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 98.00% | Acc After: 98.27%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 99.01% | Acc After: 98.98%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.53% | Acc After: 99.80%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 96.33% | Acc After: 97.81%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.33% | Acc After: 99.36%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.34% | Acc After: 97.75%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.60% | Acc After: 98.50%\n",
      "\n",
      "[Round 19] Global Test Accuracy: 98.17%\n",
      "Client Acc BEFORE (mean ± std): 97.81% ± 1.88%\n",
      "Client Acc AFTER  (mean ± std): 98.90% ± 0.81%\n",
      "Client sample count (min, max): 14413, 278944\n",
      "\n",
      "==============================\n",
      "Federated Round 20 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 43291 | Acc Before: 99.65% | Acc After: 99.98%\n",
      "  Client  2 | Samples: 40075 | Acc Before: 94.57% | Acc After: 98.48%\n",
      "  Client  3 | Samples: 278944 | Acc Before: 99.66% | Acc After: 99.99%\n",
      "  Client  4 | Samples: 142982 | Acc Before: 97.98% | Acc After: 98.21%\n",
      "  Client  5 | Samples: 101348 | Acc Before: 99.02% | Acc After: 99.02%\n",
      "  Client  6 | Samples: 14413 | Acc Before: 99.56% | Acc After: 99.85%\n",
      "  Client  7 | Samples: 63050 | Acc Before: 96.20% | Acc After: 97.71%\n",
      "  Client  8 | Samples: 123392 | Acc Before: 99.37% | Acc After: 99.40%\n",
      "  Client  9 | Samples: 96777 | Acc Before: 97.27% | Acc After: 97.73%\n",
      "  Client 10 | Samples: 79317 | Acc Before: 94.41% | Acc After: 99.12%\n",
      "\n",
      "[Round 20] Global Test Accuracy: 98.14%\n",
      "Client Acc BEFORE (mean ± std): 97.77% ± 1.97%\n",
      "Client Acc AFTER  (mean ± std): 98.95% ± 0.84%\n",
      "Client sample count (min, max): 14413, 278944\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_one_client(model, loader, epochs=1, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.cpu()\n",
    "\n",
    "def evaluate_local(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def average_weights(weight_list):\n",
    "    avg_weights = {}\n",
    "    for key in weight_list[0].keys():\n",
    "        avg_weights[key] = sum([w[key] for w in weight_list]) / len(weight_list)\n",
    "    return avg_weights\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "global_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "global_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "num_rounds = 20\n",
    "for rnd in range(1, num_rounds + 1):\n",
    "    adaptive_epochs = max(1, int(10 - 9 * (rnd-1) / (num_rounds-1)))\n",
    "    print(f\"\\n{'='*30}\\nFederated Round {rnd} (Local Epochs: {adaptive_epochs})\\n{'='*30}\")\n",
    "    local_weights = []\n",
    "    client_accuracies_before = []\n",
    "    client_accuracies_after = []\n",
    "    client_sample_counts = []\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        num_samples = len(client_loaders[client_id].dataset)\n",
    "        acc_before = evaluate_local(global_model, client_loaders[client_id])\n",
    "        local_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_model = train_one_client(local_model, client_loaders[client_id], epochs=adaptive_epochs)\n",
    "        acc_after = evaluate_local(local_model, client_loaders[client_id])\n",
    "        local_weights.append(local_model.state_dict())\n",
    "        client_sample_counts.append(num_samples)\n",
    "        client_accuracies_before.append(acc_before)\n",
    "        client_accuracies_after.append(acc_after)\n",
    "        print(f\"  Client {client_id+1:2d} | Samples: {num_samples:4d} | Acc Before: {acc_before:5.2f}% | Acc After: {acc_after:5.2f}%\")\n",
    "\n",
    "    global_model.load_state_dict(average_weights(local_weights))\n",
    "    acc_global = test_model(global_model, test_loader)\n",
    "    print(f\"\\n[Round {rnd}] Global Test Accuracy: {acc_global:.2f}%\")\n",
    "    print(f\"Client Acc BEFORE (mean ± std): {np.mean(client_accuracies_before):.2f}% ± {np.std(client_accuracies_before):.2f}%\")\n",
    "    print(f\"Client Acc AFTER  (mean ± std): {np.mean(client_accuracies_after):.2f}% ± {np.std(client_accuracies_after):.2f}%\")\n",
    "    print(f\"Client sample count (min, max): {min(client_sample_counts)}, {max(client_sample_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.14%\n",
      "Confusion Matrix:\n",
      " [[179796    601]\n",
      " [  3970  61531]]\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "acc, cm = test_modelv2(global_model, test_loader)\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1303049,
     "sourceId": 2260912,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
