{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.feature_selection import f_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For IoT IDS\n",
    "csv_path = 'ACI/ACI-IoT-2023.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "#print(df.columns)\n",
    "#print(df.shape)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label\n",
      "Port Scan             441260\n",
      "Benign                327505\n",
      "ICMP Flood            225234\n",
      "Ping Sweep             71928\n",
      "DNS Flood              46934\n",
      "Vulnerability Scan     39533\n",
      "OS Scan                37524\n",
      "Slowloris              18537\n",
      "SYN Flood              13857\n",
      "Dictionary Attack       6379\n",
      "UDP Flood                791\n",
      "Name: count, dtype: int64\n",
      "class\n",
      "Recon              590245\n",
      "Benign             327505\n",
      "DoS                305353\n",
      "Dict_BruteForce      6379\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df[df['Label'] != 'ARP Spoofing']\n",
    "print(df['Label'].value_counts())\n",
    "# Now, remap your classes as before\n",
    "def map_to_class(label):\n",
    "    if label == 'Benign':\n",
    "        return 'Benign'\n",
    "    elif label in ['ICMP Flood', 'DNS Flood', 'Slowloris', 'SYN Flood', 'UDP Flood']:\n",
    "        return 'DoS'\n",
    "    elif label in ['Port Scan', 'Ping Sweep', 'Vulnerability Scan', 'OS Scan']:\n",
    "        return 'Recon'\n",
    "    elif 'Dictionary' in label:\n",
    "        return 'Dict_BruteForce'\n",
    "    else:\n",
    "        return 'Other'\n",
    "df['class'] = df['Label'].apply(map_to_class)\n",
    "print(df['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['Flow ID', 'Src IP', 'Src Port', 'Dst IP','Dst Port', 'Protocol', 'Timestamp','Label','Connection Type','class']\n",
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "label_col = 'class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1229482, 86)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [c for c in df.columns if c not in exclude_cols]\n",
    "X = df[feature_cols].values\n",
    "y = df[label_col].values\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant features removed: 6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold=0.0)\n",
    "X_var = var_thresh.fit_transform(X)\n",
    "print(f\"Constant features removed: {X.shape[1] - X_var.shape[1]}\")\n",
    "X = X_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 1 class distribution: [ 2601   479 68457  5345]\n",
      "Client 2 class distribution: [72015   236   847 14418]\n",
      "Client 3 class distribution: [  8477    356   5471 157404]\n",
      "Client 4 class distribution: [38472   750 43480 35999]\n",
      "Client 5 class distribution: [27919   723 39167  6701]\n",
      "Client 6 class distribution: [ 2078  1168 14100 58485]\n",
      "Client 7 class distribution: [90856   119  4484    90]\n",
      "Client 8 class distribution: [  772    79 67422 61714]\n",
      "Client 9 class distribution: [  677   201   171 28571]\n",
      "Client 10 class distribution: [ 18137    992    683 103469]\n"
     ]
    }
   ],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "full_dataset = TabularDataset(X, y)\n",
    "train_idx, test_idx = train_test_split(np.arange(len(full_dataset)), test_size=0.2, stratify=y, random_state=6)\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "def partition_tabular_dataset(dataset, labels, train_idx, num_clients=10, alpha=0.5):\n",
    "    np.random.seed(6)\n",
    "    targets = np.array(labels)[train_idx]\n",
    "    num_classes = np.max(targets) + 1\n",
    "    idxs = np.arange(len(targets))\n",
    "    client_idx = [[] for _ in range(num_clients)]\n",
    "    for c in range(num_classes):\n",
    "        idx_c = idxs[targets == c]\n",
    "        np.random.shuffle(idx_c)\n",
    "        proportions = np.random.dirichlet([alpha]*num_clients)\n",
    "        proportions = (np.cumsum(proportions) * len(idx_c)).astype(int)[:-1]\n",
    "        split_idxs = np.split(idx_c, proportions)\n",
    "        for i, idx in enumerate(split_idxs):\n",
    "            client_idx[i].extend(idx)\n",
    "    return client_idx\n",
    "\n",
    "num_clients = 10\n",
    "alpha = 0.5\n",
    "client_indices = partition_tabular_dataset(train_dataset, y, train_idx, num_clients, alpha)\n",
    "\n",
    "client_data_np = []\n",
    "for i in range(num_clients):\n",
    "    idxs = client_indices[i]\n",
    "    X_client = X[train_idx][idxs]\n",
    "    y_client = y[train_idx][idxs]\n",
    "    client_data_np.append((X_client, y_client))\n",
    "\n",
    "for i, (Xc, yc) in enumerate(client_data_np):\n",
    "    print(f\"Client {i+1} class distribution:\", np.bincount(yc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_scores(X, y):\n",
    "    scores, _ = f_classif(X, y)\n",
    "    # Normalize scores to [0,1]\n",
    "    min_val = np.min(scores)\n",
    "    max_val = np.max(scores)\n",
    "    if max_val > min_val:\n",
    "        normalized_scores = (scores - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        normalized_scores = np.zeros_like(scores)\n",
    "    return normalized_scores\n",
    "\n",
    "def compute_corr_matrix(X):\n",
    "    corr = np.corrcoef(X, rowvar=False)\n",
    "    return np.abs(corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_feature_subset(subset, fisher_scores, corr_matrix, penalty_lambda=0.7):\n",
    "    if len(subset) == 0:\n",
    "        return 0\n",
    "    fisher_sum = np.sum(fisher_scores[subset])\n",
    "    if len(subset) > 1:\n",
    "        corr_penalty = np.sum(corr_matrix[np.ix_(subset, subset)]) - np.sum(np.diag(corr_matrix[subset][:, subset]))\n",
    "        corr_penalty /= 2\n",
    "    else:\n",
    "        corr_penalty = 0.0\n",
    "    return penalty_lambda * fisher_sum - (1 - penalty_lambda) * corr_penalty\n",
    "\n",
    "def one_step_binary_firefly(\n",
    "    firefly_mask_prev, global_mask_prev, local_best_mask_prev,\n",
    "    fisher_scores, corr_matrix, penalty_lambda=0.7, p_global=0.3, p_local=0.3, mutation_rate=0.05, verbose=False\n",
    "):\n",
    "    n_features = len(firefly_mask_prev)\n",
    "    new_mask = firefly_mask_prev.copy()\n",
    "    for i in range(n_features):\n",
    "        r = np.random.rand()\n",
    "        if r < p_global:\n",
    "            new_mask[i] = global_mask_prev[i]\n",
    "        elif r < p_global + p_local:\n",
    "            new_mask[i] = local_best_mask_prev[i]\n",
    "        elif np.random.rand() < mutation_rate:\n",
    "            new_mask[i] = 1 - new_mask[i]  # mutate\n",
    "\n",
    "    # Optional: flip one bit with small probability for extra exploration\n",
    "    if np.random.rand() < 0.2:\n",
    "        idx = np.random.randint(n_features)\n",
    "        new_mask[idx] = 1 - new_mask[idx]\n",
    "\n",
    "    if verbose:\n",
    "        sel = np.where(new_mask)[0]\n",
    "        fit = evaluate_feature_subset(sel, fisher_scores, corr_matrix, penalty_lambda)\n",
    "        print(f\"    - New mask: {np.sum(new_mask)} features, Fitness: {fit:.4f}\")\n",
    "\n",
    "    return new_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================ Federated BFA Round 1 ================\n",
      "  Adaptive rho for this round: 0.20\n",
      "    - New mask: 44 features, Fitness: -12.8469\n",
      "    - New mask: 42 features, Fitness: -12.2990\n",
      "    - New mask: 44 features, Fitness: -13.2229\n",
      "    - New mask: 44 features, Fitness: -13.4507\n",
      "    - New mask: 43 features, Fitness: -8.8960\n",
      "    - New mask: 47 features, Fitness: -14.4079\n",
      "    - New mask: 48 features, Fitness: -14.7118\n",
      "    - New mask: 52 features, Fitness: -15.0888\n",
      "    - New mask: 43 features, Fitness: -10.4039\n",
      "    - New mask: 36 features, Fitness: -5.4128\n",
      "    - New mask: 38 features, Fitness: -8.2706\n",
      "    - New mask: 46 features, Fitness: -13.6762\n",
      "    - New mask: 46 features, Fitness: -13.5800\n",
      "    - New mask: 42 features, Fitness: -9.4245\n",
      "    - New mask: 41 features, Fitness: -10.4170\n",
      "    - New mask: 48 features, Fitness: -13.0922\n",
      "    - New mask: 46 features, Fitness: -12.1614\n",
      "    - New mask: 48 features, Fitness: -13.9882\n",
      "    - New mask: 39 features, Fitness: -9.2091\n",
      "    - New mask: 34 features, Fitness: -4.7778\n",
      "    - New mask: 42 features, Fitness: -10.5611\n",
      "    - New mask: 35 features, Fitness: -9.0478\n",
      "    - New mask: 36 features, Fitness: -8.1330\n",
      "    - New mask: 44 features, Fitness: -11.5672\n",
      "    - New mask: 45 features, Fitness: -11.1476\n",
      "    - New mask: 39 features, Fitness: -9.9965\n",
      "    - New mask: 46 features, Fitness: -13.1475\n",
      "    - New mask: 41 features, Fitness: -9.8525\n",
      "    - New mask: 46 features, Fitness: -13.1013\n",
      "    - New mask: 37 features, Fitness: -8.3444\n",
      "    - New mask: 45 features, Fitness: -12.1118\n",
      "    - New mask: 49 features, Fitness: -14.2420\n",
      "    - New mask: 39 features, Fitness: -9.5813\n",
      "    - New mask: 38 features, Fitness: -9.5963\n",
      "    - New mask: 43 features, Fitness: -9.8747\n",
      "    - New mask: 39 features, Fitness: -8.0599\n",
      "    - New mask: 43 features, Fitness: -12.7754\n",
      "    - New mask: 43 features, Fitness: -10.9001\n",
      "    - New mask: 42 features, Fitness: -8.6226\n",
      "    - New mask: 43 features, Fitness: -11.6548\n",
      "    - New mask: 44 features, Fitness: -10.3058\n",
      "    - New mask: 48 features, Fitness: -13.6897\n",
      "    - New mask: 39 features, Fitness: -7.9854\n",
      "    - New mask: 43 features, Fitness: -9.0048\n",
      "    - New mask: 40 features, Fitness: -9.5891\n",
      "    - New mask: 43 features, Fitness: -10.0131\n",
      "    - New mask: 42 features, Fitness: -10.0915\n",
      "    - New mask: 47 features, Fitness: -12.6886\n",
      "    - New mask: 39 features, Fitness: -7.3447\n",
      "    - New mask: 45 features, Fitness: -10.5356\n",
      "    - New mask: 49 features, Fitness: -14.3464\n",
      "    - New mask: 42 features, Fitness: -8.4655\n",
      "    - New mask: 38 features, Fitness: -7.9334\n",
      "    - New mask: 46 features, Fitness: -12.3248\n",
      "    - New mask: 47 features, Fitness: -12.0341\n",
      "    - New mask: 40 features, Fitness: -9.1816\n",
      "    - New mask: 44 features, Fitness: -9.5918\n",
      "    - New mask: 39 features, Fitness: -8.5028\n",
      "    - New mask: 45 features, Fitness: -9.7391\n",
      "    - New mask: 48 features, Fitness: -12.0752\n",
      "    - New mask: 39 features, Fitness: -6.3295\n",
      "    - New mask: 42 features, Fitness: -9.1350\n",
      "    - New mask: 36 features, Fitness: -4.8058\n",
      "    - New mask: 44 features, Fitness: -9.5497\n",
      "    - New mask: 47 features, Fitness: -12.7691\n",
      "    - New mask: 46 features, Fitness: -9.8327\n",
      "    - New mask: 46 features, Fitness: -10.0307\n",
      "    - New mask: 44 features, Fitness: -8.3385\n",
      "    - New mask: 49 features, Fitness: -12.8696\n",
      "    - New mask: 42 features, Fitness: -10.0872\n",
      "    - New mask: 50 features, Fitness: -13.2865\n",
      "    - New mask: 44 features, Fitness: -10.0342\n",
      "    - New mask: 52 features, Fitness: -15.8127\n",
      "    - New mask: 49 features, Fitness: -14.0775\n",
      "    - New mask: 49 features, Fitness: -11.9459\n",
      "    - New mask: 42 features, Fitness: -9.7123\n",
      "    - New mask: 37 features, Fitness: -8.5234\n",
      "    - New mask: 50 features, Fitness: -14.4201\n",
      "    - New mask: 42 features, Fitness: -8.2596\n",
      "    - New mask: 42 features, Fitness: -8.3823\n",
      "    - New mask: 36 features, Fitness: -5.8297\n",
      "    - New mask: 41 features, Fitness: -8.8125\n",
      "    - New mask: 40 features, Fitness: -5.6392\n",
      "    - New mask: 42 features, Fitness: -9.3950\n",
      "    - New mask: 45 features, Fitness: -8.7539\n",
      "    - New mask: 46 features, Fitness: -9.3904\n",
      "    - New mask: 44 features, Fitness: -11.1605\n",
      "    - New mask: 47 features, Fitness: -10.2146\n",
      "    - New mask: 39 features, Fitness: -5.3917\n",
      "    - New mask: 41 features, Fitness: -7.9948\n",
      "    - New mask: 51 features, Fitness: -13.3835\n",
      "    - New mask: 40 features, Fitness: -8.0148\n",
      "    - New mask: 42 features, Fitness: -8.0928\n",
      "    - New mask: 43 features, Fitness: -7.9940\n",
      "    - New mask: 36 features, Fitness: -4.5860\n",
      "    - New mask: 44 features, Fitness: -9.5597\n",
      "    - New mask: 36 features, Fitness: -7.4243\n",
      "    - New mask: 43 features, Fitness: -7.0030\n",
      "    - New mask: 46 features, Fitness: -10.4744\n",
      "    - New mask: 44 features, Fitness: -8.8247\n",
      "    - New mask: 42 features, Fitness: -12.2779\n",
      "    - New mask: 42 features, Fitness: -9.4751\n",
      "    - New mask: 47 features, Fitness: -14.0257\n",
      "    - New mask: 51 features, Fitness: -14.7640\n",
      "    - New mask: 42 features, Fitness: -9.8270\n",
      "    - New mask: 45 features, Fitness: -11.7941\n",
      "    - New mask: 48 features, Fitness: -14.1472\n",
      "    - New mask: 43 features, Fitness: -11.8589\n",
      "    - New mask: 43 features, Fitness: -10.9026\n",
      "    - New mask: 41 features, Fitness: -8.4810\n",
      "    - New mask: 48 features, Fitness: -12.7338\n",
      "    - New mask: 48 features, Fitness: -13.9772\n",
      "    - New mask: 44 features, Fitness: -11.2775\n",
      "    - New mask: 43 features, Fitness: -10.2993\n",
      "    - New mask: 45 features, Fitness: -10.5466\n",
      "    - New mask: 47 features, Fitness: -11.1045\n",
      "    - New mask: 44 features, Fitness: -9.9560\n",
      "    - New mask: 37 features, Fitness: -6.5863\n",
      "    - New mask: 46 features, Fitness: -11.8408\n",
      "    - New mask: 44 features, Fitness: -11.7010\n",
      "    - New mask: 41 features, Fitness: -7.9291\n",
      "    - New mask: 44 features, Fitness: -11.1462\n",
      "    - New mask: 52 features, Fitness: -13.8663\n",
      "    - New mask: 44 features, Fitness: -9.4192\n",
      "    - New mask: 45 features, Fitness: -9.6590\n",
      "    - New mask: 48 features, Fitness: -11.9419\n",
      "    - New mask: 47 features, Fitness: -13.2872\n",
      "    - New mask: 47 features, Fitness: -10.0127\n",
      "    - New mask: 44 features, Fitness: -9.5229\n",
      "    - New mask: 50 features, Fitness: -12.1300\n",
      "    - New mask: 44 features, Fitness: -11.0036\n",
      "    - New mask: 41 features, Fitness: -6.8212\n",
      "    - New mask: 49 features, Fitness: -11.4000\n",
      "    - New mask: 45 features, Fitness: -7.4131\n",
      "    - New mask: 38 features, Fitness: -7.5513\n",
      "    - New mask: 46 features, Fitness: -11.1872\n",
      "    - New mask: 39 features, Fitness: -6.1692\n",
      "    - New mask: 42 features, Fitness: -7.3315\n",
      "    - New mask: 42 features, Fitness: -8.9510\n",
      "    - New mask: 48 features, Fitness: -10.9445\n",
      "    - New mask: 44 features, Fitness: -13.0102\n",
      "    - New mask: 48 features, Fitness: -17.3585\n",
      "    - New mask: 46 features, Fitness: -14.3441\n",
      "    - New mask: 43 features, Fitness: -15.7601\n",
      "    - New mask: 45 features, Fitness: -14.2700\n",
      "    - New mask: 40 features, Fitness: -10.8923\n",
      "    - New mask: 42 features, Fitness: -12.1610\n",
      "    - New mask: 50 features, Fitness: -19.0716\n",
      "    - New mask: 40 features, Fitness: -9.0661\n",
      "    - New mask: 40 features, Fitness: -11.0066\n",
      "    - New mask: 43 features, Fitness: -15.3438\n",
      "    - New mask: 43 features, Fitness: -14.8080\n",
      "    - New mask: 45 features, Fitness: -15.0931\n",
      "    - New mask: 39 features, Fitness: -10.6286\n",
      "    - New mask: 41 features, Fitness: -13.2603\n",
      "    - New mask: 48 features, Fitness: -16.8402\n",
      "    - New mask: 48 features, Fitness: -16.6466\n",
      "    - New mask: 40 features, Fitness: -10.5144\n",
      "    - New mask: 48 features, Fitness: -17.7403\n",
      "    - New mask: 44 features, Fitness: -17.2189\n",
      "    - New mask: 45 features, Fitness: -15.1024\n",
      "    - New mask: 43 features, Fitness: -13.9492\n",
      "    - New mask: 46 features, Fitness: -13.9571\n",
      "    - New mask: 36 features, Fitness: -8.0474\n",
      "    - New mask: 39 features, Fitness: -9.6580\n",
      "    - New mask: 42 features, Fitness: -16.0565\n",
      "    - New mask: 42 features, Fitness: -10.7915\n",
      "    - New mask: 46 features, Fitness: -18.3416\n",
      "    - New mask: 44 features, Fitness: -14.0783\n",
      "    - New mask: 42 features, Fitness: -15.3959\n",
      "    - New mask: 44 features, Fitness: -16.1534\n",
      "    - New mask: 41 features, Fitness: -9.7786\n",
      "    - New mask: 46 features, Fitness: -13.5752\n",
      "    - New mask: 37 features, Fitness: -9.0135\n",
      "    - New mask: 36 features, Fitness: -11.3475\n",
      "    - New mask: 42 features, Fitness: -12.9389\n",
      "    - New mask: 45 features, Fitness: -14.9203\n",
      "    - New mask: 44 features, Fitness: -14.3593\n",
      "    - New mask: 39 features, Fitness: -10.0579\n",
      "    - New mask: 37 features, Fitness: -8.8913\n",
      "    - New mask: 40 features, Fitness: -13.3179\n",
      "    - New mask: 44 features, Fitness: -11.5791\n",
      "    - New mask: 40 features, Fitness: -10.7294\n",
      "    - New mask: 41 features, Fitness: -9.1060\n",
      "    - New mask: 46 features, Fitness: -11.8199\n",
      "    - New mask: 46 features, Fitness: -12.5711\n",
      "    - New mask: 48 features, Fitness: -17.7612\n",
      "    - New mask: 46 features, Fitness: -11.5328\n",
      "    - New mask: 43 features, Fitness: -9.8036\n",
      "    - New mask: 39 features, Fitness: -10.0384\n",
      "    - New mask: 42 features, Fitness: -12.1641\n",
      "    - New mask: 45 features, Fitness: -14.3523\n",
      "    - New mask: 38 features, Fitness: -8.0687\n",
      "    - New mask: 42 features, Fitness: -10.0774\n",
      "    - New mask: 46 features, Fitness: -14.0977\n",
      "    - New mask: 44 features, Fitness: -12.6222\n",
      "    - New mask: 46 features, Fitness: -13.8673\n",
      "    - New mask: 45 features, Fitness: -11.7610\n",
      "    - New mask: 43 features, Fitness: -10.7258\n",
      "    - New mask: 43 features, Fitness: -11.3332\n",
      "=== End of Round 1: Vote mask selects 70 features (rho: 0.20)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 2 ================\n",
      "  Adaptive rho for this round: 0.23\n",
      "    - New mask: 48 features, Fitness: -14.6377\n",
      "    - New mask: 42 features, Fitness: -12.8042\n",
      "    - New mask: 52 features, Fitness: -19.5285\n",
      "    - New mask: 49 features, Fitness: -15.1205\n",
      "    - New mask: 49 features, Fitness: -13.7285\n",
      "    - New mask: 50 features, Fitness: -14.7599\n",
      "    - New mask: 49 features, Fitness: -14.3968\n",
      "    - New mask: 53 features, Fitness: -17.1687\n",
      "    - New mask: 45 features, Fitness: -12.9461\n",
      "    - New mask: 46 features, Fitness: -11.5473\n",
      "    - New mask: 48 features, Fitness: -13.3970\n",
      "    - New mask: 45 features, Fitness: -10.9358\n",
      "    - New mask: 51 features, Fitness: -16.4119\n",
      "    - New mask: 44 features, Fitness: -11.2837\n",
      "    - New mask: 43 features, Fitness: -10.2112\n",
      "    - New mask: 50 features, Fitness: -14.9554\n",
      "    - New mask: 53 features, Fitness: -18.3593\n",
      "    - New mask: 49 features, Fitness: -14.5003\n",
      "    - New mask: 48 features, Fitness: -13.0475\n",
      "    - New mask: 43 features, Fitness: -9.3199\n",
      "    - New mask: 54 features, Fitness: -18.4677\n",
      "    - New mask: 50 features, Fitness: -18.1029\n",
      "    - New mask: 47 features, Fitness: -13.4269\n",
      "    - New mask: 46 features, Fitness: -14.4046\n",
      "    - New mask: 51 features, Fitness: -13.6802\n",
      "    - New mask: 47 features, Fitness: -14.4425\n",
      "    - New mask: 52 features, Fitness: -17.0306\n",
      "    - New mask: 45 features, Fitness: -12.2541\n",
      "    - New mask: 51 features, Fitness: -14.5490\n",
      "    - New mask: 47 features, Fitness: -12.9621\n",
      "    - New mask: 54 features, Fitness: -17.6315\n",
      "    - New mask: 54 features, Fitness: -18.0760\n",
      "    - New mask: 46 features, Fitness: -11.3492\n",
      "    - New mask: 46 features, Fitness: -14.1377\n",
      "    - New mask: 51 features, Fitness: -15.8304\n",
      "    - New mask: 46 features, Fitness: -11.4424\n",
      "    - New mask: 49 features, Fitness: -15.5725\n",
      "    - New mask: 51 features, Fitness: -16.7024\n",
      "    - New mask: 47 features, Fitness: -12.2685\n",
      "    - New mask: 50 features, Fitness: -15.0473\n",
      "    - New mask: 51 features, Fitness: -15.1160\n",
      "    - New mask: 54 features, Fitness: -16.8701\n",
      "    - New mask: 47 features, Fitness: -13.6210\n",
      "    - New mask: 44 features, Fitness: -10.2137\n",
      "    - New mask: 48 features, Fitness: -15.1626\n",
      "    - New mask: 43 features, Fitness: -11.6274\n",
      "    - New mask: 54 features, Fitness: -18.3642\n",
      "    - New mask: 52 features, Fitness: -13.6870\n",
      "    - New mask: 56 features, Fitness: -17.9878\n",
      "    - New mask: 50 features, Fitness: -13.8205\n",
      "    - New mask: 53 features, Fitness: -16.6267\n",
      "    - New mask: 54 features, Fitness: -17.2565\n",
      "    - New mask: 44 features, Fitness: -11.1140\n",
      "    - New mask: 49 features, Fitness: -12.5179\n",
      "    - New mask: 47 features, Fitness: -11.6928\n",
      "    - New mask: 46 features, Fitness: -12.9722\n",
      "    - New mask: 45 features, Fitness: -10.2594\n",
      "    - New mask: 49 features, Fitness: -13.6744\n",
      "    - New mask: 51 features, Fitness: -13.8072\n",
      "    - New mask: 58 features, Fitness: -18.6782\n",
      "    - New mask: 48 features, Fitness: -10.8389\n",
      "    - New mask: 45 features, Fitness: -10.2739\n",
      "    - New mask: 44 features, Fitness: -10.1291\n",
      "    - New mask: 48 features, Fitness: -11.5556\n",
      "    - New mask: 45 features, Fitness: -9.7368\n",
      "    - New mask: 48 features, Fitness: -10.4128\n",
      "    - New mask: 54 features, Fitness: -15.8611\n",
      "    - New mask: 48 features, Fitness: -11.0170\n",
      "    - New mask: 46 features, Fitness: -11.1580\n",
      "    - New mask: 49 features, Fitness: -14.0975\n",
      "    - New mask: 50 features, Fitness: -13.2943\n",
      "    - New mask: 45 features, Fitness: -9.0132\n",
      "    - New mask: 48 features, Fitness: -12.3981\n",
      "    - New mask: 55 features, Fitness: -18.8152\n",
      "    - New mask: 51 features, Fitness: -14.3338\n",
      "    - New mask: 44 features, Fitness: -9.3124\n",
      "    - New mask: 46 features, Fitness: -12.5931\n",
      "    - New mask: 49 features, Fitness: -14.3062\n",
      "    - New mask: 49 features, Fitness: -12.6280\n",
      "    - New mask: 44 features, Fitness: -9.6791\n",
      "    - New mask: 49 features, Fitness: -9.5800\n",
      "    - New mask: 50 features, Fitness: -11.3492\n",
      "    - New mask: 50 features, Fitness: -13.6049\n",
      "    - New mask: 48 features, Fitness: -11.5158\n",
      "    - New mask: 46 features, Fitness: -11.0764\n",
      "    - New mask: 50 features, Fitness: -10.4313\n",
      "    - New mask: 51 features, Fitness: -14.4098\n",
      "    - New mask: 50 features, Fitness: -10.0428\n",
      "    - New mask: 50 features, Fitness: -9.5089\n",
      "    - New mask: 47 features, Fitness: -12.2817\n",
      "    - New mask: 57 features, Fitness: -18.0184\n",
      "    - New mask: 50 features, Fitness: -11.5775\n",
      "    - New mask: 51 features, Fitness: -12.8274\n",
      "    - New mask: 54 features, Fitness: -14.6944\n",
      "    - New mask: 43 features, Fitness: -7.4963\n",
      "    - New mask: 48 features, Fitness: -11.7682\n",
      "    - New mask: 49 features, Fitness: -12.2620\n",
      "    - New mask: 46 features, Fitness: -8.8707\n",
      "    - New mask: 45 features, Fitness: -9.8265\n",
      "    - New mask: 48 features, Fitness: -10.1975\n",
      "    - New mask: 49 features, Fitness: -15.4504\n",
      "    - New mask: 51 features, Fitness: -15.0813\n",
      "    - New mask: 50 features, Fitness: -13.5464\n",
      "    - New mask: 52 features, Fitness: -15.7485\n",
      "    - New mask: 52 features, Fitness: -16.2326\n",
      "    - New mask: 51 features, Fitness: -17.0847\n",
      "    - New mask: 51 features, Fitness: -17.2503\n",
      "    - New mask: 47 features, Fitness: -13.1905\n",
      "    - New mask: 50 features, Fitness: -13.7404\n",
      "    - New mask: 48 features, Fitness: -11.8258\n",
      "    - New mask: 55 features, Fitness: -17.6636\n",
      "    - New mask: 55 features, Fitness: -17.5796\n",
      "    - New mask: 47 features, Fitness: -13.9214\n",
      "    - New mask: 47 features, Fitness: -12.0288\n",
      "    - New mask: 45 features, Fitness: -11.6170\n",
      "    - New mask: 56 features, Fitness: -17.0109\n",
      "    - New mask: 48 features, Fitness: -12.5468\n",
      "    - New mask: 44 features, Fitness: -10.7249\n",
      "    - New mask: 44 features, Fitness: -11.5740\n",
      "    - New mask: 57 features, Fitness: -22.2572\n",
      "    - New mask: 50 features, Fitness: -14.2848\n",
      "    - New mask: 42 features, Fitness: -9.8744\n",
      "    - New mask: 48 features, Fitness: -11.9071\n",
      "    - New mask: 52 features, Fitness: -13.5151\n",
      "    - New mask: 52 features, Fitness: -14.2609\n",
      "    - New mask: 51 features, Fitness: -13.3900\n",
      "    - New mask: 50 features, Fitness: -13.4225\n",
      "    - New mask: 47 features, Fitness: -10.6277\n",
      "    - New mask: 52 features, Fitness: -13.3682\n",
      "    - New mask: 47 features, Fitness: -10.6123\n",
      "    - New mask: 51 features, Fitness: -15.7206\n",
      "    - New mask: 52 features, Fitness: -12.4188\n",
      "    - New mask: 54 features, Fitness: -16.2273\n",
      "    - New mask: 48 features, Fitness: -10.0917\n",
      "    - New mask: 46 features, Fitness: -11.9601\n",
      "    - New mask: 50 features, Fitness: -13.8650\n",
      "    - New mask: 45 features, Fitness: -9.9208\n",
      "    - New mask: 55 features, Fitness: -16.2586\n",
      "    - New mask: 49 features, Fitness: -10.6422\n",
      "    - New mask: 54 features, Fitness: -16.7038\n",
      "    - New mask: 44 features, Fitness: -12.2119\n",
      "    - New mask: 55 features, Fitness: -22.8154\n",
      "    - New mask: 55 features, Fitness: -20.8495\n",
      "    - New mask: 50 features, Fitness: -17.9461\n",
      "    - New mask: 49 features, Fitness: -16.7998\n",
      "    - New mask: 45 features, Fitness: -12.7762\n",
      "    - New mask: 48 features, Fitness: -16.6942\n",
      "    - New mask: 53 features, Fitness: -19.1878\n",
      "    - New mask: 46 features, Fitness: -12.6259\n",
      "    - New mask: 48 features, Fitness: -16.0547\n",
      "    - New mask: 51 features, Fitness: -18.5870\n",
      "    - New mask: 45 features, Fitness: -13.5031\n",
      "    - New mask: 49 features, Fitness: -16.8515\n",
      "    - New mask: 48 features, Fitness: -18.4557\n",
      "    - New mask: 44 features, Fitness: -13.0454\n",
      "    - New mask: 50 features, Fitness: -16.5151\n",
      "    - New mask: 53 features, Fitness: -20.7695\n",
      "    - New mask: 46 features, Fitness: -12.0558\n",
      "    - New mask: 49 features, Fitness: -17.0275\n",
      "    - New mask: 52 features, Fitness: -18.3955\n",
      "    - New mask: 53 features, Fitness: -20.7744\n",
      "    - New mask: 51 features, Fitness: -18.3948\n",
      "    - New mask: 53 features, Fitness: -20.0774\n",
      "    - New mask: 45 features, Fitness: -14.0011\n",
      "    - New mask: 49 features, Fitness: -17.2900\n",
      "    - New mask: 46 features, Fitness: -16.9577\n",
      "    - New mask: 51 features, Fitness: -19.0813\n",
      "    - New mask: 60 features, Fitness: -30.7754\n",
      "    - New mask: 39 features, Fitness: -9.2178\n",
      "    - New mask: 45 features, Fitness: -15.6558\n",
      "    - New mask: 46 features, Fitness: -14.8011\n",
      "    - New mask: 47 features, Fitness: -13.7890\n",
      "    - New mask: 45 features, Fitness: -14.3189\n",
      "    - New mask: 44 features, Fitness: -12.4405\n",
      "    - New mask: 48 features, Fitness: -18.1552\n",
      "    - New mask: 50 features, Fitness: -16.7616\n",
      "    - New mask: 45 features, Fitness: -15.0903\n",
      "    - New mask: 47 features, Fitness: -17.8607\n",
      "    - New mask: 48 features, Fitness: -16.8037\n",
      "    - New mask: 46 features, Fitness: -14.4172\n",
      "    - New mask: 53 features, Fitness: -19.5675\n",
      "    - New mask: 47 features, Fitness: -13.5729\n",
      "    - New mask: 51 features, Fitness: -17.8699\n",
      "    - New mask: 46 features, Fitness: -13.2677\n",
      "    - New mask: 52 features, Fitness: -17.4027\n",
      "    - New mask: 51 features, Fitness: -15.2019\n",
      "    - New mask: 49 features, Fitness: -17.4846\n",
      "    - New mask: 53 features, Fitness: -17.4099\n",
      "    - New mask: 48 features, Fitness: -15.0961\n",
      "    - New mask: 44 features, Fitness: -12.9712\n",
      "    - New mask: 48 features, Fitness: -15.5725\n",
      "    - New mask: 50 features, Fitness: -15.8513\n",
      "    - New mask: 47 features, Fitness: -14.3788\n",
      "    - New mask: 47 features, Fitness: -13.8562\n",
      "    - New mask: 47 features, Fitness: -12.9071\n",
      "    - New mask: 44 features, Fitness: -13.1671\n",
      "    - New mask: 50 features, Fitness: -17.5227\n",
      "    - New mask: 48 features, Fitness: -13.8155\n",
      "    - New mask: 46 features, Fitness: -12.9163\n",
      "    - New mask: 54 features, Fitness: -21.2193\n",
      "=== End of Round 2: Vote mask selects 69 features (rho: 0.23)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 3 ================\n",
      "  Adaptive rho for this round: 0.26\n",
      "    - New mask: 54 features, Fitness: -17.7179\n",
      "    - New mask: 46 features, Fitness: -14.2239\n",
      "    - New mask: 57 features, Fitness: -22.4024\n",
      "    - New mask: 58 features, Fitness: -22.4325\n",
      "    - New mask: 55 features, Fitness: -16.3882\n",
      "    - New mask: 48 features, Fitness: -12.6749\n",
      "    - New mask: 50 features, Fitness: -16.1104\n",
      "    - New mask: 53 features, Fitness: -16.8726\n",
      "    - New mask: 50 features, Fitness: -15.8379\n",
      "    - New mask: 49 features, Fitness: -13.7342\n",
      "    - New mask: 58 features, Fitness: -20.7071\n",
      "    - New mask: 49 features, Fitness: -12.7402\n",
      "    - New mask: 54 features, Fitness: -17.7647\n",
      "    - New mask: 49 features, Fitness: -14.5018\n",
      "    - New mask: 50 features, Fitness: -14.4600\n",
      "    - New mask: 56 features, Fitness: -18.1760\n",
      "    - New mask: 52 features, Fitness: -16.4587\n",
      "    - New mask: 52 features, Fitness: -17.2913\n",
      "    - New mask: 51 features, Fitness: -15.1226\n",
      "    - New mask: 52 features, Fitness: -15.6612\n",
      "    - New mask: 58 features, Fitness: -21.3160\n",
      "    - New mask: 52 features, Fitness: -16.4786\n",
      "    - New mask: 53 features, Fitness: -16.0871\n",
      "    - New mask: 51 features, Fitness: -16.1567\n",
      "    - New mask: 51 features, Fitness: -14.0025\n",
      "    - New mask: 57 features, Fitness: -18.7853\n",
      "    - New mask: 51 features, Fitness: -15.1333\n",
      "    - New mask: 50 features, Fitness: -12.8404\n",
      "    - New mask: 56 features, Fitness: -19.6298\n",
      "    - New mask: 53 features, Fitness: -15.7403\n",
      "    - New mask: 57 features, Fitness: -20.6028\n",
      "    - New mask: 54 features, Fitness: -16.7470\n",
      "    - New mask: 51 features, Fitness: -15.7899\n",
      "    - New mask: 53 features, Fitness: -18.6085\n",
      "    - New mask: 54 features, Fitness: -17.9413\n",
      "    - New mask: 54 features, Fitness: -16.1515\n",
      "    - New mask: 55 features, Fitness: -19.5421\n",
      "    - New mask: 52 features, Fitness: -18.2255\n",
      "    - New mask: 49 features, Fitness: -13.7092\n",
      "    - New mask: 55 features, Fitness: -18.5481\n",
      "    - New mask: 56 features, Fitness: -18.5764\n",
      "    - New mask: 56 features, Fitness: -19.6355\n",
      "    - New mask: 54 features, Fitness: -15.5410\n",
      "    - New mask: 54 features, Fitness: -16.4152\n",
      "    - New mask: 57 features, Fitness: -20.8540\n",
      "    - New mask: 51 features, Fitness: -17.8351\n",
      "    - New mask: 56 features, Fitness: -18.9280\n",
      "    - New mask: 53 features, Fitness: -15.6135\n",
      "    - New mask: 54 features, Fitness: -18.0416\n",
      "    - New mask: 54 features, Fitness: -16.9721\n",
      "    - New mask: 61 features, Fitness: -22.7773\n",
      "    - New mask: 51 features, Fitness: -14.4816\n",
      "    - New mask: 57 features, Fitness: -17.3138\n",
      "    - New mask: 53 features, Fitness: -15.2750\n",
      "    - New mask: 49 features, Fitness: -12.5558\n",
      "    - New mask: 56 features, Fitness: -19.5293\n",
      "    - New mask: 52 features, Fitness: -13.7890\n",
      "    - New mask: 50 features, Fitness: -13.9434\n",
      "    - New mask: 51 features, Fitness: -13.6118\n",
      "    - New mask: 53 features, Fitness: -14.7100\n",
      "    - New mask: 58 features, Fitness: -17.0043\n",
      "    - New mask: 53 features, Fitness: -14.9306\n",
      "    - New mask: 55 features, Fitness: -17.6785\n",
      "    - New mask: 58 features, Fitness: -17.5914\n",
      "    - New mask: 53 features, Fitness: -14.4742\n",
      "    - New mask: 54 features, Fitness: -15.8165\n",
      "    - New mask: 52 features, Fitness: -14.0877\n",
      "    - New mask: 55 features, Fitness: -15.7839\n",
      "    - New mask: 49 features, Fitness: -12.3097\n",
      "    - New mask: 48 features, Fitness: -12.4022\n",
      "    - New mask: 53 features, Fitness: -15.3283\n",
      "    - New mask: 51 features, Fitness: -14.2580\n",
      "    - New mask: 59 features, Fitness: -18.5582\n",
      "    - New mask: 54 features, Fitness: -16.3176\n",
      "    - New mask: 54 features, Fitness: -15.4785\n",
      "    - New mask: 54 features, Fitness: -15.1471\n",
      "    - New mask: 51 features, Fitness: -13.6435\n",
      "    - New mask: 54 features, Fitness: -15.3085\n",
      "    - New mask: 56 features, Fitness: -17.2216\n",
      "    - New mask: 53 features, Fitness: -13.6112\n",
      "    - New mask: 53 features, Fitness: -13.4404\n",
      "    - New mask: 47 features, Fitness: -9.5812\n",
      "    - New mask: 48 features, Fitness: -11.2984\n",
      "    - New mask: 55 features, Fitness: -16.3207\n",
      "    - New mask: 49 features, Fitness: -12.2271\n",
      "    - New mask: 53 features, Fitness: -13.2165\n",
      "    - New mask: 52 features, Fitness: -13.0647\n",
      "    - New mask: 49 features, Fitness: -11.2263\n",
      "    - New mask: 57 features, Fitness: -14.4351\n",
      "    - New mask: 55 features, Fitness: -15.6202\n",
      "    - New mask: 52 features, Fitness: -14.0766\n",
      "    - New mask: 54 features, Fitness: -14.8724\n",
      "    - New mask: 47 features, Fitness: -9.8934\n",
      "    - New mask: 58 features, Fitness: -16.2146\n",
      "    - New mask: 48 features, Fitness: -12.3170\n",
      "    - New mask: 49 features, Fitness: -12.7585\n",
      "    - New mask: 50 features, Fitness: -11.3126\n",
      "    - New mask: 55 features, Fitness: -13.6232\n",
      "    - New mask: 48 features, Fitness: -9.5258\n",
      "    - New mask: 53 features, Fitness: -14.7113\n",
      "    - New mask: 56 features, Fitness: -20.9387\n",
      "    - New mask: 51 features, Fitness: -14.3165\n",
      "    - New mask: 50 features, Fitness: -13.6406\n",
      "    - New mask: 57 features, Fitness: -21.6448\n",
      "    - New mask: 50 features, Fitness: -15.0782\n",
      "    - New mask: 53 features, Fitness: -16.7148\n",
      "    - New mask: 52 features, Fitness: -16.1426\n",
      "    - New mask: 55 features, Fitness: -19.5480\n",
      "    - New mask: 56 features, Fitness: -20.6925\n",
      "    - New mask: 56 features, Fitness: -18.4201\n",
      "    - New mask: 56 features, Fitness: -18.7319\n",
      "    - New mask: 56 features, Fitness: -18.2915\n",
      "    - New mask: 53 features, Fitness: -19.5485\n",
      "    - New mask: 51 features, Fitness: -13.9096\n",
      "    - New mask: 51 features, Fitness: -15.4247\n",
      "    - New mask: 53 features, Fitness: -16.1457\n",
      "    - New mask: 53 features, Fitness: -16.3538\n",
      "    - New mask: 52 features, Fitness: -17.2123\n",
      "    - New mask: 51 features, Fitness: -15.8916\n",
      "    - New mask: 55 features, Fitness: -20.3810\n",
      "    - New mask: 55 features, Fitness: -18.6697\n",
      "    - New mask: 52 features, Fitness: -15.7356\n",
      "    - New mask: 51 features, Fitness: -15.9569\n",
      "    - New mask: 55 features, Fitness: -17.4433\n",
      "    - New mask: 52 features, Fitness: -14.8247\n",
      "    - New mask: 54 features, Fitness: -15.5704\n",
      "    - New mask: 51 features, Fitness: -15.5495\n",
      "    - New mask: 43 features, Fitness: -10.0919\n",
      "    - New mask: 46 features, Fitness: -11.2320\n",
      "    - New mask: 51 features, Fitness: -14.4547\n",
      "    - New mask: 56 features, Fitness: -18.0893\n",
      "    - New mask: 49 features, Fitness: -12.3844\n",
      "    - New mask: 50 features, Fitness: -11.3497\n",
      "    - New mask: 54 features, Fitness: -18.0493\n",
      "    - New mask: 53 features, Fitness: -15.9218\n",
      "    - New mask: 53 features, Fitness: -15.3951\n",
      "    - New mask: 51 features, Fitness: -13.3814\n",
      "    - New mask: 53 features, Fitness: -14.8250\n",
      "    - New mask: 43 features, Fitness: -8.8248\n",
      "    - New mask: 57 features, Fitness: -18.1824\n",
      "    - New mask: 54 features, Fitness: -18.7759\n",
      "    - New mask: 56 features, Fitness: -22.9885\n",
      "    - New mask: 57 features, Fitness: -21.8603\n",
      "    - New mask: 54 features, Fitness: -20.6412\n",
      "    - New mask: 56 features, Fitness: -20.0916\n",
      "    - New mask: 52 features, Fitness: -16.5765\n",
      "    - New mask: 51 features, Fitness: -16.2576\n",
      "    - New mask: 50 features, Fitness: -15.2587\n",
      "    - New mask: 53 features, Fitness: -17.8928\n",
      "    - New mask: 48 features, Fitness: -16.2335\n",
      "    - New mask: 53 features, Fitness: -18.8337\n",
      "    - New mask: 53 features, Fitness: -18.5071\n",
      "    - New mask: 50 features, Fitness: -16.5185\n",
      "    - New mask: 48 features, Fitness: -13.9935\n",
      "    - New mask: 50 features, Fitness: -16.4075\n",
      "    - New mask: 50 features, Fitness: -15.9863\n",
      "    - New mask: 52 features, Fitness: -18.6786\n",
      "    - New mask: 48 features, Fitness: -13.3201\n",
      "    - New mask: 52 features, Fitness: -19.9331\n",
      "    - New mask: 50 features, Fitness: -15.7492\n",
      "    - New mask: 49 features, Fitness: -17.8261\n",
      "    - New mask: 55 features, Fitness: -23.0356\n",
      "    - New mask: 54 features, Fitness: -21.2019\n",
      "    - New mask: 50 features, Fitness: -17.7010\n",
      "    - New mask: 56 features, Fitness: -23.0983\n",
      "    - New mask: 45 features, Fitness: -13.6759\n",
      "    - New mask: 57 features, Fitness: -25.9899\n",
      "    - New mask: 59 features, Fitness: -29.1930\n",
      "    - New mask: 44 features, Fitness: -12.9740\n",
      "    - New mask: 49 features, Fitness: -19.2914\n",
      "    - New mask: 49 features, Fitness: -19.4053\n",
      "    - New mask: 53 features, Fitness: -19.3567\n",
      "    - New mask: 51 features, Fitness: -17.9050\n",
      "    - New mask: 48 features, Fitness: -15.9771\n",
      "    - New mask: 49 features, Fitness: -17.2150\n",
      "    - New mask: 50 features, Fitness: -17.8057\n",
      "    - New mask: 53 features, Fitness: -23.3141\n",
      "    - New mask: 56 features, Fitness: -23.2844\n",
      "    - New mask: 49 features, Fitness: -18.1198\n",
      "    - New mask: 50 features, Fitness: -16.2858\n",
      "    - New mask: 56 features, Fitness: -22.0881\n",
      "    - New mask: 52 features, Fitness: -18.0621\n",
      "    - New mask: 57 features, Fitness: -21.0688\n",
      "    - New mask: 55 features, Fitness: -19.0662\n",
      "    - New mask: 53 features, Fitness: -19.7420\n",
      "    - New mask: 53 features, Fitness: -16.6425\n",
      "    - New mask: 51 features, Fitness: -17.7772\n",
      "    - New mask: 50 features, Fitness: -13.7808\n",
      "    - New mask: 51 features, Fitness: -17.1789\n",
      "    - New mask: 56 features, Fitness: -21.9733\n",
      "    - New mask: 52 features, Fitness: -17.7365\n",
      "    - New mask: 52 features, Fitness: -17.8178\n",
      "    - New mask: 55 features, Fitness: -20.7277\n",
      "    - New mask: 46 features, Fitness: -14.5135\n",
      "    - New mask: 52 features, Fitness: -18.1658\n",
      "    - New mask: 53 features, Fitness: -18.6723\n",
      "    - New mask: 47 features, Fitness: -14.1723\n",
      "    - New mask: 55 features, Fitness: -18.6545\n",
      "    - New mask: 51 features, Fitness: -16.6761\n",
      "    - New mask: 53 features, Fitness: -22.4780\n",
      "=== End of Round 3: Vote mask selects 67 features (rho: 0.26)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 4 ================\n",
      "  Adaptive rho for this round: 0.29\n",
      "    - New mask: 56 features, Fitness: -18.7952\n",
      "    - New mask: 51 features, Fitness: -14.5780\n",
      "    - New mask: 54 features, Fitness: -15.7906\n",
      "    - New mask: 59 features, Fitness: -22.1777\n",
      "    - New mask: 62 features, Fitness: -23.3738\n",
      "    - New mask: 54 features, Fitness: -16.6743\n",
      "    - New mask: 53 features, Fitness: -18.9125\n",
      "    - New mask: 54 features, Fitness: -16.8480\n",
      "    - New mask: 56 features, Fitness: -19.1471\n",
      "    - New mask: 54 features, Fitness: -17.7195\n",
      "    - New mask: 57 features, Fitness: -20.9721\n",
      "    - New mask: 54 features, Fitness: -17.3109\n",
      "    - New mask: 59 features, Fitness: -23.4562\n",
      "    - New mask: 53 features, Fitness: -16.6667\n",
      "    - New mask: 52 features, Fitness: -17.5123\n",
      "    - New mask: 60 features, Fitness: -23.0151\n",
      "    - New mask: 53 features, Fitness: -17.1942\n",
      "    - New mask: 56 features, Fitness: -19.5042\n",
      "    - New mask: 54 features, Fitness: -17.0471\n",
      "    - New mask: 55 features, Fitness: -18.8037\n",
      "    - New mask: 60 features, Fitness: -21.6489\n",
      "    - New mask: 55 features, Fitness: -19.1162\n",
      "    - New mask: 61 features, Fitness: -22.0026\n",
      "    - New mask: 50 features, Fitness: -14.0923\n",
      "    - New mask: 53 features, Fitness: -17.9374\n",
      "    - New mask: 60 features, Fitness: -21.9863\n",
      "    - New mask: 55 features, Fitness: -17.9340\n",
      "    - New mask: 55 features, Fitness: -16.7446\n",
      "    - New mask: 59 features, Fitness: -20.8226\n",
      "    - New mask: 58 features, Fitness: -18.8403\n",
      "    - New mask: 60 features, Fitness: -22.0376\n",
      "    - New mask: 56 features, Fitness: -19.3488\n",
      "    - New mask: 57 features, Fitness: -20.7246\n",
      "    - New mask: 59 features, Fitness: -20.4359\n",
      "    - New mask: 56 features, Fitness: -18.6477\n",
      "    - New mask: 52 features, Fitness: -15.1228\n",
      "    - New mask: 57 features, Fitness: -18.4035\n",
      "    - New mask: 55 features, Fitness: -19.1412\n",
      "    - New mask: 57 features, Fitness: -20.0309\n",
      "    - New mask: 56 features, Fitness: -19.4341\n",
      "    - New mask: 50 features, Fitness: -13.7124\n",
      "    - New mask: 58 features, Fitness: -21.5339\n",
      "    - New mask: 52 features, Fitness: -14.6453\n",
      "    - New mask: 59 features, Fitness: -18.1688\n",
      "    - New mask: 55 features, Fitness: -17.9682\n",
      "    - New mask: 50 features, Fitness: -13.6980\n",
      "    - New mask: 52 features, Fitness: -14.3584\n",
      "    - New mask: 54 features, Fitness: -17.7526\n",
      "    - New mask: 52 features, Fitness: -15.0940\n",
      "    - New mask: 54 features, Fitness: -16.0454\n",
      "    - New mask: 54 features, Fitness: -19.1190\n",
      "    - New mask: 58 features, Fitness: -18.9285\n",
      "    - New mask: 55 features, Fitness: -18.2131\n",
      "    - New mask: 58 features, Fitness: -19.2874\n",
      "    - New mask: 51 features, Fitness: -13.5701\n",
      "    - New mask: 54 features, Fitness: -16.6651\n",
      "    - New mask: 60 features, Fitness: -21.0552\n",
      "    - New mask: 54 features, Fitness: -15.8865\n",
      "    - New mask: 54 features, Fitness: -14.2399\n",
      "    - New mask: 55 features, Fitness: -16.9205\n",
      "    - New mask: 52 features, Fitness: -15.7329\n",
      "    - New mask: 54 features, Fitness: -17.1933\n",
      "    - New mask: 54 features, Fitness: -15.3076\n",
      "    - New mask: 60 features, Fitness: -18.5869\n",
      "    - New mask: 55 features, Fitness: -17.6691\n",
      "    - New mask: 54 features, Fitness: -18.0192\n",
      "    - New mask: 53 features, Fitness: -15.8687\n",
      "    - New mask: 48 features, Fitness: -10.9347\n",
      "    - New mask: 53 features, Fitness: -14.4354\n",
      "    - New mask: 57 features, Fitness: -19.2025\n",
      "    - New mask: 58 features, Fitness: -18.2128\n",
      "    - New mask: 59 features, Fitness: -19.8222\n",
      "    - New mask: 52 features, Fitness: -14.8319\n",
      "    - New mask: 53 features, Fitness: -16.0201\n",
      "    - New mask: 60 features, Fitness: -20.2285\n",
      "    - New mask: 48 features, Fitness: -13.5087\n",
      "    - New mask: 58 features, Fitness: -18.8880\n",
      "    - New mask: 53 features, Fitness: -15.4234\n",
      "    - New mask: 59 features, Fitness: -18.6299\n",
      "    - New mask: 54 features, Fitness: -14.8698\n",
      "    - New mask: 57 features, Fitness: -16.8985\n",
      "    - New mask: 54 features, Fitness: -14.1526\n",
      "    - New mask: 54 features, Fitness: -14.2944\n",
      "    - New mask: 59 features, Fitness: -18.2800\n",
      "    - New mask: 50 features, Fitness: -11.2259\n",
      "    - New mask: 51 features, Fitness: -10.9184\n",
      "    - New mask: 58 features, Fitness: -16.2239\n",
      "    - New mask: 56 features, Fitness: -16.5410\n",
      "    - New mask: 56 features, Fitness: -14.6988\n",
      "    - New mask: 55 features, Fitness: -16.1356\n",
      "    - New mask: 53 features, Fitness: -14.5865\n",
      "    - New mask: 57 features, Fitness: -16.3684\n",
      "    - New mask: 52 features, Fitness: -12.4415\n",
      "    - New mask: 54 features, Fitness: -12.2362\n",
      "    - New mask: 54 features, Fitness: -15.1132\n",
      "    - New mask: 51 features, Fitness: -14.2378\n",
      "    - New mask: 51 features, Fitness: -10.3347\n",
      "    - New mask: 53 features, Fitness: -12.2264\n",
      "    - New mask: 53 features, Fitness: -12.0434\n",
      "    - New mask: 55 features, Fitness: -15.1659\n",
      "    - New mask: 55 features, Fitness: -20.2315\n",
      "    - New mask: 54 features, Fitness: -16.4778\n",
      "    - New mask: 55 features, Fitness: -17.2176\n",
      "    - New mask: 59 features, Fitness: -20.8881\n",
      "    - New mask: 57 features, Fitness: -19.2304\n",
      "    - New mask: 52 features, Fitness: -15.1569\n",
      "    - New mask: 56 features, Fitness: -18.9353\n",
      "    - New mask: 53 features, Fitness: -17.5974\n",
      "    - New mask: 56 features, Fitness: -20.6285\n",
      "    - New mask: 58 features, Fitness: -19.6171\n",
      "    - New mask: 56 features, Fitness: -18.9885\n",
      "    - New mask: 61 features, Fitness: -23.2153\n",
      "    - New mask: 56 features, Fitness: -19.7918\n",
      "    - New mask: 59 features, Fitness: -20.0867\n",
      "    - New mask: 58 features, Fitness: -20.9917\n",
      "    - New mask: 51 features, Fitness: -15.9767\n",
      "    - New mask: 55 features, Fitness: -18.1476\n",
      "    - New mask: 56 features, Fitness: -19.3202\n",
      "    - New mask: 53 features, Fitness: -16.0652\n",
      "    - New mask: 58 features, Fitness: -22.4273\n",
      "    - New mask: 55 features, Fitness: -15.9085\n",
      "    - New mask: 51 features, Fitness: -14.6769\n",
      "    - New mask: 52 features, Fitness: -15.7374\n",
      "    - New mask: 51 features, Fitness: -14.9599\n",
      "    - New mask: 54 features, Fitness: -14.5592\n",
      "    - New mask: 55 features, Fitness: -16.1977\n",
      "    - New mask: 54 features, Fitness: -16.1370\n",
      "    - New mask: 48 features, Fitness: -13.0415\n",
      "    - New mask: 48 features, Fitness: -11.0708\n",
      "    - New mask: 52 features, Fitness: -15.9996\n",
      "    - New mask: 55 features, Fitness: -17.7943\n",
      "    - New mask: 51 features, Fitness: -13.0595\n",
      "    - New mask: 52 features, Fitness: -13.1701\n",
      "    - New mask: 55 features, Fitness: -17.9045\n",
      "    - New mask: 52 features, Fitness: -14.9286\n",
      "    - New mask: 58 features, Fitness: -17.7092\n",
      "    - New mask: 58 features, Fitness: -17.7419\n",
      "    - New mask: 50 features, Fitness: -14.0242\n",
      "    - New mask: 49 features, Fitness: -11.9203\n",
      "    - New mask: 58 features, Fitness: -18.5393\n",
      "    - New mask: 59 features, Fitness: -23.0531\n",
      "    - New mask: 58 features, Fitness: -23.4224\n",
      "    - New mask: 61 features, Fitness: -25.9046\n",
      "    - New mask: 57 features, Fitness: -23.0703\n",
      "    - New mask: 55 features, Fitness: -18.7147\n",
      "    - New mask: 52 features, Fitness: -16.7364\n",
      "    - New mask: 58 features, Fitness: -24.2132\n",
      "    - New mask: 51 features, Fitness: -17.1067\n",
      "    - New mask: 53 features, Fitness: -19.0273\n",
      "    - New mask: 54 features, Fitness: -20.1248\n",
      "    - New mask: 47 features, Fitness: -13.5074\n",
      "    - New mask: 55 features, Fitness: -19.9573\n",
      "    - New mask: 57 features, Fitness: -22.0280\n",
      "    - New mask: 52 features, Fitness: -17.7495\n",
      "    - New mask: 53 features, Fitness: -19.4189\n",
      "    - New mask: 53 features, Fitness: -18.4017\n",
      "    - New mask: 54 features, Fitness: -19.6889\n",
      "    - New mask: 55 features, Fitness: -18.9405\n",
      "    - New mask: 56 features, Fitness: -21.6028\n",
      "    - New mask: 54 features, Fitness: -19.0820\n",
      "    - New mask: 49 features, Fitness: -18.0012\n",
      "    - New mask: 55 features, Fitness: -21.8623\n",
      "    - New mask: 53 features, Fitness: -20.1853\n",
      "    - New mask: 49 features, Fitness: -17.0318\n",
      "    - New mask: 57 features, Fitness: -26.1711\n",
      "    - New mask: 52 features, Fitness: -20.6276\n",
      "    - New mask: 54 features, Fitness: -23.1470\n",
      "    - New mask: 53 features, Fitness: -20.6229\n",
      "    - New mask: 47 features, Fitness: -15.4955\n",
      "    - New mask: 54 features, Fitness: -22.3957\n",
      "    - New mask: 53 features, Fitness: -19.3668\n",
      "    - New mask: 52 features, Fitness: -21.1416\n",
      "    - New mask: 53 features, Fitness: -20.9622\n",
      "    - New mask: 52 features, Fitness: -18.5144\n",
      "    - New mask: 52 features, Fitness: -19.5430\n",
      "    - New mask: 58 features, Fitness: -27.3165\n",
      "    - New mask: 54 features, Fitness: -22.6632\n",
      "    - New mask: 56 features, Fitness: -26.5146\n",
      "    - New mask: 52 features, Fitness: -17.8881\n",
      "    - New mask: 52 features, Fitness: -17.7281\n",
      "    - New mask: 55 features, Fitness: -18.4176\n",
      "    - New mask: 50 features, Fitness: -16.7622\n",
      "    - New mask: 57 features, Fitness: -20.0561\n",
      "    - New mask: 54 features, Fitness: -18.5127\n",
      "    - New mask: 59 features, Fitness: -22.4715\n",
      "    - New mask: 57 features, Fitness: -19.6460\n",
      "    - New mask: 57 features, Fitness: -21.0559\n",
      "    - New mask: 54 features, Fitness: -17.2995\n",
      "    - New mask: 58 features, Fitness: -22.7493\n",
      "    - New mask: 57 features, Fitness: -20.3976\n",
      "    - New mask: 57 features, Fitness: -20.7868\n",
      "    - New mask: 55 features, Fitness: -18.3036\n",
      "    - New mask: 57 features, Fitness: -21.3035\n",
      "    - New mask: 52 features, Fitness: -18.8126\n",
      "    - New mask: 52 features, Fitness: -16.6007\n",
      "    - New mask: 55 features, Fitness: -19.2317\n",
      "    - New mask: 48 features, Fitness: -12.8855\n",
      "    - New mask: 54 features, Fitness: -18.4472\n",
      "    - New mask: 55 features, Fitness: -18.6692\n",
      "    - New mask: 52 features, Fitness: -18.9628\n",
      "=== End of Round 4: Vote mask selects 69 features (rho: 0.29)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 5 ================\n",
      "  Adaptive rho for this round: 0.33\n",
      "    - New mask: 59 features, Fitness: -22.1903\n",
      "    - New mask: 61 features, Fitness: -24.0480\n",
      "    - New mask: 55 features, Fitness: -18.0086\n",
      "    - New mask: 59 features, Fitness: -21.4953\n",
      "    - New mask: 62 features, Fitness: -24.3552\n",
      "    - New mask: 62 features, Fitness: -23.3792\n",
      "    - New mask: 57 features, Fitness: -19.8282\n",
      "    - New mask: 55 features, Fitness: -17.3858\n",
      "    - New mask: 55 features, Fitness: -17.8865\n",
      "    - New mask: 56 features, Fitness: -17.6499\n",
      "    - New mask: 57 features, Fitness: -21.6014\n",
      "    - New mask: 55 features, Fitness: -17.2060\n",
      "    - New mask: 57 features, Fitness: -20.7440\n",
      "    - New mask: 56 features, Fitness: -20.9843\n",
      "    - New mask: 54 features, Fitness: -18.3333\n",
      "    - New mask: 57 features, Fitness: -19.7148\n",
      "    - New mask: 58 features, Fitness: -21.3787\n",
      "    - New mask: 59 features, Fitness: -21.8120\n",
      "    - New mask: 60 features, Fitness: -22.2242\n",
      "    - New mask: 56 features, Fitness: -19.9162\n",
      "    - New mask: 60 features, Fitness: -20.8067\n",
      "    - New mask: 58 features, Fitness: -20.9808\n",
      "    - New mask: 61 features, Fitness: -22.1325\n",
      "    - New mask: 51 features, Fitness: -14.5915\n",
      "    - New mask: 57 features, Fitness: -19.7837\n",
      "    - New mask: 58 features, Fitness: -20.2458\n",
      "    - New mask: 53 features, Fitness: -15.3590\n",
      "    - New mask: 54 features, Fitness: -17.3196\n",
      "    - New mask: 55 features, Fitness: -18.2847\n",
      "    - New mask: 55 features, Fitness: -17.6600\n",
      "    - New mask: 59 features, Fitness: -21.6320\n",
      "    - New mask: 54 features, Fitness: -19.4477\n",
      "    - New mask: 59 features, Fitness: -21.8886\n",
      "    - New mask: 60 features, Fitness: -22.3269\n",
      "    - New mask: 62 features, Fitness: -23.7815\n",
      "    - New mask: 56 features, Fitness: -19.7765\n",
      "    - New mask: 56 features, Fitness: -17.0017\n",
      "    - New mask: 59 features, Fitness: -21.5244\n",
      "    - New mask: 59 features, Fitness: -21.3228\n",
      "    - New mask: 58 features, Fitness: -21.7464\n",
      "    - New mask: 58 features, Fitness: -22.1218\n",
      "    - New mask: 59 features, Fitness: -22.4048\n",
      "    - New mask: 58 features, Fitness: -17.5460\n",
      "    - New mask: 56 features, Fitness: -16.5349\n",
      "    - New mask: 54 features, Fitness: -17.0517\n",
      "    - New mask: 51 features, Fitness: -13.7404\n",
      "    - New mask: 57 features, Fitness: -18.5339\n",
      "    - New mask: 52 features, Fitness: -14.7739\n",
      "    - New mask: 55 features, Fitness: -16.7926\n",
      "    - New mask: 58 features, Fitness: -19.2708\n",
      "    - New mask: 57 features, Fitness: -19.4427\n",
      "    - New mask: 62 features, Fitness: -22.7938\n",
      "    - New mask: 58 features, Fitness: -19.7694\n",
      "    - New mask: 54 features, Fitness: -16.4191\n",
      "    - New mask: 55 features, Fitness: -17.0705\n",
      "    - New mask: 61 features, Fitness: -20.9465\n",
      "    - New mask: 54 features, Fitness: -15.8500\n",
      "    - New mask: 55 features, Fitness: -15.4707\n",
      "    - New mask: 56 features, Fitness: -16.3740\n",
      "    - New mask: 57 features, Fitness: -19.1832\n",
      "    - New mask: 58 features, Fitness: -19.3325\n",
      "    - New mask: 54 features, Fitness: -16.9924\n",
      "    - New mask: 59 features, Fitness: -18.2936\n",
      "    - New mask: 59 features, Fitness: -20.0641\n",
      "    - New mask: 60 features, Fitness: -22.4817\n",
      "    - New mask: 51 features, Fitness: -14.1070\n",
      "    - New mask: 59 features, Fitness: -19.2887\n",
      "    - New mask: 54 features, Fitness: -15.6969\n",
      "    - New mask: 53 features, Fitness: -15.0342\n",
      "    - New mask: 57 features, Fitness: -21.5077\n",
      "    - New mask: 58 features, Fitness: -16.9686\n",
      "    - New mask: 55 features, Fitness: -19.0683\n",
      "    - New mask: 53 features, Fitness: -15.8376\n",
      "    - New mask: 57 features, Fitness: -18.0837\n",
      "    - New mask: 61 features, Fitness: -21.2371\n",
      "    - New mask: 53 features, Fitness: -15.9492\n",
      "    - New mask: 59 features, Fitness: -19.2984\n",
      "    - New mask: 55 features, Fitness: -17.1728\n",
      "    - New mask: 57 features, Fitness: -16.8098\n",
      "    - New mask: 59 features, Fitness: -19.0648\n",
      "    - New mask: 53 features, Fitness: -13.3160\n",
      "    - New mask: 59 features, Fitness: -17.8058\n",
      "    - New mask: 60 features, Fitness: -19.1064\n",
      "    - New mask: 63 features, Fitness: -20.8070\n",
      "    - New mask: 50 features, Fitness: -10.6673\n",
      "    - New mask: 54 features, Fitness: -13.5761\n",
      "    - New mask: 59 features, Fitness: -18.2655\n",
      "    - New mask: 62 features, Fitness: -20.5451\n",
      "    - New mask: 56 features, Fitness: -14.3534\n",
      "    - New mask: 52 features, Fitness: -12.2770\n",
      "    - New mask: 52 features, Fitness: -13.3136\n",
      "    - New mask: 61 features, Fitness: -18.7347\n",
      "    - New mask: 55 features, Fitness: -13.2017\n",
      "    - New mask: 61 features, Fitness: -18.7748\n",
      "    - New mask: 55 features, Fitness: -15.3365\n",
      "    - New mask: 53 features, Fitness: -13.0176\n",
      "    - New mask: 53 features, Fitness: -11.8325\n",
      "    - New mask: 59 features, Fitness: -18.3553\n",
      "    - New mask: 55 features, Fitness: -13.9005\n",
      "    - New mask: 51 features, Fitness: -11.5408\n",
      "    - New mask: 60 features, Fitness: -21.9043\n",
      "    - New mask: 58 features, Fitness: -20.2444\n",
      "    - New mask: 58 features, Fitness: -19.7926\n",
      "    - New mask: 54 features, Fitness: -17.1933\n",
      "    - New mask: 59 features, Fitness: -22.4014\n",
      "    - New mask: 55 features, Fitness: -19.8541\n",
      "    - New mask: 55 features, Fitness: -18.5551\n",
      "    - New mask: 58 features, Fitness: -21.8049\n",
      "    - New mask: 56 features, Fitness: -20.5518\n",
      "    - New mask: 56 features, Fitness: -18.1449\n",
      "    - New mask: 58 features, Fitness: -21.4707\n",
      "    - New mask: 57 features, Fitness: -20.8876\n",
      "    - New mask: 57 features, Fitness: -19.9071\n",
      "    - New mask: 59 features, Fitness: -20.7847\n",
      "    - New mask: 57 features, Fitness: -20.5279\n",
      "    - New mask: 56 features, Fitness: -19.9079\n",
      "    - New mask: 56 features, Fitness: -18.2385\n",
      "    - New mask: 56 features, Fitness: -18.4861\n",
      "    - New mask: 56 features, Fitness: -19.3555\n",
      "    - New mask: 55 features, Fitness: -19.3093\n",
      "    - New mask: 59 features, Fitness: -19.1072\n",
      "    - New mask: 53 features, Fitness: -15.9596\n",
      "    - New mask: 53 features, Fitness: -14.0449\n",
      "    - New mask: 54 features, Fitness: -15.7666\n",
      "    - New mask: 62 features, Fitness: -21.4617\n",
      "    - New mask: 55 features, Fitness: -15.7823\n",
      "    - New mask: 54 features, Fitness: -16.4378\n",
      "    - New mask: 54 features, Fitness: -15.1448\n",
      "    - New mask: 50 features, Fitness: -11.3754\n",
      "    - New mask: 55 features, Fitness: -16.1595\n",
      "    - New mask: 60 features, Fitness: -23.0306\n",
      "    - New mask: 55 features, Fitness: -15.8510\n",
      "    - New mask: 51 features, Fitness: -14.0546\n",
      "    - New mask: 57 features, Fitness: -19.2450\n",
      "    - New mask: 59 features, Fitness: -21.8107\n",
      "    - New mask: 59 features, Fitness: -19.8717\n",
      "    - New mask: 60 features, Fitness: -19.8680\n",
      "    - New mask: 54 features, Fitness: -17.2262\n",
      "    - New mask: 53 features, Fitness: -13.3306\n",
      "    - New mask: 60 features, Fitness: -20.9212\n",
      "    - New mask: 60 features, Fitness: -25.3185\n",
      "    - New mask: 61 features, Fitness: -25.4760\n",
      "    - New mask: 57 features, Fitness: -22.3882\n",
      "    - New mask: 52 features, Fitness: -19.3771\n",
      "    - New mask: 56 features, Fitness: -20.7194\n",
      "    - New mask: 55 features, Fitness: -19.0992\n",
      "    - New mask: 54 features, Fitness: -20.0475\n",
      "    - New mask: 56 features, Fitness: -22.2967\n",
      "    - New mask: 53 features, Fitness: -21.3396\n",
      "    - New mask: 49 features, Fitness: -16.3170\n",
      "    - New mask: 53 features, Fitness: -19.1151\n",
      "    - New mask: 53 features, Fitness: -18.6447\n",
      "    - New mask: 54 features, Fitness: -20.1809\n",
      "    - New mask: 54 features, Fitness: -18.8169\n",
      "    - New mask: 55 features, Fitness: -23.1731\n",
      "    - New mask: 50 features, Fitness: -16.6290\n",
      "    - New mask: 53 features, Fitness: -20.6600\n",
      "    - New mask: 54 features, Fitness: -22.1020\n",
      "    - New mask: 62 features, Fitness: -28.1069\n",
      "    - New mask: 60 features, Fitness: -25.1313\n",
      "    - New mask: 54 features, Fitness: -22.5538\n",
      "    - New mask: 53 features, Fitness: -18.9672\n",
      "    - New mask: 55 features, Fitness: -23.8011\n",
      "    - New mask: 50 features, Fitness: -17.5192\n",
      "    - New mask: 59 features, Fitness: -24.8143\n",
      "    - New mask: 55 features, Fitness: -22.4514\n",
      "    - New mask: 59 features, Fitness: -27.8816\n",
      "    - New mask: 56 features, Fitness: -25.1388\n",
      "    - New mask: 52 features, Fitness: -19.7557\n",
      "    - New mask: 58 features, Fitness: -26.4141\n",
      "    - New mask: 56 features, Fitness: -23.9291\n",
      "    - New mask: 53 features, Fitness: -22.7616\n",
      "    - New mask: 53 features, Fitness: -20.4456\n",
      "    - New mask: 53 features, Fitness: -20.7661\n",
      "    - New mask: 52 features, Fitness: -21.7809\n",
      "    - New mask: 57 features, Fitness: -25.2934\n",
      "    - New mask: 52 features, Fitness: -19.9195\n",
      "    - New mask: 53 features, Fitness: -20.9406\n",
      "    - New mask: 54 features, Fitness: -21.2936\n",
      "    - New mask: 53 features, Fitness: -20.5296\n",
      "    - New mask: 58 features, Fitness: -20.9535\n",
      "    - New mask: 56 features, Fitness: -19.5899\n",
      "    - New mask: 56 features, Fitness: -19.3202\n",
      "    - New mask: 54 features, Fitness: -17.4317\n",
      "    - New mask: 52 features, Fitness: -17.4714\n",
      "    - New mask: 58 features, Fitness: -22.3194\n",
      "    - New mask: 57 features, Fitness: -21.7236\n",
      "    - New mask: 58 features, Fitness: -20.1968\n",
      "    - New mask: 56 features, Fitness: -20.0541\n",
      "    - New mask: 60 features, Fitness: -24.0241\n",
      "    - New mask: 58 features, Fitness: -21.6984\n",
      "    - New mask: 54 features, Fitness: -17.8155\n",
      "    - New mask: 55 features, Fitness: -19.0398\n",
      "    - New mask: 56 features, Fitness: -23.2890\n",
      "    - New mask: 57 features, Fitness: -19.1745\n",
      "    - New mask: 60 features, Fitness: -23.0073\n",
      "    - New mask: 53 features, Fitness: -17.2200\n",
      "    - New mask: 58 features, Fitness: -22.0821\n",
      "    - New mask: 60 features, Fitness: -24.5951\n",
      "    - New mask: 52 features, Fitness: -17.5426\n",
      "=== End of Round 5: Vote mask selects 66 features (rho: 0.33)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 6 ================\n",
      "  Adaptive rho for this round: 0.36\n",
      "    - New mask: 60 features, Fitness: -22.0378\n",
      "    - New mask: 60 features, Fitness: -22.7315\n",
      "    - New mask: 55 features, Fitness: -17.3705\n",
      "    - New mask: 56 features, Fitness: -17.9407\n",
      "    - New mask: 61 features, Fitness: -22.4228\n",
      "    - New mask: 57 features, Fitness: -19.0071\n",
      "    - New mask: 59 features, Fitness: -21.8924\n",
      "    - New mask: 57 features, Fitness: -20.5272\n",
      "    - New mask: 56 features, Fitness: -18.9104\n",
      "    - New mask: 54 features, Fitness: -16.5371\n",
      "    - New mask: 53 features, Fitness: -16.7521\n",
      "    - New mask: 57 features, Fitness: -19.1301\n",
      "    - New mask: 58 features, Fitness: -20.6329\n",
      "    - New mask: 58 features, Fitness: -22.4711\n",
      "    - New mask: 53 features, Fitness: -16.7466\n",
      "    - New mask: 60 features, Fitness: -22.3622\n",
      "    - New mask: 58 features, Fitness: -19.0616\n",
      "    - New mask: 59 features, Fitness: -21.2268\n",
      "    - New mask: 58 features, Fitness: -20.7712\n",
      "    - New mask: 57 features, Fitness: -19.3219\n",
      "    - New mask: 57 features, Fitness: -19.0407\n",
      "    - New mask: 52 features, Fitness: -16.2340\n",
      "    - New mask: 63 features, Fitness: -24.5652\n",
      "    - New mask: 53 features, Fitness: -17.5590\n",
      "    - New mask: 56 features, Fitness: -18.8038\n",
      "    - New mask: 60 features, Fitness: -22.0889\n",
      "    - New mask: 55 features, Fitness: -17.4308\n",
      "    - New mask: 56 features, Fitness: -18.4213\n",
      "    - New mask: 52 features, Fitness: -15.8357\n",
      "    - New mask: 54 features, Fitness: -16.4680\n",
      "    - New mask: 57 features, Fitness: -19.2918\n",
      "    - New mask: 54 features, Fitness: -19.2218\n",
      "    - New mask: 59 features, Fitness: -20.2509\n",
      "    - New mask: 59 features, Fitness: -21.6810\n",
      "    - New mask: 56 features, Fitness: -19.2756\n",
      "    - New mask: 57 features, Fitness: -20.2481\n",
      "    - New mask: 60 features, Fitness: -20.7384\n",
      "    - New mask: 55 features, Fitness: -16.5596\n",
      "    - New mask: 57 features, Fitness: -19.4950\n",
      "    - New mask: 56 features, Fitness: -19.4675\n",
      "    - New mask: 56 features, Fitness: -18.8287\n",
      "    - New mask: 57 features, Fitness: -19.0812\n",
      "    - New mask: 57 features, Fitness: -16.9577\n",
      "    - New mask: 57 features, Fitness: -17.5266\n",
      "    - New mask: 61 features, Fitness: -23.3878\n",
      "    - New mask: 55 features, Fitness: -15.2209\n",
      "    - New mask: 61 features, Fitness: -21.2460\n",
      "    - New mask: 55 features, Fitness: -16.4834\n",
      "    - New mask: 58 features, Fitness: -18.8069\n",
      "    - New mask: 56 features, Fitness: -17.8372\n",
      "    - New mask: 56 features, Fitness: -16.4677\n",
      "    - New mask: 55 features, Fitness: -16.4389\n",
      "    - New mask: 57 features, Fitness: -19.0013\n",
      "    - New mask: 54 features, Fitness: -16.8135\n",
      "    - New mask: 58 features, Fitness: -19.1639\n",
      "    - New mask: 64 features, Fitness: -24.7193\n",
      "    - New mask: 54 features, Fitness: -16.8759\n",
      "    - New mask: 58 features, Fitness: -17.4679\n",
      "    - New mask: 59 features, Fitness: -19.6341\n",
      "    - New mask: 57 features, Fitness: -20.6296\n",
      "    - New mask: 53 features, Fitness: -15.5468\n",
      "    - New mask: 53 features, Fitness: -15.4735\n",
      "    - New mask: 61 features, Fitness: -20.7551\n",
      "    - New mask: 58 features, Fitness: -20.2823\n",
      "    - New mask: 51 features, Fitness: -14.3977\n",
      "    - New mask: 51 features, Fitness: -13.6696\n",
      "    - New mask: 57 features, Fitness: -18.9446\n",
      "    - New mask: 61 features, Fitness: -21.4006\n",
      "    - New mask: 56 features, Fitness: -17.1732\n",
      "    - New mask: 60 features, Fitness: -21.8726\n",
      "    - New mask: 58 features, Fitness: -17.9017\n",
      "    - New mask: 54 features, Fitness: -17.2547\n",
      "    - New mask: 57 features, Fitness: -16.7232\n",
      "    - New mask: 59 features, Fitness: -19.5692\n",
      "    - New mask: 59 features, Fitness: -21.7766\n",
      "    - New mask: 55 features, Fitness: -18.2218\n",
      "    - New mask: 55 features, Fitness: -16.4832\n",
      "    - New mask: 59 features, Fitness: -19.1306\n",
      "    - New mask: 57 features, Fitness: -16.7124\n",
      "    - New mask: 57 features, Fitness: -17.6878\n",
      "    - New mask: 54 features, Fitness: -14.3517\n",
      "    - New mask: 57 features, Fitness: -15.9984\n",
      "    - New mask: 58 features, Fitness: -16.8669\n",
      "    - New mask: 60 features, Fitness: -19.0972\n",
      "    - New mask: 52 features, Fitness: -12.4113\n",
      "    - New mask: 58 features, Fitness: -16.0863\n",
      "    - New mask: 58 features, Fitness: -15.8108\n",
      "    - New mask: 64 features, Fitness: -22.6256\n",
      "    - New mask: 57 features, Fitness: -15.8076\n",
      "    - New mask: 52 features, Fitness: -12.0373\n",
      "    - New mask: 56 features, Fitness: -14.4219\n",
      "    - New mask: 58 features, Fitness: -16.2023\n",
      "    - New mask: 54 features, Fitness: -12.8746\n",
      "    - New mask: 53 features, Fitness: -12.9822\n",
      "    - New mask: 58 features, Fitness: -17.8169\n",
      "    - New mask: 59 features, Fitness: -17.8917\n",
      "    - New mask: 55 features, Fitness: -13.4175\n",
      "    - New mask: 56 features, Fitness: -15.8282\n",
      "    - New mask: 59 features, Fitness: -17.1401\n",
      "    - New mask: 55 features, Fitness: -14.9437\n",
      "    - New mask: 61 features, Fitness: -25.0100\n",
      "    - New mask: 56 features, Fitness: -18.2969\n",
      "    - New mask: 57 features, Fitness: -19.1860\n",
      "    - New mask: 57 features, Fitness: -19.6358\n",
      "    - New mask: 59 features, Fitness: -22.4635\n",
      "    - New mask: 59 features, Fitness: -19.8908\n",
      "    - New mask: 56 features, Fitness: -19.7033\n",
      "    - New mask: 59 features, Fitness: -21.9075\n",
      "    - New mask: 55 features, Fitness: -18.5168\n",
      "    - New mask: 59 features, Fitness: -19.3522\n",
      "    - New mask: 60 features, Fitness: -21.6845\n",
      "    - New mask: 57 features, Fitness: -19.6070\n",
      "    - New mask: 52 features, Fitness: -15.8329\n",
      "    - New mask: 54 features, Fitness: -17.1707\n",
      "    - New mask: 58 features, Fitness: -20.1071\n",
      "    - New mask: 59 features, Fitness: -22.6571\n",
      "    - New mask: 62 features, Fitness: -24.1229\n",
      "    - New mask: 61 features, Fitness: -23.8723\n",
      "    - New mask: 60 features, Fitness: -24.2891\n",
      "    - New mask: 54 features, Fitness: -16.4670\n",
      "    - New mask: 58 features, Fitness: -19.4082\n",
      "    - New mask: 56 features, Fitness: -16.8275\n",
      "    - New mask: 56 features, Fitness: -16.5958\n",
      "    - New mask: 53 features, Fitness: -14.6549\n",
      "    - New mask: 57 features, Fitness: -18.2490\n",
      "    - New mask: 55 features, Fitness: -15.0314\n",
      "    - New mask: 55 features, Fitness: -16.2964\n",
      "    - New mask: 59 features, Fitness: -19.7934\n",
      "    - New mask: 56 features, Fitness: -15.7106\n",
      "    - New mask: 55 features, Fitness: -15.5540\n",
      "    - New mask: 58 features, Fitness: -19.6917\n",
      "    - New mask: 60 features, Fitness: -19.6098\n",
      "    - New mask: 59 features, Fitness: -17.6942\n",
      "    - New mask: 57 features, Fitness: -17.1444\n",
      "    - New mask: 61 features, Fitness: -21.4272\n",
      "    - New mask: 53 features, Fitness: -13.7189\n",
      "    - New mask: 56 features, Fitness: -16.5277\n",
      "    - New mask: 61 features, Fitness: -20.7343\n",
      "    - New mask: 54 features, Fitness: -13.8227\n",
      "    - New mask: 55 features, Fitness: -15.9504\n",
      "    - New mask: 57 features, Fitness: -22.1150\n",
      "    - New mask: 54 features, Fitness: -20.0937\n",
      "    - New mask: 54 features, Fitness: -18.5561\n",
      "    - New mask: 59 features, Fitness: -24.5284\n",
      "    - New mask: 55 features, Fitness: -21.3064\n",
      "    - New mask: 55 features, Fitness: -20.2930\n",
      "    - New mask: 58 features, Fitness: -26.2225\n",
      "    - New mask: 58 features, Fitness: -25.3969\n",
      "    - New mask: 56 features, Fitness: -22.3298\n",
      "    - New mask: 54 features, Fitness: -19.5651\n",
      "    - New mask: 55 features, Fitness: -21.6561\n",
      "    - New mask: 58 features, Fitness: -25.0505\n",
      "    - New mask: 59 features, Fitness: -23.6142\n",
      "    - New mask: 59 features, Fitness: -23.6692\n",
      "    - New mask: 60 features, Fitness: -26.1333\n",
      "    - New mask: 54 features, Fitness: -18.8923\n",
      "    - New mask: 53 features, Fitness: -20.6052\n",
      "    - New mask: 55 features, Fitness: -23.8749\n",
      "    - New mask: 54 features, Fitness: -22.0571\n",
      "    - New mask: 54 features, Fitness: -20.8038\n",
      "    - New mask: 56 features, Fitness: -21.5725\n",
      "    - New mask: 55 features, Fitness: -21.8918\n",
      "    - New mask: 59 features, Fitness: -26.4188\n",
      "    - New mask: 57 features, Fitness: -24.3405\n",
      "    - New mask: 60 features, Fitness: -26.1482\n",
      "    - New mask: 58 features, Fitness: -25.7093\n",
      "    - New mask: 57 features, Fitness: -24.5831\n",
      "    - New mask: 58 features, Fitness: -25.9984\n",
      "    - New mask: 54 features, Fitness: -21.3073\n",
      "    - New mask: 56 features, Fitness: -22.8602\n",
      "    - New mask: 56 features, Fitness: -20.7194\n",
      "    - New mask: 53 features, Fitness: -20.9593\n",
      "    - New mask: 56 features, Fitness: -22.2525\n",
      "    - New mask: 56 features, Fitness: -21.3438\n",
      "    - New mask: 55 features, Fitness: -20.8417\n",
      "    - New mask: 54 features, Fitness: -20.1129\n",
      "    - New mask: 53 features, Fitness: -19.5586\n",
      "    - New mask: 54 features, Fitness: -21.0624\n",
      "    - New mask: 52 features, Fitness: -19.3846\n",
      "    - New mask: 56 features, Fitness: -23.8592\n",
      "    - New mask: 57 features, Fitness: -19.7046\n",
      "    - New mask: 54 features, Fitness: -18.5762\n",
      "    - New mask: 56 features, Fitness: -18.9094\n",
      "    - New mask: 56 features, Fitness: -19.1630\n",
      "    - New mask: 57 features, Fitness: -20.9277\n",
      "    - New mask: 60 features, Fitness: -22.4434\n",
      "    - New mask: 60 features, Fitness: -24.5104\n",
      "    - New mask: 60 features, Fitness: -24.9974\n",
      "    - New mask: 60 features, Fitness: -24.3267\n",
      "    - New mask: 57 features, Fitness: -20.7777\n",
      "    - New mask: 58 features, Fitness: -19.8628\n",
      "    - New mask: 56 features, Fitness: -20.3902\n",
      "    - New mask: 57 features, Fitness: -19.7968\n",
      "    - New mask: 59 features, Fitness: -24.0333\n",
      "    - New mask: 54 features, Fitness: -16.2272\n",
      "    - New mask: 63 features, Fitness: -26.7486\n",
      "    - New mask: 57 features, Fitness: -20.7840\n",
      "    - New mask: 57 features, Fitness: -21.4474\n",
      "    - New mask: 58 features, Fitness: -21.4334\n",
      "    - New mask: 58 features, Fitness: -21.3920\n",
      "=== End of Round 6: Vote mask selects 67 features (rho: 0.36)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 7 ================\n",
      "  Adaptive rho for this round: 0.39\n",
      "    - New mask: 60 features, Fitness: -22.4418\n",
      "    - New mask: 62 features, Fitness: -24.9226\n",
      "    - New mask: 59 features, Fitness: -21.4029\n",
      "    - New mask: 54 features, Fitness: -18.4000\n",
      "    - New mask: 56 features, Fitness: -17.4823\n",
      "    - New mask: 59 features, Fitness: -21.2151\n",
      "    - New mask: 57 features, Fitness: -19.2439\n",
      "    - New mask: 57 features, Fitness: -20.7898\n",
      "    - New mask: 61 features, Fitness: -24.2296\n",
      "    - New mask: 54 features, Fitness: -16.6795\n",
      "    - New mask: 58 features, Fitness: -20.5945\n",
      "    - New mask: 56 features, Fitness: -18.0225\n",
      "    - New mask: 55 features, Fitness: -18.4980\n",
      "    - New mask: 60 features, Fitness: -22.5712\n",
      "    - New mask: 56 features, Fitness: -18.6882\n",
      "    - New mask: 57 features, Fitness: -19.7455\n",
      "    - New mask: 62 features, Fitness: -24.4135\n",
      "    - New mask: 61 features, Fitness: -22.6143\n",
      "    - New mask: 58 features, Fitness: -20.9831\n",
      "    - New mask: 56 features, Fitness: -18.9750\n",
      "    - New mask: 55 features, Fitness: -17.4674\n",
      "    - New mask: 54 features, Fitness: -19.6252\n",
      "    - New mask: 62 features, Fitness: -23.0726\n",
      "    - New mask: 55 features, Fitness: -18.4970\n",
      "    - New mask: 59 features, Fitness: -21.5348\n",
      "    - New mask: 58 features, Fitness: -21.7956\n",
      "    - New mask: 60 features, Fitness: -22.5144\n",
      "    - New mask: 63 features, Fitness: -25.3469\n",
      "    - New mask: 54 features, Fitness: -18.4566\n",
      "    - New mask: 58 features, Fitness: -19.4942\n",
      "    - New mask: 55 features, Fitness: -17.9559\n",
      "    - New mask: 57 features, Fitness: -20.6880\n",
      "    - New mask: 60 features, Fitness: -22.2422\n",
      "    - New mask: 55 features, Fitness: -17.5315\n",
      "    - New mask: 56 features, Fitness: -20.1209\n",
      "    - New mask: 56 features, Fitness: -20.4662\n",
      "    - New mask: 59 features, Fitness: -20.1280\n",
      "    - New mask: 57 features, Fitness: -19.0475\n",
      "    - New mask: 53 features, Fitness: -17.3961\n",
      "    - New mask: 58 features, Fitness: -20.6468\n",
      "    - New mask: 59 features, Fitness: -18.0371\n",
      "    - New mask: 57 features, Fitness: -17.7887\n",
      "    - New mask: 57 features, Fitness: -17.1096\n",
      "    - New mask: 62 features, Fitness: -22.3528\n",
      "    - New mask: 57 features, Fitness: -18.3454\n",
      "    - New mask: 59 features, Fitness: -19.3291\n",
      "    - New mask: 61 features, Fitness: -21.3413\n",
      "    - New mask: 55 features, Fitness: -15.3746\n",
      "    - New mask: 64 features, Fitness: -23.4338\n",
      "    - New mask: 59 features, Fitness: -20.9551\n",
      "    - New mask: 60 features, Fitness: -20.1286\n",
      "    - New mask: 59 features, Fitness: -19.2395\n",
      "    - New mask: 59 features, Fitness: -20.0232\n",
      "    - New mask: 59 features, Fitness: -19.8164\n",
      "    - New mask: 58 features, Fitness: -18.5197\n",
      "    - New mask: 63 features, Fitness: -22.4703\n",
      "    - New mask: 56 features, Fitness: -17.3040\n",
      "    - New mask: 61 features, Fitness: -20.4227\n",
      "    - New mask: 59 features, Fitness: -19.8204\n",
      "    - New mask: 61 features, Fitness: -20.9498\n",
      "    - New mask: 59 features, Fitness: -21.1416\n",
      "    - New mask: 61 features, Fitness: -21.0766\n",
      "    - New mask: 60 features, Fitness: -20.7166\n",
      "    - New mask: 56 features, Fitness: -18.8184\n",
      "    - New mask: 61 features, Fitness: -21.0318\n",
      "    - New mask: 53 features, Fitness: -14.4973\n",
      "    - New mask: 56 features, Fitness: -15.9872\n",
      "    - New mask: 55 features, Fitness: -17.4028\n",
      "    - New mask: 59 features, Fitness: -19.0723\n",
      "    - New mask: 59 features, Fitness: -20.6569\n",
      "    - New mask: 54 features, Fitness: -16.1170\n",
      "    - New mask: 59 features, Fitness: -20.1849\n",
      "    - New mask: 60 features, Fitness: -21.6394\n",
      "    - New mask: 62 features, Fitness: -22.9378\n",
      "    - New mask: 61 features, Fitness: -21.1474\n",
      "    - New mask: 51 features, Fitness: -14.3955\n",
      "    - New mask: 59 features, Fitness: -21.0849\n",
      "    - New mask: 59 features, Fitness: -18.7543\n",
      "    - New mask: 58 features, Fitness: -17.8536\n",
      "    - New mask: 54 features, Fitness: -15.2934\n",
      "    - New mask: 59 features, Fitness: -18.0643\n",
      "    - New mask: 57 features, Fitness: -17.2090\n",
      "    - New mask: 60 features, Fitness: -19.4348\n",
      "    - New mask: 60 features, Fitness: -18.9924\n",
      "    - New mask: 53 features, Fitness: -12.4648\n",
      "    - New mask: 60 features, Fitness: -18.3549\n",
      "    - New mask: 53 features, Fitness: -12.2943\n",
      "    - New mask: 64 features, Fitness: -21.7292\n",
      "    - New mask: 61 features, Fitness: -20.2541\n",
      "    - New mask: 55 features, Fitness: -14.8615\n",
      "    - New mask: 56 features, Fitness: -14.6645\n",
      "    - New mask: 59 features, Fitness: -16.9018\n",
      "    - New mask: 58 features, Fitness: -16.4266\n",
      "    - New mask: 58 features, Fitness: -17.0923\n",
      "    - New mask: 60 features, Fitness: -19.0262\n",
      "    - New mask: 59 features, Fitness: -17.9858\n",
      "    - New mask: 57 features, Fitness: -15.1288\n",
      "    - New mask: 60 features, Fitness: -17.2715\n",
      "    - New mask: 59 features, Fitness: -16.1231\n",
      "    - New mask: 61 features, Fitness: -21.1380\n",
      "    - New mask: 60 features, Fitness: -21.7543\n",
      "    - New mask: 55 features, Fitness: -19.7612\n",
      "    - New mask: 56 features, Fitness: -18.0634\n",
      "    - New mask: 57 features, Fitness: -20.1966\n",
      "    - New mask: 56 features, Fitness: -19.4004\n",
      "    - New mask: 58 features, Fitness: -20.6318\n",
      "    - New mask: 59 features, Fitness: -21.3434\n",
      "    - New mask: 57 features, Fitness: -18.3472\n",
      "    - New mask: 59 features, Fitness: -23.2447\n",
      "    - New mask: 60 features, Fitness: -22.7365\n",
      "    - New mask: 61 features, Fitness: -22.4857\n",
      "    - New mask: 59 features, Fitness: -21.1052\n",
      "    - New mask: 55 features, Fitness: -19.0924\n",
      "    - New mask: 53 features, Fitness: -18.0215\n",
      "    - New mask: 59 features, Fitness: -21.2066\n",
      "    - New mask: 60 features, Fitness: -22.5583\n",
      "    - New mask: 57 features, Fitness: -19.5179\n",
      "    - New mask: 59 features, Fitness: -21.8594\n",
      "    - New mask: 60 features, Fitness: -21.8214\n",
      "    - New mask: 55 features, Fitness: -18.1331\n",
      "    - New mask: 55 features, Fitness: -16.3065\n",
      "    - New mask: 61 features, Fitness: -21.5122\n",
      "    - New mask: 58 features, Fitness: -17.3783\n",
      "    - New mask: 56 features, Fitness: -15.5979\n",
      "    - New mask: 59 features, Fitness: -18.8269\n",
      "    - New mask: 55 features, Fitness: -15.9387\n",
      "    - New mask: 54 features, Fitness: -15.3008\n",
      "    - New mask: 59 features, Fitness: -18.5236\n",
      "    - New mask: 56 features, Fitness: -16.5056\n",
      "    - New mask: 55 features, Fitness: -15.0737\n",
      "    - New mask: 60 features, Fitness: -18.8165\n",
      "    - New mask: 57 features, Fitness: -18.2261\n",
      "    - New mask: 56 features, Fitness: -16.8836\n",
      "    - New mask: 55 features, Fitness: -15.8873\n",
      "    - New mask: 59 features, Fitness: -17.8909\n",
      "    - New mask: 59 features, Fitness: -19.1632\n",
      "    - New mask: 57 features, Fitness: -16.8055\n",
      "    - New mask: 58 features, Fitness: -19.0937\n",
      "    - New mask: 53 features, Fitness: -12.5859\n",
      "    - New mask: 57 features, Fitness: -17.8422\n",
      "    - New mask: 59 features, Fitness: -24.2818\n",
      "    - New mask: 55 features, Fitness: -20.4715\n",
      "    - New mask: 56 features, Fitness: -19.9675\n",
      "    - New mask: 60 features, Fitness: -25.3664\n",
      "    - New mask: 61 features, Fitness: -27.0169\n",
      "    - New mask: 58 features, Fitness: -23.9245\n",
      "    - New mask: 59 features, Fitness: -23.4609\n",
      "    - New mask: 58 features, Fitness: -22.5345\n",
      "    - New mask: 59 features, Fitness: -23.3184\n",
      "    - New mask: 54 features, Fitness: -18.0544\n",
      "    - New mask: 61 features, Fitness: -26.1814\n",
      "    - New mask: 59 features, Fitness: -24.8162\n",
      "    - New mask: 54 features, Fitness: -18.5341\n",
      "    - New mask: 63 features, Fitness: -26.7959\n",
      "    - New mask: 63 features, Fitness: -28.2225\n",
      "    - New mask: 58 features, Fitness: -23.5850\n",
      "    - New mask: 52 features, Fitness: -18.9922\n",
      "    - New mask: 53 features, Fitness: -20.2705\n",
      "    - New mask: 56 features, Fitness: -22.0704\n",
      "    - New mask: 58 features, Fitness: -23.6312\n",
      "    - New mask: 56 features, Fitness: -22.8219\n",
      "    - New mask: 58 features, Fitness: -24.7152\n",
      "    - New mask: 61 features, Fitness: -27.6556\n",
      "    - New mask: 54 features, Fitness: -20.5498\n",
      "    - New mask: 58 features, Fitness: -24.1439\n",
      "    - New mask: 54 features, Fitness: -21.3415\n",
      "    - New mask: 60 features, Fitness: -26.3460\n",
      "    - New mask: 58 features, Fitness: -25.4932\n",
      "    - New mask: 58 features, Fitness: -26.9656\n",
      "    - New mask: 59 features, Fitness: -25.1033\n",
      "    - New mask: 55 features, Fitness: -19.0020\n",
      "    - New mask: 54 features, Fitness: -21.1371\n",
      "    - New mask: 53 features, Fitness: -17.6110\n",
      "    - New mask: 58 features, Fitness: -25.7574\n",
      "    - New mask: 59 features, Fitness: -26.1080\n",
      "    - New mask: 56 features, Fitness: -22.7175\n",
      "    - New mask: 53 features, Fitness: -18.9894\n",
      "    - New mask: 57 features, Fitness: -24.6459\n",
      "    - New mask: 58 features, Fitness: -23.6737\n",
      "    - New mask: 59 features, Fitness: -24.7305\n",
      "    - New mask: 59 features, Fitness: -22.0298\n",
      "    - New mask: 57 features, Fitness: -20.3933\n",
      "    - New mask: 59 features, Fitness: -21.6328\n",
      "    - New mask: 57 features, Fitness: -19.6392\n",
      "    - New mask: 58 features, Fitness: -20.0357\n",
      "    - New mask: 58 features, Fitness: -20.4186\n",
      "    - New mask: 62 features, Fitness: -25.0185\n",
      "    - New mask: 61 features, Fitness: -24.3181\n",
      "    - New mask: 55 features, Fitness: -17.1259\n",
      "    - New mask: 57 features, Fitness: -21.4174\n",
      "    - New mask: 59 features, Fitness: -21.0851\n",
      "    - New mask: 54 features, Fitness: -18.7222\n",
      "    - New mask: 55 features, Fitness: -17.7237\n",
      "    - New mask: 61 features, Fitness: -25.0757\n",
      "    - New mask: 59 features, Fitness: -21.6473\n",
      "    - New mask: 61 features, Fitness: -23.6132\n",
      "    - New mask: 59 features, Fitness: -22.4319\n",
      "    - New mask: 58 features, Fitness: -21.3803\n",
      "    - New mask: 58 features, Fitness: -21.5998\n",
      "    - New mask: 57 features, Fitness: -19.4566\n",
      "=== End of Round 7: Vote mask selects 67 features (rho: 0.39)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 8 ================\n",
      "  Adaptive rho for this round: 0.42\n",
      "    - New mask: 58 features, Fitness: -20.8900\n",
      "    - New mask: 60 features, Fitness: -22.3971\n",
      "    - New mask: 61 features, Fitness: -22.1558\n",
      "    - New mask: 59 features, Fitness: -23.4071\n",
      "    - New mask: 59 features, Fitness: -20.1256\n",
      "    - New mask: 58 features, Fitness: -20.1982\n",
      "    - New mask: 59 features, Fitness: -21.6846\n",
      "    - New mask: 54 features, Fitness: -18.9228\n",
      "    - New mask: 62 features, Fitness: -24.6973\n",
      "    - New mask: 58 features, Fitness: -20.8302\n",
      "    - New mask: 59 features, Fitness: -20.0822\n",
      "    - New mask: 57 features, Fitness: -19.5637\n",
      "    - New mask: 61 features, Fitness: -23.1216\n",
      "    - New mask: 59 features, Fitness: -21.7985\n",
      "    - New mask: 59 features, Fitness: -22.0349\n",
      "    - New mask: 58 features, Fitness: -20.6352\n",
      "    - New mask: 64 features, Fitness: -27.4290\n",
      "    - New mask: 62 features, Fitness: -25.0156\n",
      "    - New mask: 60 features, Fitness: -22.4137\n",
      "    - New mask: 58 features, Fitness: -20.6800\n",
      "    - New mask: 59 features, Fitness: -22.0097\n",
      "    - New mask: 59 features, Fitness: -23.3311\n",
      "    - New mask: 61 features, Fitness: -24.7194\n",
      "    - New mask: 57 features, Fitness: -20.5489\n",
      "    - New mask: 59 features, Fitness: -21.8620\n",
      "    - New mask: 59 features, Fitness: -22.7893\n",
      "    - New mask: 57 features, Fitness: -20.2877\n",
      "    - New mask: 59 features, Fitness: -21.3644\n",
      "    - New mask: 58 features, Fitness: -20.9972\n",
      "    - New mask: 61 features, Fitness: -23.0891\n",
      "    - New mask: 60 features, Fitness: -20.7507\n",
      "    - New mask: 61 features, Fitness: -24.4749\n",
      "    - New mask: 58 features, Fitness: -20.4505\n",
      "    - New mask: 54 features, Fitness: -17.6119\n",
      "    - New mask: 58 features, Fitness: -19.9181\n",
      "    - New mask: 60 features, Fitness: -21.4510\n",
      "    - New mask: 57 features, Fitness: -19.9740\n",
      "    - New mask: 58 features, Fitness: -19.5584\n",
      "    - New mask: 61 features, Fitness: -23.7233\n",
      "    - New mask: 64 features, Fitness: -26.5794\n",
      "    - New mask: 60 features, Fitness: -19.8283\n",
      "    - New mask: 59 features, Fitness: -19.6819\n",
      "    - New mask: 55 features, Fitness: -16.2633\n",
      "    - New mask: 58 features, Fitness: -19.6431\n",
      "    - New mask: 57 features, Fitness: -16.6615\n",
      "    - New mask: 59 features, Fitness: -19.6792\n",
      "    - New mask: 60 features, Fitness: -20.4264\n",
      "    - New mask: 58 features, Fitness: -18.2709\n",
      "    - New mask: 62 features, Fitness: -21.6233\n",
      "    - New mask: 58 features, Fitness: -19.7145\n",
      "    - New mask: 57 features, Fitness: -17.3150\n",
      "    - New mask: 62 features, Fitness: -22.0162\n",
      "    - New mask: 60 features, Fitness: -19.8504\n",
      "    - New mask: 58 features, Fitness: -17.6413\n",
      "    - New mask: 58 features, Fitness: -18.0863\n",
      "    - New mask: 61 features, Fitness: -21.1972\n",
      "    - New mask: 59 features, Fitness: -18.8247\n",
      "    - New mask: 60 features, Fitness: -19.3624\n",
      "    - New mask: 58 features, Fitness: -18.1973\n",
      "    - New mask: 61 features, Fitness: -20.1196\n",
      "    - New mask: 56 features, Fitness: -19.7236\n",
      "    - New mask: 55 features, Fitness: -15.6940\n",
      "    - New mask: 54 features, Fitness: -16.3628\n",
      "    - New mask: 61 features, Fitness: -22.6714\n",
      "    - New mask: 60 features, Fitness: -19.8901\n",
      "    - New mask: 57 features, Fitness: -18.3020\n",
      "    - New mask: 55 features, Fitness: -16.3961\n",
      "    - New mask: 57 features, Fitness: -18.3788\n",
      "    - New mask: 61 features, Fitness: -21.4010\n",
      "    - New mask: 57 features, Fitness: -18.2314\n",
      "    - New mask: 58 features, Fitness: -20.3502\n",
      "    - New mask: 58 features, Fitness: -19.0856\n",
      "    - New mask: 59 features, Fitness: -20.8445\n",
      "    - New mask: 58 features, Fitness: -19.3103\n",
      "    - New mask: 62 features, Fitness: -21.4446\n",
      "    - New mask: 53 features, Fitness: -15.8841\n",
      "    - New mask: 58 features, Fitness: -19.9224\n",
      "    - New mask: 61 features, Fitness: -22.3712\n",
      "    - New mask: 57 features, Fitness: -18.0687\n",
      "    - New mask: 54 features, Fitness: -14.9492\n",
      "    - New mask: 57 features, Fitness: -15.5530\n",
      "    - New mask: 55 features, Fitness: -15.9412\n",
      "    - New mask: 58 features, Fitness: -17.5975\n",
      "    - New mask: 56 features, Fitness: -14.8295\n",
      "    - New mask: 55 features, Fitness: -12.2491\n",
      "    - New mask: 59 features, Fitness: -17.5690\n",
      "    - New mask: 57 features, Fitness: -15.9066\n",
      "    - New mask: 60 features, Fitness: -19.0603\n",
      "    - New mask: 59 features, Fitness: -17.3553\n",
      "    - New mask: 59 features, Fitness: -16.8487\n",
      "    - New mask: 55 features, Fitness: -14.7928\n",
      "    - New mask: 61 features, Fitness: -19.6884\n",
      "    - New mask: 58 features, Fitness: -16.3757\n",
      "    - New mask: 57 features, Fitness: -15.4020\n",
      "    - New mask: 56 features, Fitness: -16.0769\n",
      "    - New mask: 58 features, Fitness: -16.7872\n",
      "    - New mask: 60 features, Fitness: -18.8851\n",
      "    - New mask: 57 features, Fitness: -16.7113\n",
      "    - New mask: 61 features, Fitness: -18.1971\n",
      "    - New mask: 59 features, Fitness: -20.9379\n",
      "    - New mask: 60 features, Fitness: -23.6687\n",
      "    - New mask: 59 features, Fitness: -22.3235\n",
      "    - New mask: 60 features, Fitness: -23.2726\n",
      "    - New mask: 59 features, Fitness: -21.7887\n",
      "    - New mask: 58 features, Fitness: -20.1752\n",
      "    - New mask: 56 features, Fitness: -19.5339\n",
      "    - New mask: 62 features, Fitness: -24.1928\n",
      "    - New mask: 62 features, Fitness: -22.8109\n",
      "    - New mask: 58 features, Fitness: -21.5798\n",
      "    - New mask: 60 features, Fitness: -22.0535\n",
      "    - New mask: 58 features, Fitness: -20.0567\n",
      "    - New mask: 58 features, Fitness: -20.2476\n",
      "    - New mask: 54 features, Fitness: -17.7504\n",
      "    - New mask: 56 features, Fitness: -21.2340\n",
      "    - New mask: 57 features, Fitness: -19.1231\n",
      "    - New mask: 54 features, Fitness: -18.1694\n",
      "    - New mask: 61 features, Fitness: -23.3158\n",
      "    - New mask: 58 features, Fitness: -21.2578\n",
      "    - New mask: 59 features, Fitness: -23.4975\n",
      "    - New mask: 57 features, Fitness: -22.9258\n",
      "    - New mask: 54 features, Fitness: -15.0441\n",
      "    - New mask: 58 features, Fitness: -18.2137\n",
      "    - New mask: 61 features, Fitness: -20.1605\n",
      "    - New mask: 57 features, Fitness: -17.3062\n",
      "    - New mask: 56 features, Fitness: -16.1938\n",
      "    - New mask: 57 features, Fitness: -15.7078\n",
      "    - New mask: 56 features, Fitness: -15.4515\n",
      "    - New mask: 56 features, Fitness: -14.8532\n",
      "    - New mask: 60 features, Fitness: -18.2173\n",
      "    - New mask: 56 features, Fitness: -15.1697\n",
      "    - New mask: 56 features, Fitness: -15.1653\n",
      "    - New mask: 55 features, Fitness: -15.5949\n",
      "    - New mask: 54 features, Fitness: -16.5007\n",
      "    - New mask: 59 features, Fitness: -19.4835\n",
      "    - New mask: 55 features, Fitness: -16.1362\n",
      "    - New mask: 59 features, Fitness: -19.1638\n",
      "    - New mask: 55 features, Fitness: -14.9331\n",
      "    - New mask: 62 features, Fitness: -20.2759\n",
      "    - New mask: 59 features, Fitness: -17.4626\n",
      "    - New mask: 59 features, Fitness: -18.3454\n",
      "    - New mask: 60 features, Fitness: -23.3268\n",
      "    - New mask: 54 features, Fitness: -18.5085\n",
      "    - New mask: 56 features, Fitness: -20.2466\n",
      "    - New mask: 61 features, Fitness: -25.3399\n",
      "    - New mask: 58 features, Fitness: -20.8094\n",
      "    - New mask: 59 features, Fitness: -25.3438\n",
      "    - New mask: 60 features, Fitness: -25.3775\n",
      "    - New mask: 58 features, Fitness: -21.7868\n",
      "    - New mask: 57 features, Fitness: -22.8175\n",
      "    - New mask: 57 features, Fitness: -20.8978\n",
      "    - New mask: 57 features, Fitness: -21.1910\n",
      "    - New mask: 58 features, Fitness: -24.7008\n",
      "    - New mask: 56 features, Fitness: -20.3488\n",
      "    - New mask: 59 features, Fitness: -23.3451\n",
      "    - New mask: 57 features, Fitness: -22.7496\n",
      "    - New mask: 58 features, Fitness: -22.4418\n",
      "    - New mask: 57 features, Fitness: -22.1083\n",
      "    - New mask: 55 features, Fitness: -20.4049\n",
      "    - New mask: 58 features, Fitness: -24.9872\n",
      "    - New mask: 60 features, Fitness: -24.6858\n",
      "    - New mask: 58 features, Fitness: -23.5568\n",
      "    - New mask: 60 features, Fitness: -26.1805\n",
      "    - New mask: 56 features, Fitness: -20.2809\n",
      "    - New mask: 56 features, Fitness: -21.3472\n",
      "    - New mask: 59 features, Fitness: -24.1149\n",
      "    - New mask: 58 features, Fitness: -24.3111\n",
      "    - New mask: 56 features, Fitness: -22.3390\n",
      "    - New mask: 60 features, Fitness: -27.3730\n",
      "    - New mask: 56 features, Fitness: -22.1159\n",
      "    - New mask: 54 features, Fitness: -18.6393\n",
      "    - New mask: 57 features, Fitness: -23.6850\n",
      "    - New mask: 55 features, Fitness: -21.3233\n",
      "    - New mask: 60 features, Fitness: -24.8240\n",
      "    - New mask: 58 features, Fitness: -25.9998\n",
      "    - New mask: 60 features, Fitness: -26.8854\n",
      "    - New mask: 56 features, Fitness: -21.3008\n",
      "    - New mask: 52 features, Fitness: -17.0241\n",
      "    - New mask: 55 features, Fitness: -21.9707\n",
      "    - New mask: 60 features, Fitness: -26.2409\n",
      "    - New mask: 59 features, Fitness: -25.3397\n",
      "    - New mask: 58 features, Fitness: -20.5717\n",
      "    - New mask: 57 features, Fitness: -19.6645\n",
      "    - New mask: 55 features, Fitness: -17.4533\n",
      "    - New mask: 61 features, Fitness: -22.8858\n",
      "    - New mask: 57 features, Fitness: -18.4619\n",
      "    - New mask: 62 features, Fitness: -26.5698\n",
      "    - New mask: 63 features, Fitness: -26.3859\n",
      "    - New mask: 60 features, Fitness: -22.3012\n",
      "    - New mask: 56 features, Fitness: -18.8475\n",
      "    - New mask: 56 features, Fitness: -20.4471\n",
      "    - New mask: 58 features, Fitness: -20.3750\n",
      "    - New mask: 59 features, Fitness: -22.8268\n",
      "    - New mask: 59 features, Fitness: -22.1597\n",
      "    - New mask: 60 features, Fitness: -24.7688\n",
      "    - New mask: 58 features, Fitness: -20.9736\n",
      "    - New mask: 58 features, Fitness: -21.0519\n",
      "    - New mask: 59 features, Fitness: -21.2899\n",
      "    - New mask: 63 features, Fitness: -26.1218\n",
      "    - New mask: 58 features, Fitness: -20.8400\n",
      "    - New mask: 59 features, Fitness: -20.4276\n",
      "=== End of Round 8: Vote mask selects 67 features (rho: 0.42)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 9 ================\n",
      "  Adaptive rho for this round: 0.45\n",
      "    - New mask: 60 features, Fitness: -22.6553\n",
      "    - New mask: 58 features, Fitness: -23.2910\n",
      "    - New mask: 58 features, Fitness: -20.1191\n",
      "    - New mask: 58 features, Fitness: -22.5979\n",
      "    - New mask: 58 features, Fitness: -23.3981\n",
      "    - New mask: 58 features, Fitness: -20.0381\n",
      "    - New mask: 60 features, Fitness: -22.4255\n",
      "    - New mask: 55 features, Fitness: -18.9714\n",
      "    - New mask: 61 features, Fitness: -23.7216\n",
      "    - New mask: 56 features, Fitness: -20.8186\n",
      "    - New mask: 58 features, Fitness: -20.4259\n",
      "    - New mask: 53 features, Fitness: -18.0350\n",
      "    - New mask: 58 features, Fitness: -19.5297\n",
      "    - New mask: 60 features, Fitness: -25.3105\n",
      "    - New mask: 60 features, Fitness: -23.1590\n",
      "    - New mask: 60 features, Fitness: -22.8777\n",
      "    - New mask: 65 features, Fitness: -27.5432\n",
      "    - New mask: 63 features, Fitness: -27.0443\n",
      "    - New mask: 61 features, Fitness: -22.4598\n",
      "    - New mask: 56 features, Fitness: -19.4236\n",
      "    - New mask: 60 features, Fitness: -23.3341\n",
      "    - New mask: 60 features, Fitness: -22.1200\n",
      "    - New mask: 61 features, Fitness: -26.0178\n",
      "    - New mask: 59 features, Fitness: -23.1775\n",
      "    - New mask: 60 features, Fitness: -23.0187\n",
      "    - New mask: 62 features, Fitness: -26.5266\n",
      "    - New mask: 61 features, Fitness: -23.4918\n",
      "    - New mask: 57 features, Fitness: -19.5731\n",
      "    - New mask: 57 features, Fitness: -20.2717\n",
      "    - New mask: 54 features, Fitness: -19.6564\n",
      "    - New mask: 61 features, Fitness: -23.5099\n",
      "    - New mask: 60 features, Fitness: -24.2169\n",
      "    - New mask: 62 features, Fitness: -24.9016\n",
      "    - New mask: 58 features, Fitness: -21.4385\n",
      "    - New mask: 59 features, Fitness: -22.4080\n",
      "    - New mask: 59 features, Fitness: -22.8122\n",
      "    - New mask: 60 features, Fitness: -22.5364\n",
      "    - New mask: 59 features, Fitness: -20.9890\n",
      "    - New mask: 59 features, Fitness: -21.5556\n",
      "    - New mask: 61 features, Fitness: -23.1964\n",
      "    - New mask: 57 features, Fitness: -17.3403\n",
      "    - New mask: 55 features, Fitness: -16.5019\n",
      "    - New mask: 56 features, Fitness: -17.0878\n",
      "    - New mask: 59 features, Fitness: -18.8683\n",
      "    - New mask: 60 features, Fitness: -19.4157\n",
      "    - New mask: 56 features, Fitness: -16.8989\n",
      "    - New mask: 59 features, Fitness: -19.0378\n",
      "    - New mask: 57 features, Fitness: -17.5648\n",
      "    - New mask: 57 features, Fitness: -18.1149\n",
      "    - New mask: 57 features, Fitness: -18.5444\n",
      "    - New mask: 52 features, Fitness: -13.5987\n",
      "    - New mask: 64 features, Fitness: -24.3045\n",
      "    - New mask: 61 features, Fitness: -20.7945\n",
      "    - New mask: 59 features, Fitness: -20.1162\n",
      "    - New mask: 59 features, Fitness: -19.7354\n",
      "    - New mask: 60 features, Fitness: -19.4442\n",
      "    - New mask: 59 features, Fitness: -20.4128\n",
      "    - New mask: 60 features, Fitness: -21.9309\n",
      "    - New mask: 56 features, Fitness: -17.8969\n",
      "    - New mask: 61 features, Fitness: -21.2031\n",
      "    - New mask: 56 features, Fitness: -17.5999\n",
      "    - New mask: 56 features, Fitness: -16.0529\n",
      "    - New mask: 58 features, Fitness: -17.6559\n",
      "    - New mask: 59 features, Fitness: -19.8696\n",
      "    - New mask: 57 features, Fitness: -16.6844\n",
      "    - New mask: 56 features, Fitness: -16.7179\n",
      "    - New mask: 57 features, Fitness: -16.3729\n",
      "    - New mask: 57 features, Fitness: -17.3599\n",
      "    - New mask: 59 features, Fitness: -20.1727\n",
      "    - New mask: 57 features, Fitness: -17.9287\n",
      "    - New mask: 59 features, Fitness: -19.6391\n",
      "    - New mask: 58 features, Fitness: -19.6847\n",
      "    - New mask: 60 features, Fitness: -20.0747\n",
      "    - New mask: 58 features, Fitness: -19.2773\n",
      "    - New mask: 62 features, Fitness: -22.0279\n",
      "    - New mask: 57 features, Fitness: -19.5047\n",
      "    - New mask: 56 features, Fitness: -17.0374\n",
      "    - New mask: 63 features, Fitness: -22.9291\n",
      "    - New mask: 58 features, Fitness: -19.6186\n",
      "    - New mask: 54 features, Fitness: -14.9530\n",
      "    - New mask: 56 features, Fitness: -15.1682\n",
      "    - New mask: 53 features, Fitness: -11.4978\n",
      "    - New mask: 62 features, Fitness: -18.0899\n",
      "    - New mask: 60 features, Fitness: -18.1801\n",
      "    - New mask: 58 features, Fitness: -16.0274\n",
      "    - New mask: 57 features, Fitness: -15.0137\n",
      "    - New mask: 56 features, Fitness: -15.9366\n",
      "    - New mask: 59 features, Fitness: -16.1354\n",
      "    - New mask: 61 features, Fitness: -17.2152\n",
      "    - New mask: 60 features, Fitness: -17.4051\n",
      "    - New mask: 56 features, Fitness: -15.1333\n",
      "    - New mask: 61 features, Fitness: -20.9343\n",
      "    - New mask: 63 features, Fitness: -19.5735\n",
      "    - New mask: 58 features, Fitness: -16.2127\n",
      "    - New mask: 58 features, Fitness: -15.5700\n",
      "    - New mask: 60 features, Fitness: -19.3266\n",
      "    - New mask: 57 features, Fitness: -15.0173\n",
      "    - New mask: 63 features, Fitness: -21.5270\n",
      "    - New mask: 63 features, Fitness: -19.7087\n",
      "    - New mask: 61 features, Fitness: -18.5474\n",
      "    - New mask: 59 features, Fitness: -24.2473\n",
      "    - New mask: 56 features, Fitness: -18.9700\n",
      "    - New mask: 61 features, Fitness: -22.1503\n",
      "    - New mask: 56 features, Fitness: -20.8380\n",
      "    - New mask: 57 features, Fitness: -21.9886\n",
      "    - New mask: 58 features, Fitness: -21.0147\n",
      "    - New mask: 56 features, Fitness: -19.4739\n",
      "    - New mask: 61 features, Fitness: -23.3827\n",
      "    - New mask: 60 features, Fitness: -23.4194\n",
      "    - New mask: 59 features, Fitness: -23.0204\n",
      "    - New mask: 56 features, Fitness: -18.0900\n",
      "    - New mask: 57 features, Fitness: -20.0996\n",
      "    - New mask: 55 features, Fitness: -20.7405\n",
      "    - New mask: 59 features, Fitness: -23.1283\n",
      "    - New mask: 62 features, Fitness: -23.7065\n",
      "    - New mask: 57 features, Fitness: -22.4314\n",
      "    - New mask: 61 features, Fitness: -21.7597\n",
      "    - New mask: 58 features, Fitness: -21.8504\n",
      "    - New mask: 58 features, Fitness: -20.7948\n",
      "    - New mask: 59 features, Fitness: -23.4475\n",
      "    - New mask: 61 features, Fitness: -19.5975\n",
      "    - New mask: 59 features, Fitness: -18.1507\n",
      "    - New mask: 58 features, Fitness: -17.2049\n",
      "    - New mask: 58 features, Fitness: -17.9246\n",
      "    - New mask: 58 features, Fitness: -17.0438\n",
      "    - New mask: 58 features, Fitness: -17.5174\n",
      "    - New mask: 55 features, Fitness: -15.3397\n",
      "    - New mask: 58 features, Fitness: -17.1770\n",
      "    - New mask: 63 features, Fitness: -21.8378\n",
      "    - New mask: 60 features, Fitness: -17.9369\n",
      "    - New mask: 55 features, Fitness: -13.8951\n",
      "    - New mask: 60 features, Fitness: -18.6584\n",
      "    - New mask: 59 features, Fitness: -19.2017\n",
      "    - New mask: 60 features, Fitness: -20.3110\n",
      "    - New mask: 52 features, Fitness: -14.0652\n",
      "    - New mask: 63 features, Fitness: -21.9695\n",
      "    - New mask: 57 features, Fitness: -16.4532\n",
      "    - New mask: 58 features, Fitness: -18.9403\n",
      "    - New mask: 61 features, Fitness: -19.3455\n",
      "    - New mask: 62 features, Fitness: -21.4938\n",
      "    - New mask: 59 features, Fitness: -22.2292\n",
      "    - New mask: 56 features, Fitness: -20.9374\n",
      "    - New mask: 57 features, Fitness: -21.8940\n",
      "    - New mask: 57 features, Fitness: -23.1686\n",
      "    - New mask: 55 features, Fitness: -18.8043\n",
      "    - New mask: 60 features, Fitness: -26.6871\n",
      "    - New mask: 58 features, Fitness: -23.2999\n",
      "    - New mask: 62 features, Fitness: -27.2594\n",
      "    - New mask: 61 features, Fitness: -24.1614\n",
      "    - New mask: 56 features, Fitness: -20.9731\n",
      "    - New mask: 61 features, Fitness: -26.8853\n",
      "    - New mask: 59 features, Fitness: -23.5226\n",
      "    - New mask: 57 features, Fitness: -21.8822\n",
      "    - New mask: 58 features, Fitness: -23.4231\n",
      "    - New mask: 59 features, Fitness: -24.3753\n",
      "    - New mask: 54 features, Fitness: -19.3667\n",
      "    - New mask: 59 features, Fitness: -23.3607\n",
      "    - New mask: 57 features, Fitness: -23.0992\n",
      "    - New mask: 61 features, Fitness: -27.4492\n",
      "    - New mask: 56 features, Fitness: -21.8288\n",
      "    - New mask: 54 features, Fitness: -18.7427\n",
      "    - New mask: 60 features, Fitness: -24.6626\n",
      "    - New mask: 55 features, Fitness: -20.0692\n",
      "    - New mask: 55 features, Fitness: -19.9703\n",
      "    - New mask: 58 features, Fitness: -23.6435\n",
      "    - New mask: 60 features, Fitness: -25.9254\n",
      "    - New mask: 57 features, Fitness: -24.4823\n",
      "    - New mask: 57 features, Fitness: -24.9322\n",
      "    - New mask: 54 features, Fitness: -19.6987\n",
      "    - New mask: 54 features, Fitness: -20.8010\n",
      "    - New mask: 58 features, Fitness: -23.8340\n",
      "    - New mask: 59 features, Fitness: -25.5676\n",
      "    - New mask: 58 features, Fitness: -23.2357\n",
      "    - New mask: 57 features, Fitness: -23.8217\n",
      "    - New mask: 58 features, Fitness: -24.4993\n",
      "    - New mask: 60 features, Fitness: -24.3531\n",
      "    - New mask: 58 features, Fitness: -23.0716\n",
      "    - New mask: 59 features, Fitness: -28.6227\n",
      "    - New mask: 61 features, Fitness: -26.1092\n",
      "    - New mask: 59 features, Fitness: -26.0276\n",
      "    - New mask: 61 features, Fitness: -24.1048\n",
      "    - New mask: 57 features, Fitness: -18.7947\n",
      "    - New mask: 59 features, Fitness: -22.9162\n",
      "    - New mask: 59 features, Fitness: -21.5098\n",
      "    - New mask: 54 features, Fitness: -16.0499\n",
      "    - New mask: 61 features, Fitness: -25.7654\n",
      "    - New mask: 61 features, Fitness: -23.6425\n",
      "    - New mask: 57 features, Fitness: -19.3219\n",
      "    - New mask: 59 features, Fitness: -21.3608\n",
      "    - New mask: 55 features, Fitness: -19.1532\n",
      "    - New mask: 54 features, Fitness: -16.6542\n",
      "    - New mask: 61 features, Fitness: -25.2109\n",
      "    - New mask: 57 features, Fitness: -20.1496\n",
      "    - New mask: 58 features, Fitness: -21.6147\n",
      "    - New mask: 61 features, Fitness: -25.0910\n",
      "    - New mask: 58 features, Fitness: -21.1983\n",
      "    - New mask: 59 features, Fitness: -22.2798\n",
      "    - New mask: 60 features, Fitness: -24.0158\n",
      "    - New mask: 57 features, Fitness: -19.3969\n",
      "    - New mask: 61 features, Fitness: -24.1838\n",
      "=== End of Round 9: Vote mask selects 66 features (rho: 0.45)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 10 ================\n",
      "  Adaptive rho for this round: 0.48\n",
      "    - New mask: 57 features, Fitness: -20.5629\n",
      "    - New mask: 59 features, Fitness: -23.5974\n",
      "    - New mask: 57 features, Fitness: -20.4244\n",
      "    - New mask: 57 features, Fitness: -21.2613\n",
      "    - New mask: 58 features, Fitness: -21.4367\n",
      "    - New mask: 58 features, Fitness: -20.9322\n",
      "    - New mask: 57 features, Fitness: -18.5809\n",
      "    - New mask: 60 features, Fitness: -23.8889\n",
      "    - New mask: 63 features, Fitness: -27.0416\n",
      "    - New mask: 57 features, Fitness: -21.9643\n",
      "    - New mask: 55 features, Fitness: -18.4405\n",
      "    - New mask: 56 features, Fitness: -21.0868\n",
      "    - New mask: 57 features, Fitness: -18.8943\n",
      "    - New mask: 58 features, Fitness: -22.1974\n",
      "    - New mask: 58 features, Fitness: -23.3272\n",
      "    - New mask: 61 features, Fitness: -24.0407\n",
      "    - New mask: 60 features, Fitness: -21.2193\n",
      "    - New mask: 61 features, Fitness: -25.5124\n",
      "    - New mask: 59 features, Fitness: -21.3717\n",
      "    - New mask: 61 features, Fitness: -23.0787\n",
      "    - New mask: 60 features, Fitness: -22.0683\n",
      "    - New mask: 61 features, Fitness: -22.4568\n",
      "    - New mask: 60 features, Fitness: -24.0170\n",
      "    - New mask: 60 features, Fitness: -22.2206\n",
      "    - New mask: 62 features, Fitness: -24.9050\n",
      "    - New mask: 57 features, Fitness: -20.7400\n",
      "    - New mask: 55 features, Fitness: -18.8810\n",
      "    - New mask: 58 features, Fitness: -19.5793\n",
      "    - New mask: 56 features, Fitness: -19.6698\n",
      "    - New mask: 57 features, Fitness: -20.9175\n",
      "    - New mask: 63 features, Fitness: -24.7796\n",
      "    - New mask: 56 features, Fitness: -19.5638\n",
      "    - New mask: 63 features, Fitness: -24.5918\n",
      "    - New mask: 56 features, Fitness: -18.7175\n",
      "    - New mask: 55 features, Fitness: -18.3255\n",
      "    - New mask: 55 features, Fitness: -19.2745\n",
      "    - New mask: 56 features, Fitness: -17.9706\n",
      "    - New mask: 63 features, Fitness: -25.9528\n",
      "    - New mask: 58 features, Fitness: -20.0192\n",
      "    - New mask: 58 features, Fitness: -21.2597\n",
      "    - New mask: 59 features, Fitness: -20.1962\n",
      "    - New mask: 57 features, Fitness: -18.2761\n",
      "    - New mask: 55 features, Fitness: -14.9297\n",
      "    - New mask: 60 features, Fitness: -20.0212\n",
      "    - New mask: 58 features, Fitness: -17.5590\n",
      "    - New mask: 59 features, Fitness: -19.7911\n",
      "    - New mask: 60 features, Fitness: -19.3591\n",
      "    - New mask: 58 features, Fitness: -17.2130\n",
      "    - New mask: 62 features, Fitness: -21.3239\n",
      "    - New mask: 58 features, Fitness: -18.0985\n",
      "    - New mask: 56 features, Fitness: -16.1910\n",
      "    - New mask: 59 features, Fitness: -18.6287\n",
      "    - New mask: 55 features, Fitness: -16.8381\n",
      "    - New mask: 61 features, Fitness: -21.4424\n",
      "    - New mask: 55 features, Fitness: -16.5858\n",
      "    - New mask: 59 features, Fitness: -18.9966\n",
      "    - New mask: 60 features, Fitness: -20.5848\n",
      "    - New mask: 61 features, Fitness: -20.9805\n",
      "    - New mask: 58 features, Fitness: -18.9946\n",
      "    - New mask: 58 features, Fitness: -19.4787\n",
      "    - New mask: 58 features, Fitness: -17.3391\n",
      "    - New mask: 58 features, Fitness: -17.7388\n",
      "    - New mask: 55 features, Fitness: -16.9547\n",
      "    - New mask: 57 features, Fitness: -18.2310\n",
      "    - New mask: 59 features, Fitness: -19.8387\n",
      "    - New mask: 58 features, Fitness: -18.7026\n",
      "    - New mask: 58 features, Fitness: -18.6888\n",
      "    - New mask: 55 features, Fitness: -16.1498\n",
      "    - New mask: 57 features, Fitness: -16.7810\n",
      "    - New mask: 59 features, Fitness: -19.0086\n",
      "    - New mask: 55 features, Fitness: -15.4562\n",
      "    - New mask: 58 features, Fitness: -18.1859\n",
      "    - New mask: 58 features, Fitness: -19.2669\n",
      "    - New mask: 57 features, Fitness: -16.6554\n",
      "    - New mask: 61 features, Fitness: -20.9992\n",
      "    - New mask: 54 features, Fitness: -15.1849\n",
      "    - New mask: 58 features, Fitness: -18.4923\n",
      "    - New mask: 57 features, Fitness: -17.1483\n",
      "    - New mask: 57 features, Fitness: -17.3314\n",
      "    - New mask: 61 features, Fitness: -20.6539\n",
      "    - New mask: 60 features, Fitness: -16.9593\n",
      "    - New mask: 55 features, Fitness: -12.2923\n",
      "    - New mask: 59 features, Fitness: -15.9411\n",
      "    - New mask: 56 features, Fitness: -16.0147\n",
      "    - New mask: 51 features, Fitness: -11.3301\n",
      "    - New mask: 56 features, Fitness: -13.9302\n",
      "    - New mask: 53 features, Fitness: -13.3731\n",
      "    - New mask: 58 features, Fitness: -16.9677\n",
      "    - New mask: 60 features, Fitness: -18.3061\n",
      "    - New mask: 54 features, Fitness: -13.7237\n",
      "    - New mask: 57 features, Fitness: -15.9470\n",
      "    - New mask: 57 features, Fitness: -15.0624\n",
      "    - New mask: 58 features, Fitness: -15.3231\n",
      "    - New mask: 56 features, Fitness: -13.3469\n",
      "    - New mask: 59 features, Fitness: -17.7586\n",
      "    - New mask: 63 features, Fitness: -20.4879\n",
      "    - New mask: 53 features, Fitness: -12.4659\n",
      "    - New mask: 59 features, Fitness: -17.3531\n",
      "    - New mask: 61 features, Fitness: -19.5962\n",
      "    - New mask: 60 features, Fitness: -18.0985\n",
      "    - New mask: 61 features, Fitness: -24.4790\n",
      "    - New mask: 58 features, Fitness: -20.0689\n",
      "    - New mask: 61 features, Fitness: -22.7604\n",
      "    - New mask: 59 features, Fitness: -22.5363\n",
      "    - New mask: 54 features, Fitness: -17.7267\n",
      "    - New mask: 59 features, Fitness: -22.9682\n",
      "    - New mask: 60 features, Fitness: -21.6835\n",
      "    - New mask: 60 features, Fitness: -21.4981\n",
      "    - New mask: 59 features, Fitness: -19.9538\n",
      "    - New mask: 60 features, Fitness: -22.4095\n",
      "    - New mask: 59 features, Fitness: -20.5221\n",
      "    - New mask: 58 features, Fitness: -20.8751\n",
      "    - New mask: 60 features, Fitness: -22.1140\n",
      "    - New mask: 59 features, Fitness: -21.1281\n",
      "    - New mask: 62 features, Fitness: -24.7309\n",
      "    - New mask: 56 features, Fitness: -21.4128\n",
      "    - New mask: 59 features, Fitness: -19.3092\n",
      "    - New mask: 56 features, Fitness: -18.8121\n",
      "    - New mask: 63 features, Fitness: -26.0767\n",
      "    - New mask: 58 features, Fitness: -20.5951\n",
      "    - New mask: 58 features, Fitness: -18.9004\n",
      "    - New mask: 62 features, Fitness: -20.1606\n",
      "    - New mask: 60 features, Fitness: -18.4480\n",
      "    - New mask: 59 features, Fitness: -18.0426\n",
      "    - New mask: 62 features, Fitness: -21.5746\n",
      "    - New mask: 55 features, Fitness: -14.3080\n",
      "    - New mask: 61 features, Fitness: -20.5602\n",
      "    - New mask: 58 features, Fitness: -17.3356\n",
      "    - New mask: 61 features, Fitness: -19.3397\n",
      "    - New mask: 59 features, Fitness: -17.1062\n",
      "    - New mask: 52 features, Fitness: -12.0469\n",
      "    - New mask: 57 features, Fitness: -16.3935\n",
      "    - New mask: 60 features, Fitness: -19.0137\n",
      "    - New mask: 57 features, Fitness: -16.6654\n",
      "    - New mask: 57 features, Fitness: -16.7902\n",
      "    - New mask: 57 features, Fitness: -16.5127\n",
      "    - New mask: 58 features, Fitness: -16.8031\n",
      "    - New mask: 53 features, Fitness: -14.2479\n",
      "    - New mask: 61 features, Fitness: -18.8920\n",
      "    - New mask: 57 features, Fitness: -17.3417\n",
      "    - New mask: 57 features, Fitness: -20.3373\n",
      "    - New mask: 57 features, Fitness: -21.6149\n",
      "    - New mask: 58 features, Fitness: -23.4879\n",
      "    - New mask: 53 features, Fitness: -19.1960\n",
      "    - New mask: 60 features, Fitness: -25.4459\n",
      "    - New mask: 62 features, Fitness: -26.4073\n",
      "    - New mask: 59 features, Fitness: -23.9610\n",
      "    - New mask: 60 features, Fitness: -24.3854\n",
      "    - New mask: 58 features, Fitness: -21.7549\n",
      "    - New mask: 60 features, Fitness: -26.0520\n",
      "    - New mask: 58 features, Fitness: -23.8932\n",
      "    - New mask: 61 features, Fitness: -26.9419\n",
      "    - New mask: 57 features, Fitness: -21.3213\n",
      "    - New mask: 65 features, Fitness: -30.8096\n",
      "    - New mask: 59 features, Fitness: -23.7867\n",
      "    - New mask: 62 features, Fitness: -26.0979\n",
      "    - New mask: 57 features, Fitness: -20.6156\n",
      "    - New mask: 58 features, Fitness: -23.9565\n",
      "    - New mask: 53 features, Fitness: -19.5431\n",
      "    - New mask: 56 features, Fitness: -21.9513\n",
      "    - New mask: 57 features, Fitness: -22.7562\n",
      "    - New mask: 57 features, Fitness: -21.4216\n",
      "    - New mask: 60 features, Fitness: -25.7331\n",
      "    - New mask: 57 features, Fitness: -24.9340\n",
      "    - New mask: 63 features, Fitness: -28.3355\n",
      "    - New mask: 56 features, Fitness: -21.6726\n",
      "    - New mask: 59 features, Fitness: -26.3062\n",
      "    - New mask: 58 features, Fitness: -25.5490\n",
      "    - New mask: 55 features, Fitness: -22.2661\n",
      "    - New mask: 57 features, Fitness: -25.5677\n",
      "    - New mask: 59 features, Fitness: -23.9993\n",
      "    - New mask: 58 features, Fitness: -23.2044\n",
      "    - New mask: 60 features, Fitness: -24.3190\n",
      "    - New mask: 56 features, Fitness: -22.1246\n",
      "    - New mask: 60 features, Fitness: -27.8735\n",
      "    - New mask: 55 features, Fitness: -20.9003\n",
      "    - New mask: 59 features, Fitness: -25.4623\n",
      "    - New mask: 62 features, Fitness: -29.8728\n",
      "    - New mask: 59 features, Fitness: -22.7213\n",
      "    - New mask: 58 features, Fitness: -22.5445\n",
      "    - New mask: 54 features, Fitness: -18.6468\n",
      "    - New mask: 58 features, Fitness: -19.2339\n",
      "    - New mask: 61 features, Fitness: -24.5333\n",
      "    - New mask: 59 features, Fitness: -21.0450\n",
      "    - New mask: 55 features, Fitness: -17.2835\n",
      "    - New mask: 57 features, Fitness: -20.3332\n",
      "    - New mask: 61 features, Fitness: -22.4130\n",
      "    - New mask: 61 features, Fitness: -24.0686\n",
      "    - New mask: 56 features, Fitness: -19.5193\n",
      "    - New mask: 58 features, Fitness: -20.7696\n",
      "    - New mask: 54 features, Fitness: -16.3266\n",
      "    - New mask: 57 features, Fitness: -20.0403\n",
      "    - New mask: 53 features, Fitness: -15.2386\n",
      "    - New mask: 60 features, Fitness: -22.7375\n",
      "    - New mask: 62 features, Fitness: -24.1560\n",
      "    - New mask: 58 features, Fitness: -21.4189\n",
      "    - New mask: 57 features, Fitness: -20.2822\n",
      "    - New mask: 58 features, Fitness: -20.1209\n",
      "    - New mask: 62 features, Fitness: -24.7673\n",
      "    - New mask: 59 features, Fitness: -19.9363\n",
      "=== End of Round 10: Vote mask selects 65 features (rho: 0.48)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 11 ================\n",
      "  Adaptive rho for this round: 0.52\n",
      "    - New mask: 54 features, Fitness: -18.5556\n",
      "    - New mask: 61 features, Fitness: -22.9098\n",
      "    - New mask: 55 features, Fitness: -18.2276\n",
      "    - New mask: 55 features, Fitness: -19.5929\n",
      "    - New mask: 59 features, Fitness: -22.3936\n",
      "    - New mask: 59 features, Fitness: -20.7747\n",
      "    - New mask: 59 features, Fitness: -21.8258\n",
      "    - New mask: 59 features, Fitness: -20.4274\n",
      "    - New mask: 63 features, Fitness: -23.8751\n",
      "    - New mask: 60 features, Fitness: -23.5281\n",
      "    - New mask: 57 features, Fitness: -18.4045\n",
      "    - New mask: 59 features, Fitness: -24.6597\n",
      "    - New mask: 57 features, Fitness: -21.0239\n",
      "    - New mask: 57 features, Fitness: -21.3886\n",
      "    - New mask: 58 features, Fitness: -20.8025\n",
      "    - New mask: 59 features, Fitness: -21.1079\n",
      "    - New mask: 57 features, Fitness: -19.1449\n",
      "    - New mask: 61 features, Fitness: -24.0631\n",
      "    - New mask: 60 features, Fitness: -21.6723\n",
      "    - New mask: 60 features, Fitness: -20.7570\n",
      "    - New mask: 62 features, Fitness: -23.5447\n",
      "    - New mask: 62 features, Fitness: -23.6773\n",
      "    - New mask: 61 features, Fitness: -23.3854\n",
      "    - New mask: 57 features, Fitness: -19.5228\n",
      "    - New mask: 56 features, Fitness: -17.9316\n",
      "    - New mask: 60 features, Fitness: -22.3483\n",
      "    - New mask: 58 features, Fitness: -20.9772\n",
      "    - New mask: 55 features, Fitness: -17.2389\n",
      "    - New mask: 57 features, Fitness: -18.7500\n",
      "    - New mask: 58 features, Fitness: -19.5188\n",
      "    - New mask: 59 features, Fitness: -21.8868\n",
      "    - New mask: 59 features, Fitness: -20.2431\n",
      "    - New mask: 61 features, Fitness: -23.1305\n",
      "    - New mask: 57 features, Fitness: -18.2616\n",
      "    - New mask: 59 features, Fitness: -21.3369\n",
      "    - New mask: 60 features, Fitness: -21.9902\n",
      "    - New mask: 62 features, Fitness: -23.9847\n",
      "    - New mask: 57 features, Fitness: -19.5252\n",
      "    - New mask: 58 features, Fitness: -22.4862\n",
      "    - New mask: 62 features, Fitness: -24.5175\n",
      "    - New mask: 58 features, Fitness: -19.0578\n",
      "    - New mask: 55 features, Fitness: -15.9752\n",
      "    - New mask: 58 features, Fitness: -17.4260\n",
      "    - New mask: 60 features, Fitness: -19.1976\n",
      "    - New mask: 58 features, Fitness: -18.0075\n",
      "    - New mask: 57 features, Fitness: -16.9291\n",
      "    - New mask: 57 features, Fitness: -15.6275\n",
      "    - New mask: 56 features, Fitness: -15.6895\n",
      "    - New mask: 59 features, Fitness: -18.7155\n",
      "    - New mask: 55 features, Fitness: -16.7942\n",
      "    - New mask: 59 features, Fitness: -18.2489\n",
      "    - New mask: 58 features, Fitness: -18.1897\n",
      "    - New mask: 58 features, Fitness: -19.0106\n",
      "    - New mask: 57 features, Fitness: -17.4328\n",
      "    - New mask: 57 features, Fitness: -17.3753\n",
      "    - New mask: 58 features, Fitness: -17.1625\n",
      "    - New mask: 60 features, Fitness: -18.9237\n",
      "    - New mask: 59 features, Fitness: -19.0225\n",
      "    - New mask: 57 features, Fitness: -18.5098\n",
      "    - New mask: 57 features, Fitness: -18.1314\n",
      "    - New mask: 55 features, Fitness: -15.8887\n",
      "    - New mask: 61 features, Fitness: -20.5955\n",
      "    - New mask: 58 features, Fitness: -19.4781\n",
      "    - New mask: 58 features, Fitness: -18.0497\n",
      "    - New mask: 59 features, Fitness: -18.6512\n",
      "    - New mask: 61 features, Fitness: -20.9123\n",
      "    - New mask: 55 features, Fitness: -16.9894\n",
      "    - New mask: 54 features, Fitness: -15.3488\n",
      "    - New mask: 56 features, Fitness: -16.3020\n",
      "    - New mask: 61 features, Fitness: -20.0028\n",
      "    - New mask: 59 features, Fitness: -18.4286\n",
      "    - New mask: 56 features, Fitness: -16.5642\n",
      "    - New mask: 57 features, Fitness: -16.9975\n",
      "    - New mask: 60 features, Fitness: -20.3082\n",
      "    - New mask: 57 features, Fitness: -17.7265\n",
      "    - New mask: 59 features, Fitness: -18.5554\n",
      "    - New mask: 58 features, Fitness: -17.4569\n",
      "    - New mask: 58 features, Fitness: -18.1071\n",
      "    - New mask: 58 features, Fitness: -16.6612\n",
      "    - New mask: 60 features, Fitness: -19.6099\n",
      "    - New mask: 55 features, Fitness: -13.5340\n",
      "    - New mask: 55 features, Fitness: -13.2213\n",
      "    - New mask: 58 features, Fitness: -16.0829\n",
      "    - New mask: 56 features, Fitness: -16.3636\n",
      "    - New mask: 59 features, Fitness: -15.0782\n",
      "    - New mask: 58 features, Fitness: -17.6654\n",
      "    - New mask: 56 features, Fitness: -13.8192\n",
      "    - New mask: 55 features, Fitness: -15.7544\n",
      "    - New mask: 56 features, Fitness: -15.5275\n",
      "    - New mask: 54 features, Fitness: -12.3791\n",
      "    - New mask: 53 features, Fitness: -13.4534\n",
      "    - New mask: 56 features, Fitness: -14.9851\n",
      "    - New mask: 56 features, Fitness: -14.1392\n",
      "    - New mask: 55 features, Fitness: -13.7889\n",
      "    - New mask: 57 features, Fitness: -15.9754\n",
      "    - New mask: 59 features, Fitness: -16.5440\n",
      "    - New mask: 54 features, Fitness: -12.5081\n",
      "    - New mask: 57 features, Fitness: -16.2619\n",
      "    - New mask: 59 features, Fitness: -17.0203\n",
      "    - New mask: 58 features, Fitness: -15.3289\n",
      "    - New mask: 57 features, Fitness: -19.6549\n",
      "    - New mask: 58 features, Fitness: -20.9162\n",
      "    - New mask: 58 features, Fitness: -20.7037\n",
      "    - New mask: 57 features, Fitness: -20.9777\n",
      "    - New mask: 58 features, Fitness: -21.3894\n",
      "    - New mask: 56 features, Fitness: -19.0360\n",
      "    - New mask: 55 features, Fitness: -19.8615\n",
      "    - New mask: 58 features, Fitness: -19.5990\n",
      "    - New mask: 58 features, Fitness: -18.6821\n",
      "    - New mask: 58 features, Fitness: -20.2082\n",
      "    - New mask: 54 features, Fitness: -17.4305\n",
      "    - New mask: 58 features, Fitness: -20.3154\n",
      "    - New mask: 55 features, Fitness: -17.1271\n",
      "    - New mask: 57 features, Fitness: -21.1880\n",
      "    - New mask: 61 features, Fitness: -24.5088\n",
      "    - New mask: 59 features, Fitness: -22.6695\n",
      "    - New mask: 53 features, Fitness: -16.8013\n",
      "    - New mask: 58 features, Fitness: -20.6874\n",
      "    - New mask: 61 features, Fitness: -24.4582\n",
      "    - New mask: 60 features, Fitness: -22.5553\n",
      "    - New mask: 58 features, Fitness: -17.7449\n",
      "    - New mask: 59 features, Fitness: -16.6411\n",
      "    - New mask: 60 features, Fitness: -18.6276\n",
      "    - New mask: 57 features, Fitness: -16.6017\n",
      "    - New mask: 60 features, Fitness: -18.2349\n",
      "    - New mask: 56 features, Fitness: -15.6406\n",
      "    - New mask: 58 features, Fitness: -16.9998\n",
      "    - New mask: 54 features, Fitness: -14.2332\n",
      "    - New mask: 57 features, Fitness: -15.8325\n",
      "    - New mask: 60 features, Fitness: -17.9443\n",
      "    - New mask: 59 features, Fitness: -16.9428\n",
      "    - New mask: 51 features, Fitness: -12.5760\n",
      "    - New mask: 58 features, Fitness: -17.0833\n",
      "    - New mask: 55 features, Fitness: -14.4429\n",
      "    - New mask: 61 features, Fitness: -18.9464\n",
      "    - New mask: 57 features, Fitness: -16.7494\n",
      "    - New mask: 58 features, Fitness: -16.7589\n",
      "    - New mask: 51 features, Fitness: -13.7613\n",
      "    - New mask: 56 features, Fitness: -14.1501\n",
      "    - New mask: 55 features, Fitness: -13.6338\n",
      "    - New mask: 57 features, Fitness: -21.2820\n",
      "    - New mask: 59 features, Fitness: -24.3206\n",
      "    - New mask: 58 features, Fitness: -23.3756\n",
      "    - New mask: 53 features, Fitness: -18.1755\n",
      "    - New mask: 58 features, Fitness: -22.8868\n",
      "    - New mask: 59 features, Fitness: -24.4121\n",
      "    - New mask: 57 features, Fitness: -20.7914\n",
      "    - New mask: 60 features, Fitness: -25.8280\n",
      "    - New mask: 60 features, Fitness: -24.4187\n",
      "    - New mask: 56 features, Fitness: -21.6187\n",
      "    - New mask: 59 features, Fitness: -24.5518\n",
      "    - New mask: 56 features, Fitness: -22.2804\n",
      "    - New mask: 64 features, Fitness: -28.3896\n",
      "    - New mask: 62 features, Fitness: -27.4883\n",
      "    - New mask: 58 features, Fitness: -22.0646\n",
      "    - New mask: 58 features, Fitness: -22.0300\n",
      "    - New mask: 54 features, Fitness: -19.6192\n",
      "    - New mask: 55 features, Fitness: -20.1001\n",
      "    - New mask: 55 features, Fitness: -20.4266\n",
      "    - New mask: 57 features, Fitness: -23.0317\n",
      "    - New mask: 58 features, Fitness: -24.6651\n",
      "    - New mask: 61 features, Fitness: -26.9668\n",
      "    - New mask: 60 features, Fitness: -26.0574\n",
      "    - New mask: 60 features, Fitness: -27.6979\n",
      "    - New mask: 62 features, Fitness: -28.5971\n",
      "    - New mask: 59 features, Fitness: -26.8716\n",
      "    - New mask: 61 features, Fitness: -25.8329\n",
      "    - New mask: 62 features, Fitness: -28.3331\n",
      "    - New mask: 53 features, Fitness: -18.8703\n",
      "    - New mask: 61 features, Fitness: -27.5042\n",
      "    - New mask: 59 features, Fitness: -24.3289\n",
      "    - New mask: 61 features, Fitness: -26.2888\n",
      "    - New mask: 57 features, Fitness: -21.4687\n",
      "    - New mask: 58 features, Fitness: -23.8655\n",
      "    - New mask: 60 features, Fitness: -27.9250\n",
      "    - New mask: 58 features, Fitness: -24.3976\n",
      "    - New mask: 59 features, Fitness: -25.7787\n",
      "    - New mask: 59 features, Fitness: -26.3849\n",
      "    - New mask: 61 features, Fitness: -25.0041\n",
      "    - New mask: 57 features, Fitness: -22.5318\n",
      "    - New mask: 54 features, Fitness: -17.2813\n",
      "    - New mask: 55 features, Fitness: -16.4141\n",
      "    - New mask: 55 features, Fitness: -17.0931\n",
      "    - New mask: 56 features, Fitness: -18.8002\n",
      "    - New mask: 58 features, Fitness: -21.7376\n",
      "    - New mask: 59 features, Fitness: -21.0648\n",
      "    - New mask: 55 features, Fitness: -16.5133\n",
      "    - New mask: 58 features, Fitness: -20.7396\n",
      "    - New mask: 56 features, Fitness: -19.1396\n",
      "    - New mask: 54 features, Fitness: -16.8546\n",
      "    - New mask: 55 features, Fitness: -18.3095\n",
      "    - New mask: 56 features, Fitness: -17.6595\n",
      "    - New mask: 56 features, Fitness: -18.8566\n",
      "    - New mask: 57 features, Fitness: -20.1313\n",
      "    - New mask: 56 features, Fitness: -17.2723\n",
      "    - New mask: 57 features, Fitness: -19.2682\n",
      "    - New mask: 61 features, Fitness: -24.6841\n",
      "    - New mask: 59 features, Fitness: -20.6806\n",
      "    - New mask: 55 features, Fitness: -17.6537\n",
      "    - New mask: 58 features, Fitness: -19.6951\n",
      "=== End of Round 11: Vote mask selects 64 features (rho: 0.52)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 12 ================\n",
      "  Adaptive rho for this round: 0.55\n",
      "    - New mask: 56 features, Fitness: -18.4337\n",
      "    - New mask: 60 features, Fitness: -20.9737\n",
      "    - New mask: 56 features, Fitness: -19.2192\n",
      "    - New mask: 59 features, Fitness: -22.8288\n",
      "    - New mask: 55 features, Fitness: -18.0083\n",
      "    - New mask: 62 features, Fitness: -23.4696\n",
      "    - New mask: 59 features, Fitness: -21.9164\n",
      "    - New mask: 59 features, Fitness: -20.9311\n",
      "    - New mask: 58 features, Fitness: -20.5102\n",
      "    - New mask: 60 features, Fitness: -21.8079\n",
      "    - New mask: 58 features, Fitness: -19.9908\n",
      "    - New mask: 57 features, Fitness: -22.3906\n",
      "    - New mask: 57 features, Fitness: -21.0345\n",
      "    - New mask: 59 features, Fitness: -20.6520\n",
      "    - New mask: 58 features, Fitness: -20.7539\n",
      "    - New mask: 55 features, Fitness: -18.1091\n",
      "    - New mask: 58 features, Fitness: -21.4327\n",
      "    - New mask: 56 features, Fitness: -19.4317\n",
      "    - New mask: 57 features, Fitness: -19.4886\n",
      "    - New mask: 60 features, Fitness: -21.5502\n",
      "    - New mask: 59 features, Fitness: -20.1540\n",
      "    - New mask: 55 features, Fitness: -16.1657\n",
      "    - New mask: 59 features, Fitness: -21.1232\n",
      "    - New mask: 62 features, Fitness: -24.9905\n",
      "    - New mask: 56 features, Fitness: -17.5723\n",
      "    - New mask: 60 features, Fitness: -21.1493\n",
      "    - New mask: 57 features, Fitness: -20.3106\n",
      "    - New mask: 55 features, Fitness: -17.2389\n",
      "    - New mask: 58 features, Fitness: -21.1218\n",
      "    - New mask: 56 features, Fitness: -18.0432\n",
      "    - New mask: 59 features, Fitness: -21.9097\n",
      "    - New mask: 57 features, Fitness: -19.6542\n",
      "    - New mask: 56 features, Fitness: -18.4455\n",
      "    - New mask: 57 features, Fitness: -18.1542\n",
      "    - New mask: 54 features, Fitness: -16.4074\n",
      "    - New mask: 59 features, Fitness: -20.7625\n",
      "    - New mask: 59 features, Fitness: -21.0607\n",
      "    - New mask: 50 features, Fitness: -15.2900\n",
      "    - New mask: 60 features, Fitness: -22.1533\n",
      "    - New mask: 61 features, Fitness: -23.2472\n",
      "    - New mask: 56 features, Fitness: -16.0910\n",
      "    - New mask: 54 features, Fitness: -14.1848\n",
      "    - New mask: 60 features, Fitness: -18.9720\n",
      "    - New mask: 58 features, Fitness: -16.8601\n",
      "    - New mask: 60 features, Fitness: -20.0851\n",
      "    - New mask: 58 features, Fitness: -17.5057\n",
      "    - New mask: 58 features, Fitness: -16.7364\n",
      "    - New mask: 59 features, Fitness: -17.8728\n",
      "    - New mask: 56 features, Fitness: -15.3043\n",
      "    - New mask: 60 features, Fitness: -19.9080\n",
      "    - New mask: 61 features, Fitness: -20.0181\n",
      "    - New mask: 59 features, Fitness: -18.5011\n",
      "    - New mask: 58 features, Fitness: -18.4107\n",
      "    - New mask: 56 features, Fitness: -15.7326\n",
      "    - New mask: 58 features, Fitness: -18.3076\n",
      "    - New mask: 59 features, Fitness: -17.7633\n",
      "    - New mask: 58 features, Fitness: -17.9600\n",
      "    - New mask: 58 features, Fitness: -17.3608\n",
      "    - New mask: 58 features, Fitness: -20.1926\n",
      "    - New mask: 60 features, Fitness: -18.2515\n",
      "    - New mask: 57 features, Fitness: -17.3387\n",
      "    - New mask: 62 features, Fitness: -21.3999\n",
      "    - New mask: 58 features, Fitness: -18.4754\n",
      "    - New mask: 59 features, Fitness: -19.4726\n",
      "    - New mask: 61 features, Fitness: -20.2743\n",
      "    - New mask: 58 features, Fitness: -17.4344\n",
      "    - New mask: 56 features, Fitness: -16.4546\n",
      "    - New mask: 57 features, Fitness: -17.6507\n",
      "    - New mask: 58 features, Fitness: -17.4896\n",
      "    - New mask: 61 features, Fitness: -20.4849\n",
      "    - New mask: 58 features, Fitness: -18.7278\n",
      "    - New mask: 57 features, Fitness: -17.7444\n",
      "    - New mask: 53 features, Fitness: -14.1300\n",
      "    - New mask: 60 features, Fitness: -20.7091\n",
      "    - New mask: 58 features, Fitness: -18.9247\n",
      "    - New mask: 60 features, Fitness: -20.2388\n",
      "    - New mask: 54 features, Fitness: -14.3911\n",
      "    - New mask: 56 features, Fitness: -16.8111\n",
      "    - New mask: 61 features, Fitness: -20.0215\n",
      "    - New mask: 59 features, Fitness: -18.2994\n",
      "    - New mask: 55 features, Fitness: -13.0364\n",
      "    - New mask: 57 features, Fitness: -14.6935\n",
      "    - New mask: 60 features, Fitness: -18.5074\n",
      "    - New mask: 57 features, Fitness: -18.0107\n",
      "    - New mask: 57 features, Fitness: -14.7251\n",
      "    - New mask: 53 features, Fitness: -12.7660\n",
      "    - New mask: 60 features, Fitness: -16.9153\n",
      "    - New mask: 56 features, Fitness: -14.3727\n",
      "    - New mask: 57 features, Fitness: -15.3167\n",
      "    - New mask: 56 features, Fitness: -14.5273\n",
      "    - New mask: 56 features, Fitness: -16.2914\n",
      "    - New mask: 57 features, Fitness: -16.3059\n",
      "    - New mask: 58 features, Fitness: -15.3351\n",
      "    - New mask: 54 features, Fitness: -13.0935\n",
      "    - New mask: 56 features, Fitness: -14.4108\n",
      "    - New mask: 54 features, Fitness: -11.6895\n",
      "    - New mask: 52 features, Fitness: -10.7338\n",
      "    - New mask: 57 features, Fitness: -16.4157\n",
      "    - New mask: 59 features, Fitness: -15.9966\n",
      "    - New mask: 57 features, Fitness: -15.1906\n",
      "    - New mask: 58 features, Fitness: -20.8579\n",
      "    - New mask: 55 features, Fitness: -18.5822\n",
      "    - New mask: 55 features, Fitness: -19.5198\n",
      "    - New mask: 58 features, Fitness: -21.0139\n",
      "    - New mask: 64 features, Fitness: -26.6932\n",
      "    - New mask: 60 features, Fitness: -21.9733\n",
      "    - New mask: 56 features, Fitness: -20.5165\n",
      "    - New mask: 58 features, Fitness: -21.0764\n",
      "    - New mask: 57 features, Fitness: -18.7352\n",
      "    - New mask: 59 features, Fitness: -19.5501\n",
      "    - New mask: 54 features, Fitness: -16.5186\n",
      "    - New mask: 59 features, Fitness: -20.6450\n",
      "    - New mask: 58 features, Fitness: -21.7990\n",
      "    - New mask: 61 features, Fitness: -23.7058\n",
      "    - New mask: 60 features, Fitness: -23.3728\n",
      "    - New mask: 62 features, Fitness: -23.9557\n",
      "    - New mask: 58 features, Fitness: -20.5633\n",
      "    - New mask: 58 features, Fitness: -20.3002\n",
      "    - New mask: 59 features, Fitness: -21.7643\n",
      "    - New mask: 58 features, Fitness: -19.9941\n",
      "    - New mask: 58 features, Fitness: -19.1041\n",
      "    - New mask: 56 features, Fitness: -14.8484\n",
      "    - New mask: 58 features, Fitness: -17.7904\n",
      "    - New mask: 57 features, Fitness: -16.8510\n",
      "    - New mask: 61 features, Fitness: -20.2233\n",
      "    - New mask: 57 features, Fitness: -16.9408\n",
      "    - New mask: 55 features, Fitness: -16.0222\n",
      "    - New mask: 54 features, Fitness: -14.3866\n",
      "    - New mask: 58 features, Fitness: -17.7293\n",
      "    - New mask: 58 features, Fitness: -16.6586\n",
      "    - New mask: 60 features, Fitness: -17.8924\n",
      "    - New mask: 58 features, Fitness: -17.8541\n",
      "    - New mask: 59 features, Fitness: -18.3516\n",
      "    - New mask: 55 features, Fitness: -15.5189\n",
      "    - New mask: 55 features, Fitness: -14.3714\n",
      "    - New mask: 57 features, Fitness: -17.9033\n",
      "    - New mask: 56 features, Fitness: -16.2247\n",
      "    - New mask: 57 features, Fitness: -16.0229\n",
      "    - New mask: 57 features, Fitness: -15.6645\n",
      "    - New mask: 60 features, Fitness: -17.9168\n",
      "    - New mask: 58 features, Fitness: -23.2399\n",
      "    - New mask: 58 features, Fitness: -23.0022\n",
      "    - New mask: 56 features, Fitness: -21.2923\n",
      "    - New mask: 52 features, Fitness: -16.7206\n",
      "    - New mask: 58 features, Fitness: -22.8704\n",
      "    - New mask: 60 features, Fitness: -26.2982\n",
      "    - New mask: 57 features, Fitness: -20.7982\n",
      "    - New mask: 57 features, Fitness: -22.9045\n",
      "    - New mask: 58 features, Fitness: -22.1509\n",
      "    - New mask: 56 features, Fitness: -21.8977\n",
      "    - New mask: 61 features, Fitness: -27.1310\n",
      "    - New mask: 54 features, Fitness: -19.2200\n",
      "    - New mask: 57 features, Fitness: -22.4413\n",
      "    - New mask: 56 features, Fitness: -22.4795\n",
      "    - New mask: 63 features, Fitness: -28.3463\n",
      "    - New mask: 54 features, Fitness: -18.0678\n",
      "    - New mask: 54 features, Fitness: -19.5903\n",
      "    - New mask: 57 features, Fitness: -21.6747\n",
      "    - New mask: 55 features, Fitness: -20.2022\n",
      "    - New mask: 58 features, Fitness: -23.0051\n",
      "    - New mask: 58 features, Fitness: -22.2890\n",
      "    - New mask: 58 features, Fitness: -23.7175\n",
      "    - New mask: 57 features, Fitness: -23.5976\n",
      "    - New mask: 60 features, Fitness: -24.8435\n",
      "    - New mask: 59 features, Fitness: -26.2097\n",
      "    - New mask: 60 features, Fitness: -25.2815\n",
      "    - New mask: 59 features, Fitness: -24.0999\n",
      "    - New mask: 60 features, Fitness: -25.3853\n",
      "    - New mask: 59 features, Fitness: -25.0452\n",
      "    - New mask: 61 features, Fitness: -27.1733\n",
      "    - New mask: 58 features, Fitness: -23.0403\n",
      "    - New mask: 52 features, Fitness: -20.2116\n",
      "    - New mask: 62 features, Fitness: -27.5965\n",
      "    - New mask: 53 features, Fitness: -20.4275\n",
      "    - New mask: 59 features, Fitness: -24.8690\n",
      "    - New mask: 58 features, Fitness: -23.2456\n",
      "    - New mask: 58 features, Fitness: -23.8295\n",
      "    - New mask: 53 features, Fitness: -18.9087\n",
      "    - New mask: 59 features, Fitness: -23.7560\n",
      "    - New mask: 57 features, Fitness: -23.2798\n",
      "    - New mask: 58 features, Fitness: -21.3007\n",
      "    - New mask: 58 features, Fitness: -19.0778\n",
      "    - New mask: 59 features, Fitness: -20.9062\n",
      "    - New mask: 55 features, Fitness: -18.2706\n",
      "    - New mask: 57 features, Fitness: -19.2668\n",
      "    - New mask: 57 features, Fitness: -19.0892\n",
      "    - New mask: 57 features, Fitness: -18.4953\n",
      "    - New mask: 58 features, Fitness: -20.4431\n",
      "    - New mask: 55 features, Fitness: -19.6354\n",
      "    - New mask: 56 features, Fitness: -20.1711\n",
      "    - New mask: 64 features, Fitness: -26.1782\n",
      "    - New mask: 56 features, Fitness: -19.3474\n",
      "    - New mask: 56 features, Fitness: -19.5743\n",
      "    - New mask: 57 features, Fitness: -19.2755\n",
      "    - New mask: 58 features, Fitness: -19.6762\n",
      "    - New mask: 58 features, Fitness: -19.8439\n",
      "    - New mask: 59 features, Fitness: -21.8498\n",
      "    - New mask: 59 features, Fitness: -20.9447\n",
      "    - New mask: 57 features, Fitness: -22.1463\n",
      "    - New mask: 54 features, Fitness: -17.1208\n",
      "=== End of Round 12: Vote mask selects 60 features (rho: 0.55)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 25, 26, 27, 29, 30, 32, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 13 ================\n",
      "  Adaptive rho for this round: 0.58\n",
      "    - New mask: 51 features, Fitness: -16.4106\n",
      "    - New mask: 60 features, Fitness: -21.7444\n",
      "    - New mask: 57 features, Fitness: -18.1590\n",
      "    - New mask: 59 features, Fitness: -20.8720\n",
      "    - New mask: 55 features, Fitness: -16.5738\n",
      "    - New mask: 57 features, Fitness: -22.1728\n",
      "    - New mask: 58 features, Fitness: -19.4469\n",
      "    - New mask: 57 features, Fitness: -18.7753\n",
      "    - New mask: 55 features, Fitness: -18.5837\n",
      "    - New mask: 55 features, Fitness: -18.4296\n",
      "    - New mask: 58 features, Fitness: -20.2111\n",
      "    - New mask: 54 features, Fitness: -19.2455\n",
      "    - New mask: 55 features, Fitness: -18.8635\n",
      "    - New mask: 60 features, Fitness: -21.7887\n",
      "    - New mask: 55 features, Fitness: -17.0200\n",
      "    - New mask: 56 features, Fitness: -17.0881\n",
      "    - New mask: 53 features, Fitness: -17.1042\n",
      "    - New mask: 60 features, Fitness: -22.1196\n",
      "    - New mask: 55 features, Fitness: -18.6638\n",
      "    - New mask: 54 features, Fitness: -15.4800\n",
      "    - New mask: 54 features, Fitness: -17.2060\n",
      "    - New mask: 55 features, Fitness: -17.2754\n",
      "    - New mask: 56 features, Fitness: -20.1701\n",
      "    - New mask: 59 features, Fitness: -19.6732\n",
      "    - New mask: 56 features, Fitness: -17.9526\n",
      "    - New mask: 55 features, Fitness: -19.5457\n",
      "    - New mask: 55 features, Fitness: -17.2726\n",
      "    - New mask: 49 features, Fitness: -13.1090\n",
      "    - New mask: 55 features, Fitness: -17.6875\n",
      "    - New mask: 57 features, Fitness: -19.5210\n",
      "    - New mask: 53 features, Fitness: -16.2831\n",
      "    - New mask: 54 features, Fitness: -16.3459\n",
      "    - New mask: 51 features, Fitness: -15.2544\n",
      "    - New mask: 56 features, Fitness: -18.6933\n",
      "    - New mask: 51 features, Fitness: -13.3972\n",
      "    - New mask: 53 features, Fitness: -16.3607\n",
      "    - New mask: 56 features, Fitness: -19.6378\n",
      "    - New mask: 51 features, Fitness: -15.2229\n",
      "    - New mask: 52 features, Fitness: -15.6309\n",
      "    - New mask: 57 features, Fitness: -19.7294\n",
      "    - New mask: 58 features, Fitness: -16.7129\n",
      "    - New mask: 55 features, Fitness: -14.7840\n",
      "    - New mask: 59 features, Fitness: -19.1318\n",
      "    - New mask: 55 features, Fitness: -14.3739\n",
      "    - New mask: 59 features, Fitness: -19.0202\n",
      "    - New mask: 60 features, Fitness: -19.5607\n",
      "    - New mask: 59 features, Fitness: -19.4384\n",
      "    - New mask: 58 features, Fitness: -17.3336\n",
      "    - New mask: 52 features, Fitness: -11.6387\n",
      "    - New mask: 56 features, Fitness: -15.8593\n",
      "    - New mask: 59 features, Fitness: -19.2174\n",
      "    - New mask: 58 features, Fitness: -17.6570\n",
      "    - New mask: 57 features, Fitness: -17.0276\n",
      "    - New mask: 57 features, Fitness: -16.3267\n",
      "    - New mask: 57 features, Fitness: -17.6425\n",
      "    - New mask: 56 features, Fitness: -16.2201\n",
      "    - New mask: 55 features, Fitness: -15.8514\n",
      "    - New mask: 54 features, Fitness: -13.4871\n",
      "    - New mask: 55 features, Fitness: -16.7781\n",
      "    - New mask: 54 features, Fitness: -14.6795\n",
      "    - New mask: 60 features, Fitness: -19.9212\n",
      "    - New mask: 59 features, Fitness: -19.1141\n",
      "    - New mask: 58 features, Fitness: -17.9271\n",
      "    - New mask: 60 features, Fitness: -19.7023\n",
      "    - New mask: 58 features, Fitness: -17.7463\n",
      "    - New mask: 54 features, Fitness: -14.0643\n",
      "    - New mask: 57 features, Fitness: -18.2141\n",
      "    - New mask: 54 features, Fitness: -15.3346\n",
      "    - New mask: 57 features, Fitness: -17.5327\n",
      "    - New mask: 55 features, Fitness: -15.8146\n",
      "    - New mask: 56 features, Fitness: -17.4977\n",
      "    - New mask: 57 features, Fitness: -17.0499\n",
      "    - New mask: 56 features, Fitness: -16.5316\n",
      "    - New mask: 55 features, Fitness: -15.7913\n",
      "    - New mask: 54 features, Fitness: -15.0222\n",
      "    - New mask: 56 features, Fitness: -16.1982\n",
      "    - New mask: 55 features, Fitness: -15.1255\n",
      "    - New mask: 56 features, Fitness: -16.6894\n",
      "    - New mask: 57 features, Fitness: -16.6028\n",
      "    - New mask: 58 features, Fitness: -17.3787\n",
      "    - New mask: 57 features, Fitness: -15.4652\n",
      "    - New mask: 55 features, Fitness: -12.5489\n",
      "    - New mask: 57 features, Fitness: -14.8016\n",
      "    - New mask: 55 features, Fitness: -14.3065\n",
      "    - New mask: 52 features, Fitness: -13.1752\n",
      "    - New mask: 55 features, Fitness: -13.0354\n",
      "    - New mask: 55 features, Fitness: -11.8731\n",
      "    - New mask: 58 features, Fitness: -15.3443\n",
      "    - New mask: 60 features, Fitness: -16.2096\n",
      "    - New mask: 58 features, Fitness: -15.3605\n",
      "    - New mask: 56 features, Fitness: -13.5898\n",
      "    - New mask: 59 features, Fitness: -17.0651\n",
      "    - New mask: 56 features, Fitness: -12.8723\n",
      "    - New mask: 60 features, Fitness: -18.0131\n",
      "    - New mask: 53 features, Fitness: -12.8405\n",
      "    - New mask: 56 features, Fitness: -13.4791\n",
      "    - New mask: 55 features, Fitness: -13.1370\n",
      "    - New mask: 54 features, Fitness: -12.3161\n",
      "    - New mask: 56 features, Fitness: -13.3673\n",
      "    - New mask: 53 features, Fitness: -12.4582\n",
      "    - New mask: 51 features, Fitness: -16.3097\n",
      "    - New mask: 53 features, Fitness: -17.0366\n",
      "    - New mask: 55 features, Fitness: -18.1627\n",
      "    - New mask: 56 features, Fitness: -18.8714\n",
      "    - New mask: 61 features, Fitness: -22.2070\n",
      "    - New mask: 59 features, Fitness: -21.0405\n",
      "    - New mask: 57 features, Fitness: -22.1606\n",
      "    - New mask: 57 features, Fitness: -18.3437\n",
      "    - New mask: 58 features, Fitness: -20.7841\n",
      "    - New mask: 60 features, Fitness: -20.9521\n",
      "    - New mask: 55 features, Fitness: -17.4666\n",
      "    - New mask: 57 features, Fitness: -19.1272\n",
      "    - New mask: 55 features, Fitness: -17.3092\n",
      "    - New mask: 58 features, Fitness: -19.4889\n",
      "    - New mask: 58 features, Fitness: -19.6178\n",
      "    - New mask: 53 features, Fitness: -16.2548\n",
      "    - New mask: 55 features, Fitness: -19.1331\n",
      "    - New mask: 58 features, Fitness: -19.6336\n",
      "    - New mask: 53 features, Fitness: -16.5044\n",
      "    - New mask: 55 features, Fitness: -17.8388\n",
      "    - New mask: 61 features, Fitness: -19.9292\n",
      "    - New mask: 57 features, Fitness: -16.0561\n",
      "    - New mask: 56 features, Fitness: -16.2832\n",
      "    - New mask: 58 features, Fitness: -16.6069\n",
      "    - New mask: 60 features, Fitness: -18.6883\n",
      "    - New mask: 58 features, Fitness: -17.5871\n",
      "    - New mask: 57 features, Fitness: -16.7030\n",
      "    - New mask: 55 features, Fitness: -15.4626\n",
      "    - New mask: 57 features, Fitness: -16.6809\n",
      "    - New mask: 54 features, Fitness: -14.3561\n",
      "    - New mask: 55 features, Fitness: -14.5432\n",
      "    - New mask: 55 features, Fitness: -14.9266\n",
      "    - New mask: 61 features, Fitness: -19.2108\n",
      "    - New mask: 59 features, Fitness: -17.4707\n",
      "    - New mask: 55 features, Fitness: -13.5492\n",
      "    - New mask: 52 features, Fitness: -13.1821\n",
      "    - New mask: 55 features, Fitness: -14.8707\n",
      "    - New mask: 58 features, Fitness: -17.0972\n",
      "    - New mask: 52 features, Fitness: -11.7995\n",
      "    - New mask: 58 features, Fitness: -17.2748\n",
      "    - New mask: 58 features, Fitness: -22.3259\n",
      "    - New mask: 58 features, Fitness: -22.1671\n",
      "    - New mask: 55 features, Fitness: -19.7831\n",
      "    - New mask: 55 features, Fitness: -20.1826\n",
      "    - New mask: 59 features, Fitness: -24.4799\n",
      "    - New mask: 60 features, Fitness: -25.8379\n",
      "    - New mask: 58 features, Fitness: -22.0743\n",
      "    - New mask: 53 features, Fitness: -19.9159\n",
      "    - New mask: 55 features, Fitness: -18.4330\n",
      "    - New mask: 50 features, Fitness: -16.7660\n",
      "    - New mask: 59 features, Fitness: -22.4626\n",
      "    - New mask: 52 features, Fitness: -18.7480\n",
      "    - New mask: 58 features, Fitness: -21.0989\n",
      "    - New mask: 54 features, Fitness: -21.4571\n",
      "    - New mask: 57 features, Fitness: -22.9238\n",
      "    - New mask: 55 features, Fitness: -18.5831\n",
      "    - New mask: 54 features, Fitness: -19.8550\n",
      "    - New mask: 53 features, Fitness: -19.6349\n",
      "    - New mask: 53 features, Fitness: -17.6880\n",
      "    - New mask: 58 features, Fitness: -23.4351\n",
      "    - New mask: 54 features, Fitness: -20.1142\n",
      "    - New mask: 56 features, Fitness: -22.6951\n",
      "    - New mask: 56 features, Fitness: -22.1000\n",
      "    - New mask: 57 features, Fitness: -23.3780\n",
      "    - New mask: 56 features, Fitness: -21.2421\n",
      "    - New mask: 55 features, Fitness: -23.5088\n",
      "    - New mask: 59 features, Fitness: -24.8187\n",
      "    - New mask: 55 features, Fitness: -21.7084\n",
      "    - New mask: 54 features, Fitness: -19.9767\n",
      "    - New mask: 57 features, Fitness: -23.5763\n",
      "    - New mask: 53 features, Fitness: -20.4208\n",
      "    - New mask: 56 features, Fitness: -20.6313\n",
      "    - New mask: 59 features, Fitness: -23.4035\n",
      "    - New mask: 57 features, Fitness: -24.4162\n",
      "    - New mask: 58 features, Fitness: -23.3121\n",
      "    - New mask: 56 features, Fitness: -21.4564\n",
      "    - New mask: 58 features, Fitness: -24.8483\n",
      "    - New mask: 56 features, Fitness: -23.0160\n",
      "    - New mask: 56 features, Fitness: -21.8856\n",
      "    - New mask: 52 features, Fitness: -18.6868\n",
      "    - New mask: 54 features, Fitness: -18.2571\n",
      "    - New mask: 57 features, Fitness: -19.6335\n",
      "    - New mask: 55 features, Fitness: -17.6254\n",
      "    - New mask: 57 features, Fitness: -18.5953\n",
      "    - New mask: 56 features, Fitness: -17.8638\n",
      "    - New mask: 58 features, Fitness: -19.3094\n",
      "    - New mask: 57 features, Fitness: -18.4389\n",
      "    - New mask: 57 features, Fitness: -18.8249\n",
      "    - New mask: 58 features, Fitness: -21.1072\n",
      "    - New mask: 59 features, Fitness: -20.7680\n",
      "    - New mask: 60 features, Fitness: -21.7024\n",
      "    - New mask: 59 features, Fitness: -21.9907\n",
      "    - New mask: 58 features, Fitness: -22.9718\n",
      "    - New mask: 55 features, Fitness: -16.8884\n",
      "    - New mask: 56 features, Fitness: -17.8761\n",
      "    - New mask: 55 features, Fitness: -18.1345\n",
      "    - New mask: 63 features, Fitness: -26.1764\n",
      "    - New mask: 58 features, Fitness: -19.8659\n",
      "    - New mask: 59 features, Fitness: -23.4876\n",
      "    - New mask: 54 features, Fitness: -17.0008\n",
      "=== End of Round 13: Vote mask selects 59 features (rho: 0.58)\n",
      "    Indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 25, 26, 27, 29, 30, 33, 34, 35, 36, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 14 ================\n",
      "  Adaptive rho for this round: 0.61\n",
      "    - New mask: 49 features, Fitness: -14.5192\n",
      "    - New mask: 61 features, Fitness: -22.1009\n",
      "    - New mask: 58 features, Fitness: -19.1236\n",
      "    - New mask: 57 features, Fitness: -18.3156\n",
      "    - New mask: 55 features, Fitness: -16.1222\n",
      "    - New mask: 58 features, Fitness: -21.6093\n",
      "    - New mask: 58 features, Fitness: -18.9060\n",
      "    - New mask: 57 features, Fitness: -18.8018\n",
      "    - New mask: 55 features, Fitness: -16.9689\n",
      "    - New mask: 55 features, Fitness: -16.7366\n",
      "    - New mask: 58 features, Fitness: -19.2333\n",
      "    - New mask: 58 features, Fitness: -20.4678\n",
      "    - New mask: 57 features, Fitness: -18.4210\n",
      "    - New mask: 54 features, Fitness: -16.1298\n",
      "    - New mask: 55 features, Fitness: -16.3673\n",
      "    - New mask: 56 features, Fitness: -17.6256\n",
      "    - New mask: 54 features, Fitness: -16.7147\n",
      "    - New mask: 58 features, Fitness: -19.5114\n",
      "    - New mask: 54 features, Fitness: -16.1511\n",
      "    - New mask: 53 features, Fitness: -15.0199\n",
      "    - New mask: 53 features, Fitness: -15.2810\n",
      "    - New mask: 51 features, Fitness: -14.1741\n",
      "    - New mask: 55 features, Fitness: -19.1852\n",
      "    - New mask: 55 features, Fitness: -17.2685\n",
      "    - New mask: 56 features, Fitness: -18.9417\n",
      "    - New mask: 51 features, Fitness: -15.9351\n",
      "    - New mask: 52 features, Fitness: -14.6842\n",
      "    - New mask: 52 features, Fitness: -15.5386\n",
      "    - New mask: 53 features, Fitness: -15.0671\n",
      "    - New mask: 53 features, Fitness: -15.8971\n",
      "    - New mask: 51 features, Fitness: -14.2191\n",
      "    - New mask: 57 features, Fitness: -18.4559\n",
      "    - New mask: 53 features, Fitness: -15.5596\n",
      "    - New mask: 53 features, Fitness: -16.9549\n",
      "    - New mask: 51 features, Fitness: -12.8828\n",
      "    - New mask: 55 features, Fitness: -17.3592\n",
      "    - New mask: 52 features, Fitness: -14.3687\n",
      "    - New mask: 57 features, Fitness: -18.3548\n",
      "    - New mask: 55 features, Fitness: -17.5244\n",
      "    - New mask: 56 features, Fitness: -18.0089\n",
      "    - New mask: 56 features, Fitness: -15.7911\n",
      "    - New mask: 52 features, Fitness: -11.7481\n",
      "    - New mask: 56 features, Fitness: -15.4933\n",
      "    - New mask: 51 features, Fitness: -11.5833\n",
      "    - New mask: 56 features, Fitness: -15.7744\n",
      "    - New mask: 55 features, Fitness: -14.8267\n",
      "    - New mask: 56 features, Fitness: -15.9954\n",
      "    - New mask: 58 features, Fitness: -16.1348\n",
      "    - New mask: 58 features, Fitness: -17.2113\n",
      "    - New mask: 58 features, Fitness: -17.8267\n",
      "    - New mask: 53 features, Fitness: -13.3048\n",
      "    - New mask: 54 features, Fitness: -14.3062\n",
      "    - New mask: 52 features, Fitness: -12.2409\n",
      "    - New mask: 52 features, Fitness: -11.8205\n",
      "    - New mask: 54 features, Fitness: -15.5249\n",
      "    - New mask: 54 features, Fitness: -14.0632\n",
      "    - New mask: 56 features, Fitness: -15.8903\n",
      "    - New mask: 57 features, Fitness: -16.8613\n",
      "    - New mask: 56 features, Fitness: -16.4887\n",
      "    - New mask: 52 features, Fitness: -12.7487\n",
      "    - New mask: 60 features, Fitness: -18.6956\n",
      "    - New mask: 56 features, Fitness: -16.7628\n",
      "    - New mask: 55 features, Fitness: -16.2476\n",
      "    - New mask: 57 features, Fitness: -16.7921\n",
      "    - New mask: 54 features, Fitness: -13.7141\n",
      "    - New mask: 53 features, Fitness: -13.4405\n",
      "    - New mask: 56 features, Fitness: -17.0088\n",
      "    - New mask: 57 features, Fitness: -16.9792\n",
      "    - New mask: 57 features, Fitness: -17.0196\n",
      "    - New mask: 54 features, Fitness: -13.6718\n",
      "    - New mask: 57 features, Fitness: -16.5428\n",
      "    - New mask: 58 features, Fitness: -17.1877\n",
      "    - New mask: 58 features, Fitness: -17.0652\n",
      "    - New mask: 56 features, Fitness: -16.9332\n",
      "    - New mask: 56 features, Fitness: -15.7860\n",
      "    - New mask: 55 features, Fitness: -15.0056\n",
      "    - New mask: 55 features, Fitness: -15.2311\n",
      "    - New mask: 51 features, Fitness: -11.9554\n",
      "    - New mask: 52 features, Fitness: -13.3112\n",
      "    - New mask: 56 features, Fitness: -16.0673\n",
      "    - New mask: 51 features, Fitness: -9.6022\n",
      "    - New mask: 54 features, Fitness: -11.5738\n",
      "    - New mask: 57 features, Fitness: -14.2159\n",
      "    - New mask: 55 features, Fitness: -13.2954\n",
      "    - New mask: 51 features, Fitness: -11.3738\n",
      "    - New mask: 55 features, Fitness: -13.6670\n",
      "    - New mask: 53 features, Fitness: -11.3240\n",
      "    - New mask: 58 features, Fitness: -14.4359\n",
      "    - New mask: 56 features, Fitness: -13.0382\n",
      "    - New mask: 55 features, Fitness: -12.6462\n",
      "    - New mask: 52 features, Fitness: -10.4700\n",
      "    - New mask: 55 features, Fitness: -12.9257\n",
      "    - New mask: 59 features, Fitness: -15.6287\n",
      "    - New mask: 56 features, Fitness: -14.4786\n",
      "    - New mask: 53 features, Fitness: -11.9131\n",
      "    - New mask: 52 features, Fitness: -11.3077\n",
      "    - New mask: 54 features, Fitness: -14.5410\n",
      "    - New mask: 57 features, Fitness: -15.4059\n",
      "    - New mask: 54 features, Fitness: -12.7132\n",
      "    - New mask: 56 features, Fitness: -13.3713\n",
      "    - New mask: 57 features, Fitness: -19.5240\n",
      "    - New mask: 51 features, Fitness: -15.1544\n",
      "    - New mask: 53 features, Fitness: -15.8703\n",
      "    - New mask: 58 features, Fitness: -19.9458\n",
      "    - New mask: 58 features, Fitness: -19.3565\n",
      "    - New mask: 53 features, Fitness: -15.8951\n",
      "    - New mask: 57 features, Fitness: -20.2765\n",
      "    - New mask: 56 features, Fitness: -18.2891\n",
      "    - New mask: 56 features, Fitness: -18.3178\n",
      "    - New mask: 57 features, Fitness: -19.2716\n",
      "    - New mask: 52 features, Fitness: -16.2077\n",
      "    - New mask: 55 features, Fitness: -17.7480\n",
      "    - New mask: 53 features, Fitness: -17.1630\n",
      "    - New mask: 58 features, Fitness: -20.2711\n",
      "    - New mask: 52 features, Fitness: -17.6489\n",
      "    - New mask: 55 features, Fitness: -18.8007\n",
      "    - New mask: 56 features, Fitness: -19.9168\n",
      "    - New mask: 57 features, Fitness: -19.3376\n",
      "    - New mask: 50 features, Fitness: -14.6807\n",
      "    - New mask: 54 features, Fitness: -18.1797\n",
      "    - New mask: 52 features, Fitness: -12.8865\n",
      "    - New mask: 56 features, Fitness: -15.1602\n",
      "    - New mask: 54 features, Fitness: -13.6177\n",
      "    - New mask: 56 features, Fitness: -14.9322\n",
      "    - New mask: 56 features, Fitness: -15.2648\n",
      "    - New mask: 54 features, Fitness: -13.5930\n",
      "    - New mask: 54 features, Fitness: -13.2825\n",
      "    - New mask: 55 features, Fitness: -14.8617\n",
      "    - New mask: 51 features, Fitness: -11.4621\n",
      "    - New mask: 55 features, Fitness: -14.3547\n",
      "    - New mask: 50 features, Fitness: -10.9643\n",
      "    - New mask: 55 features, Fitness: -14.1723\n",
      "    - New mask: 59 features, Fitness: -16.8293\n",
      "    - New mask: 58 features, Fitness: -17.0404\n",
      "    - New mask: 54 features, Fitness: -12.9738\n",
      "    - New mask: 52 features, Fitness: -11.8498\n",
      "    - New mask: 54 features, Fitness: -13.2730\n",
      "    - New mask: 54 features, Fitness: -13.7941\n",
      "    - New mask: 55 features, Fitness: -14.5988\n",
      "    - New mask: 56 features, Fitness: -16.0680\n",
      "    - New mask: 52 features, Fitness: -16.9661\n",
      "    - New mask: 54 features, Fitness: -18.6677\n",
      "    - New mask: 53 features, Fitness: -18.5383\n",
      "    - New mask: 59 features, Fitness: -25.7426\n",
      "    - New mask: 56 features, Fitness: -21.8032\n",
      "    - New mask: 56 features, Fitness: -22.9632\n",
      "    - New mask: 55 features, Fitness: -20.0532\n",
      "    - New mask: 54 features, Fitness: -21.1018\n",
      "    - New mask: 53 features, Fitness: -17.8469\n",
      "    - New mask: 53 features, Fitness: -19.2828\n",
      "    - New mask: 57 features, Fitness: -22.8877\n",
      "    - New mask: 52 features, Fitness: -18.8891\n",
      "    - New mask: 56 features, Fitness: -21.4683\n",
      "    - New mask: 51 features, Fitness: -17.1225\n",
      "    - New mask: 57 features, Fitness: -21.9970\n",
      "    - New mask: 51 features, Fitness: -15.7964\n",
      "    - New mask: 58 features, Fitness: -23.0803\n",
      "    - New mask: 54 features, Fitness: -19.9679\n",
      "    - New mask: 55 features, Fitness: -19.3837\n",
      "    - New mask: 52 features, Fitness: -18.8560\n",
      "    - New mask: 54 features, Fitness: -20.3121\n",
      "    - New mask: 52 features, Fitness: -16.3068\n",
      "    - New mask: 53 features, Fitness: -19.7268\n",
      "    - New mask: 60 features, Fitness: -26.8032\n",
      "    - New mask: 56 features, Fitness: -22.1868\n",
      "    - New mask: 50 features, Fitness: -16.4828\n",
      "    - New mask: 52 features, Fitness: -17.0936\n",
      "    - New mask: 54 features, Fitness: -20.8893\n",
      "    - New mask: 52 features, Fitness: -18.8434\n",
      "    - New mask: 59 features, Fitness: -24.3176\n",
      "    - New mask: 53 features, Fitness: -18.1675\n",
      "    - New mask: 54 features, Fitness: -18.7836\n",
      "    - New mask: 57 features, Fitness: -22.6781\n",
      "    - New mask: 56 features, Fitness: -21.7783\n",
      "    - New mask: 57 features, Fitness: -21.2486\n",
      "    - New mask: 54 features, Fitness: -17.8104\n",
      "    - New mask: 56 features, Fitness: -22.4265\n",
      "    - New mask: 51 features, Fitness: -18.9758\n",
      "    - New mask: 57 features, Fitness: -22.9744\n",
      "    - New mask: 56 features, Fitness: -21.8502\n",
      "    - New mask: 54 features, Fitness: -15.3812\n",
      "    - New mask: 59 features, Fitness: -20.4991\n",
      "    - New mask: 56 features, Fitness: -18.1864\n",
      "    - New mask: 56 features, Fitness: -17.9328\n",
      "    - New mask: 57 features, Fitness: -18.7943\n",
      "    - New mask: 53 features, Fitness: -15.6105\n",
      "    - New mask: 55 features, Fitness: -17.5224\n",
      "    - New mask: 55 features, Fitness: -16.6109\n",
      "    - New mask: 59 features, Fitness: -20.4130\n",
      "    - New mask: 55 features, Fitness: -18.1493\n",
      "    - New mask: 56 features, Fitness: -18.0176\n",
      "    - New mask: 57 features, Fitness: -19.2199\n",
      "    - New mask: 57 features, Fitness: -20.9123\n",
      "    - New mask: 53 features, Fitness: -15.8456\n",
      "    - New mask: 59 features, Fitness: -20.2158\n",
      "    - New mask: 56 features, Fitness: -18.2054\n",
      "    - New mask: 57 features, Fitness: -19.8518\n",
      "    - New mask: 57 features, Fitness: -18.5217\n",
      "    - New mask: 56 features, Fitness: -20.1130\n",
      "    - New mask: 55 features, Fitness: -17.2360\n",
      "=== End of Round 14: Vote mask selects 55 features (rho: 0.61)\n",
      "    Indices: [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 25, 26, 27, 29, 30, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 15 ================\n",
      "  Adaptive rho for this round: 0.64\n",
      "    - New mask: 50 features, Fitness: -15.5763\n",
      "    - New mask: 54 features, Fitness: -17.2905\n",
      "    - New mask: 51 features, Fitness: -14.6333\n",
      "    - New mask: 51 features, Fitness: -14.1704\n",
      "    - New mask: 56 features, Fitness: -18.7626\n",
      "    - New mask: 57 features, Fitness: -18.6206\n",
      "    - New mask: 53 features, Fitness: -16.1624\n",
      "    - New mask: 53 features, Fitness: -15.7220\n",
      "    - New mask: 54 features, Fitness: -15.9407\n",
      "    - New mask: 50 features, Fitness: -14.3330\n",
      "    - New mask: 52 features, Fitness: -17.1582\n",
      "    - New mask: 54 features, Fitness: -16.1405\n",
      "    - New mask: 52 features, Fitness: -14.4567\n",
      "    - New mask: 52 features, Fitness: -14.7080\n",
      "    - New mask: 52 features, Fitness: -15.7981\n",
      "    - New mask: 50 features, Fitness: -13.2253\n",
      "    - New mask: 55 features, Fitness: -17.6695\n",
      "    - New mask: 51 features, Fitness: -15.7700\n",
      "    - New mask: 52 features, Fitness: -16.4495\n",
      "    - New mask: 50 features, Fitness: -12.1937\n",
      "    - New mask: 53 features, Fitness: -14.6964\n",
      "    - New mask: 54 features, Fitness: -15.3405\n",
      "    - New mask: 49 features, Fitness: -12.9460\n",
      "    - New mask: 51 features, Fitness: -14.8930\n",
      "    - New mask: 49 features, Fitness: -12.6290\n",
      "    - New mask: 52 features, Fitness: -15.9084\n",
      "    - New mask: 50 features, Fitness: -11.7488\n",
      "    - New mask: 48 features, Fitness: -12.8731\n",
      "    - New mask: 54 features, Fitness: -15.8788\n",
      "    - New mask: 53 features, Fitness: -15.0620\n",
      "    - New mask: 51 features, Fitness: -12.9257\n",
      "    - New mask: 52 features, Fitness: -13.6339\n",
      "    - New mask: 52 features, Fitness: -14.7222\n",
      "    - New mask: 55 features, Fitness: -16.8841\n",
      "    - New mask: 47 features, Fitness: -10.2570\n",
      "    - New mask: 55 features, Fitness: -16.6740\n",
      "    - New mask: 54 features, Fitness: -14.2665\n",
      "    - New mask: 55 features, Fitness: -15.7450\n",
      "    - New mask: 51 features, Fitness: -12.4096\n",
      "    - New mask: 55 features, Fitness: -17.0309\n",
      "    - New mask: 55 features, Fitness: -15.9422\n",
      "    - New mask: 47 features, Fitness: -9.0994\n",
      "    - New mask: 56 features, Fitness: -15.5762\n",
      "    - New mask: 51 features, Fitness: -11.3022\n",
      "    - New mask: 53 features, Fitness: -15.6032\n",
      "    - New mask: 54 features, Fitness: -13.9146\n",
      "    - New mask: 56 features, Fitness: -16.1178\n",
      "    - New mask: 52 features, Fitness: -11.6981\n",
      "    - New mask: 55 features, Fitness: -14.5956\n",
      "    - New mask: 54 features, Fitness: -13.3245\n",
      "    - New mask: 50 features, Fitness: -11.8855\n",
      "    - New mask: 51 features, Fitness: -12.2802\n",
      "    - New mask: 51 features, Fitness: -12.0741\n",
      "    - New mask: 50 features, Fitness: -10.3022\n",
      "    - New mask: 56 features, Fitness: -17.3772\n",
      "    - New mask: 53 features, Fitness: -13.4725\n",
      "    - New mask: 56 features, Fitness: -15.0354\n",
      "    - New mask: 58 features, Fitness: -18.2651\n",
      "    - New mask: 57 features, Fitness: -17.1461\n",
      "    - New mask: 49 features, Fitness: -9.7434\n",
      "    - New mask: 57 features, Fitness: -16.8745\n",
      "    - New mask: 50 features, Fitness: -12.6475\n",
      "    - New mask: 51 features, Fitness: -12.9186\n",
      "    - New mask: 57 features, Fitness: -17.3651\n",
      "    - New mask: 51 features, Fitness: -12.2310\n",
      "    - New mask: 51 features, Fitness: -11.3923\n",
      "    - New mask: 54 features, Fitness: -15.3429\n",
      "    - New mask: 56 features, Fitness: -15.5251\n",
      "    - New mask: 54 features, Fitness: -14.8088\n",
      "    - New mask: 55 features, Fitness: -14.2175\n",
      "    - New mask: 52 features, Fitness: -12.4698\n",
      "    - New mask: 55 features, Fitness: -15.1000\n",
      "    - New mask: 55 features, Fitness: -14.5160\n",
      "    - New mask: 51 features, Fitness: -12.1152\n",
      "    - New mask: 56 features, Fitness: -14.9066\n",
      "    - New mask: 55 features, Fitness: -15.5700\n",
      "    - New mask: 51 features, Fitness: -12.6855\n",
      "    - New mask: 53 features, Fitness: -14.1585\n",
      "    - New mask: 53 features, Fitness: -14.1789\n",
      "    - New mask: 49 features, Fitness: -11.1536\n",
      "    - New mask: 52 features, Fitness: -10.5898\n",
      "    - New mask: 52 features, Fitness: -10.3885\n",
      "    - New mask: 50 features, Fitness: -10.4741\n",
      "    - New mask: 51 features, Fitness: -10.0095\n",
      "    - New mask: 48 features, Fitness: -8.5834\n",
      "    - New mask: 56 features, Fitness: -14.1864\n",
      "    - New mask: 51 features, Fitness: -10.3080\n",
      "    - New mask: 52 features, Fitness: -10.3212\n",
      "    - New mask: 54 features, Fitness: -11.8172\n",
      "    - New mask: 53 features, Fitness: -12.1798\n",
      "    - New mask: 53 features, Fitness: -10.8162\n",
      "    - New mask: 52 features, Fitness: -10.2490\n",
      "    - New mask: 56 features, Fitness: -12.7966\n",
      "    - New mask: 55 features, Fitness: -13.4950\n",
      "    - New mask: 51 features, Fitness: -9.7199\n",
      "    - New mask: 52 features, Fitness: -11.1071\n",
      "    - New mask: 53 features, Fitness: -11.8876\n",
      "    - New mask: 54 features, Fitness: -12.5898\n",
      "    - New mask: 49 features, Fitness: -9.7618\n",
      "    - New mask: 51 features, Fitness: -9.1090\n",
      "    - New mask: 52 features, Fitness: -16.1505\n",
      "    - New mask: 53 features, Fitness: -16.1662\n",
      "    - New mask: 51 features, Fitness: -14.2109\n",
      "    - New mask: 51 features, Fitness: -15.8079\n",
      "    - New mask: 52 features, Fitness: -15.2640\n",
      "    - New mask: 47 features, Fitness: -12.3162\n",
      "    - New mask: 54 features, Fitness: -17.8927\n",
      "    - New mask: 51 features, Fitness: -14.8196\n",
      "    - New mask: 52 features, Fitness: -15.3774\n",
      "    - New mask: 58 features, Fitness: -20.8448\n",
      "    - New mask: 53 features, Fitness: -16.1459\n",
      "    - New mask: 50 features, Fitness: -14.4008\n",
      "    - New mask: 52 features, Fitness: -15.7864\n",
      "    - New mask: 52 features, Fitness: -15.2315\n",
      "    - New mask: 52 features, Fitness: -15.4317\n",
      "    - New mask: 54 features, Fitness: -18.1693\n",
      "    - New mask: 55 features, Fitness: -17.3123\n",
      "    - New mask: 50 features, Fitness: -13.7705\n",
      "    - New mask: 51 features, Fitness: -13.9549\n",
      "    - New mask: 52 features, Fitness: -15.9479\n",
      "    - New mask: 54 features, Fitness: -14.3982\n",
      "    - New mask: 53 features, Fitness: -12.1808\n",
      "    - New mask: 51 features, Fitness: -10.5828\n",
      "    - New mask: 51 features, Fitness: -11.5908\n",
      "    - New mask: 54 features, Fitness: -13.1165\n",
      "    - New mask: 52 features, Fitness: -11.4390\n",
      "    - New mask: 52 features, Fitness: -10.8360\n",
      "    - New mask: 52 features, Fitness: -11.8894\n",
      "    - New mask: 48 features, Fitness: -9.2156\n",
      "    - New mask: 55 features, Fitness: -13.9341\n",
      "    - New mask: 54 features, Fitness: -13.4768\n",
      "    - New mask: 52 features, Fitness: -13.0265\n",
      "    - New mask: 56 features, Fitness: -14.3492\n",
      "    - New mask: 55 features, Fitness: -15.1659\n",
      "    - New mask: 52 features, Fitness: -11.6297\n",
      "    - New mask: 52 features, Fitness: -12.4213\n",
      "    - New mask: 54 features, Fitness: -13.2920\n",
      "    - New mask: 52 features, Fitness: -12.0434\n",
      "    - New mask: 53 features, Fitness: -13.4158\n",
      "    - New mask: 55 features, Fitness: -14.7080\n",
      "    - New mask: 52 features, Fitness: -17.1208\n",
      "    - New mask: 54 features, Fitness: -17.8424\n",
      "    - New mask: 51 features, Fitness: -17.4096\n",
      "    - New mask: 52 features, Fitness: -16.0256\n",
      "    - New mask: 55 features, Fitness: -20.3791\n",
      "    - New mask: 56 features, Fitness: -21.3108\n",
      "    - New mask: 54 features, Fitness: -18.6982\n",
      "    - New mask: 54 features, Fitness: -18.5864\n",
      "    - New mask: 54 features, Fitness: -18.3568\n",
      "    - New mask: 55 features, Fitness: -19.6422\n",
      "    - New mask: 50 features, Fitness: -15.0872\n",
      "    - New mask: 51 features, Fitness: -16.0463\n",
      "    - New mask: 51 features, Fitness: -15.6253\n",
      "    - New mask: 50 features, Fitness: -16.7555\n",
      "    - New mask: 53 features, Fitness: -17.9665\n",
      "    - New mask: 52 features, Fitness: -16.6486\n",
      "    - New mask: 50 features, Fitness: -15.2903\n",
      "    - New mask: 53 features, Fitness: -17.9646\n",
      "    - New mask: 52 features, Fitness: -16.3964\n",
      "    - New mask: 51 features, Fitness: -17.0139\n",
      "    - New mask: 52 features, Fitness: -16.5922\n",
      "    - New mask: 52 features, Fitness: -17.0830\n",
      "    - New mask: 52 features, Fitness: -17.1526\n",
      "    - New mask: 55 features, Fitness: -22.1002\n",
      "    - New mask: 53 features, Fitness: -17.5688\n",
      "    - New mask: 50 features, Fitness: -14.7543\n",
      "    - New mask: 50 features, Fitness: -15.2233\n",
      "    - New mask: 52 features, Fitness: -18.8491\n",
      "    - New mask: 54 features, Fitness: -19.0395\n",
      "    - New mask: 58 features, Fitness: -22.5641\n",
      "    - New mask: 55 features, Fitness: -19.7362\n",
      "    - New mask: 53 features, Fitness: -17.9378\n",
      "    - New mask: 52 features, Fitness: -17.4618\n",
      "    - New mask: 56 features, Fitness: -20.3196\n",
      "    - New mask: 58 features, Fitness: -22.7075\n",
      "    - New mask: 54 features, Fitness: -18.4616\n",
      "    - New mask: 53 features, Fitness: -18.2673\n",
      "    - New mask: 52 features, Fitness: -16.9687\n",
      "    - New mask: 54 features, Fitness: -19.3633\n",
      "    - New mask: 52 features, Fitness: -15.4961\n",
      "    - New mask: 53 features, Fitness: -14.4468\n",
      "    - New mask: 56 features, Fitness: -17.6230\n",
      "    - New mask: 54 features, Fitness: -16.6185\n",
      "    - New mask: 53 features, Fitness: -15.6065\n",
      "    - New mask: 55 features, Fitness: -17.1655\n",
      "    - New mask: 56 features, Fitness: -18.8567\n",
      "    - New mask: 55 features, Fitness: -17.2822\n",
      "    - New mask: 55 features, Fitness: -16.1256\n",
      "    - New mask: 54 features, Fitness: -15.9392\n",
      "    - New mask: 55 features, Fitness: -16.2575\n",
      "    - New mask: 53 features, Fitness: -15.5148\n",
      "    - New mask: 53 features, Fitness: -15.4415\n",
      "    - New mask: 53 features, Fitness: -16.3246\n",
      "    - New mask: 55 features, Fitness: -16.6436\n",
      "    - New mask: 55 features, Fitness: -16.7461\n",
      "    - New mask: 54 features, Fitness: -15.8514\n",
      "    - New mask: 53 features, Fitness: -14.8660\n",
      "    - New mask: 54 features, Fitness: -15.2091\n",
      "    - New mask: 55 features, Fitness: -17.8386\n",
      "    - New mask: 54 features, Fitness: -15.8363\n",
      "=== End of Round 15: Vote mask selects 54 features (rho: 0.64)\n",
      "    Indices: [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 25, 26, 27, 29, 30, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 16 ================\n",
      "  Adaptive rho for this round: 0.67\n",
      "    - New mask: 50 features, Fitness: -14.5935\n",
      "    - New mask: 51 features, Fitness: -14.1823\n",
      "    - New mask: 48 features, Fitness: -11.6386\n",
      "    - New mask: 51 features, Fitness: -13.4463\n",
      "    - New mask: 52 features, Fitness: -15.1421\n",
      "    - New mask: 54 features, Fitness: -15.3814\n",
      "    - New mask: 53 features, Fitness: -16.4513\n",
      "    - New mask: 50 features, Fitness: -13.7299\n",
      "    - New mask: 56 features, Fitness: -17.1554\n",
      "    - New mask: 55 features, Fitness: -17.2159\n",
      "    - New mask: 52 features, Fitness: -14.3201\n",
      "    - New mask: 55 features, Fitness: -17.0376\n",
      "    - New mask: 50 features, Fitness: -12.5946\n",
      "    - New mask: 50 features, Fitness: -13.2096\n",
      "    - New mask: 47 features, Fitness: -12.2975\n",
      "    - New mask: 49 features, Fitness: -12.1013\n",
      "    - New mask: 53 features, Fitness: -14.7703\n",
      "    - New mask: 51 features, Fitness: -14.3358\n",
      "    - New mask: 50 features, Fitness: -13.0218\n",
      "    - New mask: 51 features, Fitness: -12.8520\n",
      "    - New mask: 50 features, Fitness: -12.4370\n",
      "    - New mask: 51 features, Fitness: -13.4222\n",
      "    - New mask: 51 features, Fitness: -13.4184\n",
      "    - New mask: 51 features, Fitness: -13.7068\n",
      "    - New mask: 49 features, Fitness: -11.7429\n",
      "    - New mask: 51 features, Fitness: -13.2959\n",
      "    - New mask: 51 features, Fitness: -12.1688\n",
      "    - New mask: 51 features, Fitness: -13.5046\n",
      "    - New mask: 48 features, Fitness: -11.6731\n",
      "    - New mask: 48 features, Fitness: -11.5035\n",
      "    - New mask: 53 features, Fitness: -14.0421\n",
      "    - New mask: 52 features, Fitness: -12.9993\n",
      "    - New mask: 53 features, Fitness: -14.5573\n",
      "    - New mask: 51 features, Fitness: -12.9175\n",
      "    - New mask: 50 features, Fitness: -12.7420\n",
      "    - New mask: 50 features, Fitness: -12.5458\n",
      "    - New mask: 50 features, Fitness: -11.2394\n",
      "    - New mask: 49 features, Fitness: -10.5750\n",
      "    - New mask: 49 features, Fitness: -11.7535\n",
      "    - New mask: 54 features, Fitness: -15.5109\n",
      "    - New mask: 52 features, Fitness: -13.7154\n",
      "    - New mask: 50 features, Fitness: -12.9456\n",
      "    - New mask: 50 features, Fitness: -11.2396\n",
      "    - New mask: 53 features, Fitness: -13.3237\n",
      "    - New mask: 48 features, Fitness: -9.8123\n",
      "    - New mask: 51 features, Fitness: -10.7732\n",
      "    - New mask: 55 features, Fitness: -15.2381\n",
      "    - New mask: 51 features, Fitness: -11.8793\n",
      "    - New mask: 53 features, Fitness: -12.7263\n",
      "    - New mask: 52 features, Fitness: -11.4065\n",
      "    - New mask: 51 features, Fitness: -12.3054\n",
      "    - New mask: 49 features, Fitness: -12.2554\n",
      "    - New mask: 46 features, Fitness: -7.9482\n",
      "    - New mask: 53 features, Fitness: -13.4119\n",
      "    - New mask: 52 features, Fitness: -13.5659\n",
      "    - New mask: 49 features, Fitness: -10.4727\n",
      "    - New mask: 50 features, Fitness: -12.2751\n",
      "    - New mask: 53 features, Fitness: -13.9263\n",
      "    - New mask: 55 features, Fitness: -14.3755\n",
      "    - New mask: 51 features, Fitness: -11.6605\n",
      "    - New mask: 53 features, Fitness: -12.7705\n",
      "    - New mask: 47 features, Fitness: -10.2412\n",
      "    - New mask: 50 features, Fitness: -12.8909\n",
      "    - New mask: 54 features, Fitness: -14.6590\n",
      "    - New mask: 52 features, Fitness: -11.9622\n",
      "    - New mask: 50 features, Fitness: -11.9225\n",
      "    - New mask: 48 features, Fitness: -10.6107\n",
      "    - New mask: 51 features, Fitness: -12.5283\n",
      "    - New mask: 51 features, Fitness: -12.9672\n",
      "    - New mask: 53 features, Fitness: -13.4514\n",
      "    - New mask: 50 features, Fitness: -12.1306\n",
      "    - New mask: 52 features, Fitness: -13.1516\n",
      "    - New mask: 51 features, Fitness: -12.1142\n",
      "    - New mask: 53 features, Fitness: -14.4557\n",
      "    - New mask: 51 features, Fitness: -11.8618\n",
      "    - New mask: 52 features, Fitness: -12.0885\n",
      "    - New mask: 57 features, Fitness: -16.7041\n",
      "    - New mask: 51 features, Fitness: -12.8213\n",
      "    - New mask: 48 features, Fitness: -11.4918\n",
      "    - New mask: 48 features, Fitness: -10.2562\n",
      "    - New mask: 49 features, Fitness: -9.4501\n",
      "    - New mask: 48 features, Fitness: -8.3501\n",
      "    - New mask: 49 features, Fitness: -10.4086\n",
      "    - New mask: 54 features, Fitness: -12.2223\n",
      "    - New mask: 47 features, Fitness: -8.4908\n",
      "    - New mask: 55 features, Fitness: -12.7685\n",
      "    - New mask: 49 features, Fitness: -9.4116\n",
      "    - New mask: 50 features, Fitness: -9.8029\n",
      "    - New mask: 50 features, Fitness: -9.4561\n",
      "    - New mask: 53 features, Fitness: -11.1308\n",
      "    - New mask: 52 features, Fitness: -10.8568\n",
      "    - New mask: 48 features, Fitness: -8.8021\n",
      "    - New mask: 51 features, Fitness: -9.5641\n",
      "    - New mask: 50 features, Fitness: -8.8539\n",
      "    - New mask: 49 features, Fitness: -8.6421\n",
      "    - New mask: 54 features, Fitness: -12.9595\n",
      "    - New mask: 51 features, Fitness: -10.8688\n",
      "    - New mask: 53 features, Fitness: -12.5101\n",
      "    - New mask: 55 features, Fitness: -12.1956\n",
      "    - New mask: 50 features, Fitness: -8.6044\n",
      "    - New mask: 51 features, Fitness: -14.4000\n",
      "    - New mask: 48 features, Fitness: -13.5778\n",
      "    - New mask: 50 features, Fitness: -14.4334\n",
      "    - New mask: 50 features, Fitness: -14.4797\n",
      "    - New mask: 50 features, Fitness: -14.2935\n",
      "    - New mask: 49 features, Fitness: -13.0283\n",
      "    - New mask: 51 features, Fitness: -13.9751\n",
      "    - New mask: 52 features, Fitness: -14.6876\n",
      "    - New mask: 49 features, Fitness: -12.4813\n",
      "    - New mask: 48 features, Fitness: -12.7689\n",
      "    - New mask: 54 features, Fitness: -16.1948\n",
      "    - New mask: 49 features, Fitness: -12.7522\n",
      "    - New mask: 49 features, Fitness: -14.3591\n",
      "    - New mask: 46 features, Fitness: -11.7530\n",
      "    - New mask: 50 features, Fitness: -13.8454\n",
      "    - New mask: 51 features, Fitness: -15.7106\n",
      "    - New mask: 53 features, Fitness: -14.7009\n",
      "    - New mask: 56 features, Fitness: -17.4193\n",
      "    - New mask: 51 features, Fitness: -14.3665\n",
      "    - New mask: 52 features, Fitness: -15.5749\n",
      "    - New mask: 52 features, Fitness: -11.8070\n",
      "    - New mask: 46 features, Fitness: -7.9587\n",
      "    - New mask: 53 features, Fitness: -12.3560\n",
      "    - New mask: 51 features, Fitness: -11.3007\n",
      "    - New mask: 54 features, Fitness: -13.3163\n",
      "    - New mask: 48 features, Fitness: -8.8936\n",
      "    - New mask: 50 features, Fitness: -10.0774\n",
      "    - New mask: 51 features, Fitness: -10.5178\n",
      "    - New mask: 49 features, Fitness: -10.2866\n",
      "    - New mask: 52 features, Fitness: -12.4701\n",
      "    - New mask: 51 features, Fitness: -11.1484\n",
      "    - New mask: 54 features, Fitness: -12.9260\n",
      "    - New mask: 49 features, Fitness: -10.1028\n",
      "    - New mask: 48 features, Fitness: -10.8950\n",
      "    - New mask: 50 features, Fitness: -10.9823\n",
      "    - New mask: 52 features, Fitness: -12.0116\n",
      "    - New mask: 51 features, Fitness: -11.2400\n",
      "    - New mask: 52 features, Fitness: -11.7856\n",
      "    - New mask: 51 features, Fitness: -11.3119\n",
      "    - New mask: 49 features, Fitness: -10.2944\n",
      "    - New mask: 50 features, Fitness: -14.4684\n",
      "    - New mask: 52 features, Fitness: -15.9019\n",
      "    - New mask: 46 features, Fitness: -13.4902\n",
      "    - New mask: 47 features, Fitness: -12.2120\n",
      "    - New mask: 48 features, Fitness: -14.0401\n",
      "    - New mask: 53 features, Fitness: -18.0363\n",
      "    - New mask: 48 features, Fitness: -14.0688\n",
      "    - New mask: 53 features, Fitness: -17.5018\n",
      "    - New mask: 59 features, Fitness: -22.6435\n",
      "    - New mask: 57 features, Fitness: -21.9671\n",
      "    - New mask: 54 features, Fitness: -18.2594\n",
      "    - New mask: 54 features, Fitness: -18.5000\n",
      "    - New mask: 55 features, Fitness: -18.6079\n",
      "    - New mask: 46 features, Fitness: -11.9820\n",
      "    - New mask: 49 features, Fitness: -14.4150\n",
      "    - New mask: 52 features, Fitness: -16.3705\n",
      "    - New mask: 51 features, Fitness: -15.8441\n",
      "    - New mask: 53 features, Fitness: -17.4581\n",
      "    - New mask: 52 features, Fitness: -16.5467\n",
      "    - New mask: 52 features, Fitness: -16.5899\n",
      "    - New mask: 51 features, Fitness: -14.8447\n",
      "    - New mask: 50 features, Fitness: -15.1988\n",
      "    - New mask: 49 features, Fitness: -15.0126\n",
      "    - New mask: 52 features, Fitness: -20.7695\n",
      "    - New mask: 52 features, Fitness: -18.9079\n",
      "    - New mask: 50 features, Fitness: -14.1476\n",
      "    - New mask: 47 features, Fitness: -13.5823\n",
      "    - New mask: 49 features, Fitness: -14.8936\n",
      "    - New mask: 50 features, Fitness: -15.1838\n",
      "    - New mask: 52 features, Fitness: -17.1546\n",
      "    - New mask: 51 features, Fitness: -16.4602\n",
      "    - New mask: 52 features, Fitness: -15.9025\n",
      "    - New mask: 54 features, Fitness: -18.8979\n",
      "    - New mask: 52 features, Fitness: -16.2130\n",
      "    - New mask: 52 features, Fitness: -17.4325\n",
      "    - New mask: 54 features, Fitness: -18.9844\n",
      "    - New mask: 55 features, Fitness: -19.9487\n",
      "    - New mask: 51 features, Fitness: -15.8100\n",
      "    - New mask: 51 features, Fitness: -17.1535\n",
      "    - New mask: 55 features, Fitness: -19.8528\n",
      "    - New mask: 51 features, Fitness: -13.5618\n",
      "    - New mask: 53 features, Fitness: -14.2438\n",
      "    - New mask: 54 features, Fitness: -16.4788\n",
      "    - New mask: 54 features, Fitness: -15.5504\n",
      "    - New mask: 51 features, Fitness: -13.1419\n",
      "    - New mask: 50 features, Fitness: -13.8597\n",
      "    - New mask: 55 features, Fitness: -16.0425\n",
      "    - New mask: 50 features, Fitness: -12.0399\n",
      "    - New mask: 53 features, Fitness: -15.3426\n",
      "    - New mask: 51 features, Fitness: -13.6693\n",
      "    - New mask: 47 features, Fitness: -11.5405\n",
      "    - New mask: 54 features, Fitness: -15.7772\n",
      "    - New mask: 52 features, Fitness: -14.1546\n",
      "    - New mask: 53 features, Fitness: -14.7507\n",
      "    - New mask: 53 features, Fitness: -16.0217\n",
      "    - New mask: 56 features, Fitness: -16.6305\n",
      "    - New mask: 55 features, Fitness: -16.1910\n",
      "    - New mask: 55 features, Fitness: -17.2438\n",
      "    - New mask: 55 features, Fitness: -16.5649\n",
      "    - New mask: 54 features, Fitness: -16.1183\n",
      "=== End of Round 16: Vote mask selects 53 features (rho: 0.67)\n",
      "    Indices: [0, 1, 2, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 25, 26, 27, 29, 30, 33, 34, 35, 36, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 52, 53, 54, 55, 57, 58, 59, 60, 61, 62, 63, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 17 ================\n",
      "  Adaptive rho for this round: 0.71\n",
      "    - New mask: 51 features, Fitness: -13.0903\n",
      "    - New mask: 48 features, Fitness: -13.1425\n",
      "    - New mask: 49 features, Fitness: -12.0590\n",
      "    - New mask: 51 features, Fitness: -13.4967\n",
      "    - New mask: 52 features, Fitness: -14.8955\n",
      "    - New mask: 53 features, Fitness: -15.8799\n",
      "    - New mask: 50 features, Fitness: -12.5745\n",
      "    - New mask: 51 features, Fitness: -14.0370\n",
      "    - New mask: 50 features, Fitness: -14.1847\n",
      "    - New mask: 51 features, Fitness: -15.3183\n",
      "    - New mask: 51 features, Fitness: -13.4071\n",
      "    - New mask: 49 features, Fitness: -13.1484\n",
      "    - New mask: 49 features, Fitness: -12.8980\n",
      "    - New mask: 52 features, Fitness: -15.0134\n",
      "    - New mask: 50 features, Fitness: -13.4637\n",
      "    - New mask: 51 features, Fitness: -13.6357\n",
      "    - New mask: 52 features, Fitness: -13.6331\n",
      "    - New mask: 54 features, Fitness: -15.8970\n",
      "    - New mask: 49 features, Fitness: -12.3475\n",
      "    - New mask: 53 features, Fitness: -14.8742\n",
      "    - New mask: 51 features, Fitness: -12.6900\n",
      "    - New mask: 46 features, Fitness: -10.4549\n",
      "    - New mask: 55 features, Fitness: -17.0015\n",
      "    - New mask: 50 features, Fitness: -12.8390\n",
      "    - New mask: 51 features, Fitness: -11.9669\n",
      "    - New mask: 48 features, Fitness: -10.8172\n",
      "    - New mask: 50 features, Fitness: -11.2122\n",
      "    - New mask: 53 features, Fitness: -13.9770\n",
      "    - New mask: 48 features, Fitness: -10.4272\n",
      "    - New mask: 52 features, Fitness: -13.1392\n",
      "    - New mask: 51 features, Fitness: -12.5047\n",
      "    - New mask: 48 features, Fitness: -10.5466\n",
      "    - New mask: 54 features, Fitness: -14.6782\n",
      "    - New mask: 53 features, Fitness: -14.3158\n",
      "    - New mask: 46 features, Fitness: -9.0779\n",
      "    - New mask: 52 features, Fitness: -13.0904\n",
      "    - New mask: 49 features, Fitness: -10.6574\n",
      "    - New mask: 51 features, Fitness: -11.9713\n",
      "    - New mask: 51 features, Fitness: -13.9531\n",
      "    - New mask: 54 features, Fitness: -15.5978\n",
      "    - New mask: 45 features, Fitness: -8.8316\n",
      "    - New mask: 48 features, Fitness: -10.7556\n",
      "    - New mask: 50 features, Fitness: -11.9863\n",
      "    - New mask: 51 features, Fitness: -11.1452\n",
      "    - New mask: 49 features, Fitness: -9.4086\n",
      "    - New mask: 48 features, Fitness: -8.8312\n",
      "    - New mask: 51 features, Fitness: -11.7400\n",
      "    - New mask: 50 features, Fitness: -11.2608\n",
      "    - New mask: 51 features, Fitness: -10.9451\n",
      "    - New mask: 51 features, Fitness: -10.5275\n",
      "    - New mask: 51 features, Fitness: -10.6592\n",
      "    - New mask: 50 features, Fitness: -10.9111\n",
      "    - New mask: 42 features, Fitness: -7.0052\n",
      "    - New mask: 48 features, Fitness: -8.8842\n",
      "    - New mask: 49 features, Fitness: -11.2861\n",
      "    - New mask: 45 features, Fitness: -6.9475\n",
      "    - New mask: 49 features, Fitness: -9.8315\n",
      "    - New mask: 47 features, Fitness: -9.2602\n",
      "    - New mask: 52 features, Fitness: -12.6528\n",
      "    - New mask: 45 features, Fitness: -7.6645\n",
      "    - New mask: 53 features, Fitness: -13.4298\n",
      "    - New mask: 49 features, Fitness: -11.2952\n",
      "    - New mask: 51 features, Fitness: -11.9143\n",
      "    - New mask: 52 features, Fitness: -12.0571\n",
      "    - New mask: 50 features, Fitness: -11.4859\n",
      "    - New mask: 48 features, Fitness: -11.3480\n",
      "    - New mask: 49 features, Fitness: -10.0303\n",
      "    - New mask: 48 features, Fitness: -10.8532\n",
      "    - New mask: 51 features, Fitness: -13.0444\n",
      "    - New mask: 50 features, Fitness: -12.0584\n",
      "    - New mask: 51 features, Fitness: -12.7719\n",
      "    - New mask: 49 features, Fitness: -11.4502\n",
      "    - New mask: 50 features, Fitness: -12.3526\n",
      "    - New mask: 49 features, Fitness: -11.4288\n",
      "    - New mask: 47 features, Fitness: -10.0664\n",
      "    - New mask: 49 features, Fitness: -11.4533\n",
      "    - New mask: 53 features, Fitness: -13.8539\n",
      "    - New mask: 51 features, Fitness: -12.8741\n",
      "    - New mask: 46 features, Fitness: -7.8938\n",
      "    - New mask: 52 features, Fitness: -13.3245\n",
      "    - New mask: 50 features, Fitness: -9.7299\n",
      "    - New mask: 50 features, Fitness: -9.8665\n",
      "    - New mask: 50 features, Fitness: -9.7340\n",
      "    - New mask: 54 features, Fitness: -12.8427\n",
      "    - New mask: 46 features, Fitness: -6.6713\n",
      "    - New mask: 49 features, Fitness: -8.2919\n",
      "    - New mask: 50 features, Fitness: -9.0931\n",
      "    - New mask: 47 features, Fitness: -9.1652\n",
      "    - New mask: 47 features, Fitness: -7.9028\n",
      "    - New mask: 50 features, Fitness: -11.0109\n",
      "    - New mask: 52 features, Fitness: -11.4130\n",
      "    - New mask: 48 features, Fitness: -8.7943\n",
      "    - New mask: 49 features, Fitness: -8.8945\n",
      "    - New mask: 50 features, Fitness: -8.9845\n",
      "    - New mask: 50 features, Fitness: -9.2382\n",
      "    - New mask: 50 features, Fitness: -8.8685\n",
      "    - New mask: 55 features, Fitness: -13.3216\n",
      "    - New mask: 52 features, Fitness: -11.2796\n",
      "    - New mask: 52 features, Fitness: -10.1964\n",
      "    - New mask: 50 features, Fitness: -8.1218\n",
      "    - New mask: 48 features, Fitness: -12.7249\n",
      "    - New mask: 44 features, Fitness: -11.3334\n",
      "    - New mask: 49 features, Fitness: -12.6369\n",
      "    - New mask: 48 features, Fitness: -13.3189\n",
      "    - New mask: 51 features, Fitness: -15.3694\n",
      "    - New mask: 48 features, Fitness: -12.1024\n",
      "    - New mask: 47 features, Fitness: -11.5203\n",
      "    - New mask: 52 features, Fitness: -15.7604\n",
      "    - New mask: 48 features, Fitness: -12.2224\n",
      "    - New mask: 49 features, Fitness: -13.4559\n",
      "    - New mask: 52 features, Fitness: -15.4654\n",
      "    - New mask: 46 features, Fitness: -11.5014\n",
      "    - New mask: 46 features, Fitness: -11.7912\n",
      "    - New mask: 47 features, Fitness: -12.0345\n",
      "    - New mask: 50 features, Fitness: -12.9006\n",
      "    - New mask: 50 features, Fitness: -14.3186\n",
      "    - New mask: 49 features, Fitness: -12.9908\n",
      "    - New mask: 49 features, Fitness: -11.4444\n",
      "    - New mask: 50 features, Fitness: -12.4047\n",
      "    - New mask: 49 features, Fitness: -14.2747\n",
      "    - New mask: 53 features, Fitness: -13.1535\n",
      "    - New mask: 48 features, Fitness: -8.7538\n",
      "    - New mask: 51 features, Fitness: -11.0854\n",
      "    - New mask: 52 features, Fitness: -11.6986\n",
      "    - New mask: 52 features, Fitness: -12.5474\n",
      "    - New mask: 48 features, Fitness: -9.1724\n",
      "    - New mask: 48 features, Fitness: -10.2918\n",
      "    - New mask: 49 features, Fitness: -9.9518\n",
      "    - New mask: 47 features, Fitness: -8.1837\n",
      "    - New mask: 50 features, Fitness: -10.6077\n",
      "    - New mask: 49 features, Fitness: -9.7158\n",
      "    - New mask: 53 features, Fitness: -12.2089\n",
      "    - New mask: 49 features, Fitness: -10.2149\n",
      "    - New mask: 44 features, Fitness: -7.1610\n",
      "    - New mask: 52 features, Fitness: -12.4056\n",
      "    - New mask: 51 features, Fitness: -13.0311\n",
      "    - New mask: 44 features, Fitness: -7.5132\n",
      "    - New mask: 50 features, Fitness: -10.2915\n",
      "    - New mask: 48 features, Fitness: -10.0229\n",
      "    - New mask: 49 features, Fitness: -8.7135\n",
      "    - New mask: 50 features, Fitness: -14.3432\n",
      "    - New mask: 50 features, Fitness: -14.3206\n",
      "    - New mask: 47 features, Fitness: -13.7020\n",
      "    - New mask: 48 features, Fitness: -12.9605\n",
      "    - New mask: 45 features, Fitness: -11.2445\n",
      "    - New mask: 52 features, Fitness: -17.9366\n",
      "    - New mask: 48 features, Fitness: -12.8166\n",
      "    - New mask: 52 features, Fitness: -16.9773\n",
      "    - New mask: 52 features, Fitness: -17.2994\n",
      "    - New mask: 49 features, Fitness: -15.0280\n",
      "    - New mask: 50 features, Fitness: -16.2981\n",
      "    - New mask: 50 features, Fitness: -16.0652\n",
      "    - New mask: 50 features, Fitness: -15.3414\n",
      "    - New mask: 49 features, Fitness: -14.3912\n",
      "    - New mask: 48 features, Fitness: -12.5329\n",
      "    - New mask: 50 features, Fitness: -14.8412\n",
      "    - New mask: 51 features, Fitness: -15.9971\n",
      "    - New mask: 50 features, Fitness: -14.6825\n",
      "    - New mask: 49 features, Fitness: -14.5415\n",
      "    - New mask: 51 features, Fitness: -14.5913\n",
      "    - New mask: 53 features, Fitness: -18.8944\n",
      "    - New mask: 54 features, Fitness: -18.7137\n",
      "    - New mask: 50 features, Fitness: -15.7684\n",
      "    - New mask: 54 features, Fitness: -19.8740\n",
      "    - New mask: 52 features, Fitness: -16.4638\n",
      "    - New mask: 49 features, Fitness: -15.5784\n",
      "    - New mask: 51 features, Fitness: -15.5059\n",
      "    - New mask: 49 features, Fitness: -15.1251\n",
      "    - New mask: 50 features, Fitness: -14.9632\n",
      "    - New mask: 48 features, Fitness: -13.6743\n",
      "    - New mask: 52 features, Fitness: -19.1907\n",
      "    - New mask: 45 features, Fitness: -12.0979\n",
      "    - New mask: 50 features, Fitness: -16.3230\n",
      "    - New mask: 47 features, Fitness: -14.4500\n",
      "    - New mask: 51 features, Fitness: -15.7584\n",
      "    - New mask: 51 features, Fitness: -17.6457\n",
      "    - New mask: 49 features, Fitness: -16.4381\n",
      "    - New mask: 52 features, Fitness: -16.6852\n",
      "    - New mask: 46 features, Fitness: -14.0702\n",
      "    - New mask: 51 features, Fitness: -16.9220\n",
      "    - New mask: 50 features, Fitness: -13.7468\n",
      "    - New mask: 48 features, Fitness: -12.0933\n",
      "    - New mask: 51 features, Fitness: -13.6410\n",
      "    - New mask: 54 features, Fitness: -16.4128\n",
      "    - New mask: 52 features, Fitness: -15.4988\n",
      "    - New mask: 47 features, Fitness: -11.1011\n",
      "    - New mask: 48 features, Fitness: -12.0667\n",
      "    - New mask: 49 features, Fitness: -11.9114\n",
      "    - New mask: 51 features, Fitness: -13.5107\n",
      "    - New mask: 51 features, Fitness: -14.0578\n",
      "    - New mask: 50 features, Fitness: -13.7866\n",
      "    - New mask: 44 features, Fitness: -8.9214\n",
      "    - New mask: 51 features, Fitness: -13.9227\n",
      "    - New mask: 49 features, Fitness: -12.2724\n",
      "    - New mask: 53 features, Fitness: -15.5017\n",
      "    - New mask: 55 features, Fitness: -16.3344\n",
      "    - New mask: 51 features, Fitness: -14.1637\n",
      "    - New mask: 53 features, Fitness: -15.6403\n",
      "    - New mask: 51 features, Fitness: -14.8304\n",
      "    - New mask: 51 features, Fitness: -14.6572\n",
      "=== End of Round 17: Vote mask selects 45 features (rho: 0.71)\n",
      "    Indices: [0, 4, 5, 6, 8, 10, 11, 12, 13, 14, 16, 18, 19, 22, 25, 26, 27, 29, 30, 33, 34, 35, 40, 41, 42, 43, 44, 45, 46, 47, 49, 52, 53, 54, 55, 57, 58, 59, 60, 61, 63, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 18 ================\n",
      "  Adaptive rho for this round: 0.74\n",
      "    - New mask: 46 features, Fitness: -9.9078\n",
      "    - New mask: 46 features, Fitness: -10.4115\n",
      "    - New mask: 46 features, Fitness: -10.0602\n",
      "    - New mask: 48 features, Fitness: -10.8641\n",
      "    - New mask: 49 features, Fitness: -11.9045\n",
      "    - New mask: 52 features, Fitness: -14.7476\n",
      "    - New mask: 47 features, Fitness: -11.3252\n",
      "    - New mask: 49 features, Fitness: -12.0266\n",
      "    - New mask: 50 features, Fitness: -13.1937\n",
      "    - New mask: 48 features, Fitness: -11.3962\n",
      "    - New mask: 50 features, Fitness: -13.1577\n",
      "    - New mask: 47 features, Fitness: -10.7542\n",
      "    - New mask: 47 features, Fitness: -10.7416\n",
      "    - New mask: 46 features, Fitness: -10.2590\n",
      "    - New mask: 48 features, Fitness: -11.2978\n",
      "    - New mask: 49 features, Fitness: -11.9588\n",
      "    - New mask: 46 features, Fitness: -9.5388\n",
      "    - New mask: 46 features, Fitness: -9.5470\n",
      "    - New mask: 45 features, Fitness: -9.2432\n",
      "    - New mask: 47 features, Fitness: -9.8970\n",
      "    - New mask: 45 features, Fitness: -9.0476\n",
      "    - New mask: 45 features, Fitness: -9.2375\n",
      "    - New mask: 48 features, Fitness: -11.3172\n",
      "    - New mask: 47 features, Fitness: -9.9707\n",
      "    - New mask: 46 features, Fitness: -9.3004\n",
      "    - New mask: 44 features, Fitness: -8.2832\n",
      "    - New mask: 45 features, Fitness: -8.6786\n",
      "    - New mask: 47 features, Fitness: -9.4102\n",
      "    - New mask: 46 features, Fitness: -9.2690\n",
      "    - New mask: 46 features, Fitness: -8.6486\n",
      "    - New mask: 43 features, Fitness: -7.3150\n",
      "    - New mask: 48 features, Fitness: -9.7015\n",
      "    - New mask: 48 features, Fitness: -9.8075\n",
      "    - New mask: 52 features, Fitness: -13.0750\n",
      "    - New mask: 44 features, Fitness: -7.8695\n",
      "    - New mask: 47 features, Fitness: -9.6531\n",
      "    - New mask: 46 features, Fitness: -8.8438\n",
      "    - New mask: 48 features, Fitness: -10.5275\n",
      "    - New mask: 47 features, Fitness: -10.4009\n",
      "    - New mask: 49 features, Fitness: -10.5751\n",
      "    - New mask: 46 features, Fitness: -8.2352\n",
      "    - New mask: 44 features, Fitness: -7.3404\n",
      "    - New mask: 50 features, Fitness: -11.4648\n",
      "    - New mask: 52 features, Fitness: -11.4321\n",
      "    - New mask: 45 features, Fitness: -7.6839\n",
      "    - New mask: 47 features, Fitness: -8.7705\n",
      "    - New mask: 46 features, Fitness: -7.8793\n",
      "    - New mask: 50 features, Fitness: -9.9937\n",
      "    - New mask: 47 features, Fitness: -7.8216\n",
      "    - New mask: 44 features, Fitness: -6.7394\n",
      "    - New mask: 48 features, Fitness: -9.4236\n",
      "    - New mask: 48 features, Fitness: -8.9835\n",
      "    - New mask: 45 features, Fitness: -7.3279\n",
      "    - New mask: 46 features, Fitness: -7.4329\n",
      "    - New mask: 48 features, Fitness: -9.3133\n",
      "    - New mask: 46 features, Fitness: -7.7981\n",
      "    - New mask: 48 features, Fitness: -9.2000\n",
      "    - New mask: 43 features, Fitness: -6.7355\n",
      "    - New mask: 48 features, Fitness: -9.8480\n",
      "    - New mask: 46 features, Fitness: -8.0295\n",
      "    - New mask: 47 features, Fitness: -9.2614\n",
      "    - New mask: 45 features, Fitness: -7.7289\n",
      "    - New mask: 47 features, Fitness: -8.7838\n",
      "    - New mask: 47 features, Fitness: -8.3867\n",
      "    - New mask: 46 features, Fitness: -8.3242\n",
      "    - New mask: 50 features, Fitness: -11.1415\n",
      "    - New mask: 49 features, Fitness: -10.0770\n",
      "    - New mask: 45 features, Fitness: -8.4736\n",
      "    - New mask: 48 features, Fitness: -10.1733\n",
      "    - New mask: 43 features, Fitness: -6.8250\n",
      "    - New mask: 48 features, Fitness: -10.5457\n",
      "    - New mask: 41 features, Fitness: -8.1108\n",
      "    - New mask: 49 features, Fitness: -11.1137\n",
      "    - New mask: 41 features, Fitness: -6.1305\n",
      "    - New mask: 48 features, Fitness: -9.1547\n",
      "    - New mask: 43 features, Fitness: -7.2108\n",
      "    - New mask: 48 features, Fitness: -9.4606\n",
      "    - New mask: 47 features, Fitness: -9.2411\n",
      "    - New mask: 45 features, Fitness: -7.9515\n",
      "    - New mask: 48 features, Fitness: -9.6315\n",
      "    - New mask: 48 features, Fitness: -7.0441\n",
      "    - New mask: 48 features, Fitness: -7.3776\n",
      "    - New mask: 48 features, Fitness: -8.8847\n",
      "    - New mask: 49 features, Fitness: -7.8748\n",
      "    - New mask: 46 features, Fitness: -8.0526\n",
      "    - New mask: 44 features, Fitness: -5.7154\n",
      "    - New mask: 50 features, Fitness: -9.1459\n",
      "    - New mask: 41 features, Fitness: -5.1904\n",
      "    - New mask: 43 features, Fitness: -5.6451\n",
      "    - New mask: 48 features, Fitness: -8.3406\n",
      "    - New mask: 49 features, Fitness: -9.5877\n",
      "    - New mask: 43 features, Fitness: -6.0920\n",
      "    - New mask: 50 features, Fitness: -9.1553\n",
      "    - New mask: 47 features, Fitness: -7.3574\n",
      "    - New mask: 49 features, Fitness: -8.4447\n",
      "    - New mask: 48 features, Fitness: -7.7763\n",
      "    - New mask: 45 features, Fitness: -6.1384\n",
      "    - New mask: 47 features, Fitness: -7.5568\n",
      "    - New mask: 46 features, Fitness: -6.4308\n",
      "    - New mask: 45 features, Fitness: -7.5203\n",
      "    - New mask: 46 features, Fitness: -11.3403\n",
      "    - New mask: 42 features, Fitness: -9.2294\n",
      "    - New mask: 48 features, Fitness: -12.1995\n",
      "    - New mask: 48 features, Fitness: -12.2816\n",
      "    - New mask: 47 features, Fitness: -11.3504\n",
      "    - New mask: 46 features, Fitness: -10.2577\n",
      "    - New mask: 49 features, Fitness: -12.1053\n",
      "    - New mask: 50 features, Fitness: -13.1444\n",
      "    - New mask: 44 features, Fitness: -9.9185\n",
      "    - New mask: 41 features, Fitness: -8.4928\n",
      "    - New mask: 43 features, Fitness: -8.8919\n",
      "    - New mask: 43 features, Fitness: -10.2026\n",
      "    - New mask: 44 features, Fitness: -10.4142\n",
      "    - New mask: 46 features, Fitness: -10.3776\n",
      "    - New mask: 40 features, Fitness: -7.9573\n",
      "    - New mask: 48 features, Fitness: -13.0907\n",
      "    - New mask: 43 features, Fitness: -9.3069\n",
      "    - New mask: 44 features, Fitness: -10.5203\n",
      "    - New mask: 48 features, Fitness: -11.0866\n",
      "    - New mask: 43 features, Fitness: -10.2590\n",
      "    - New mask: 50 features, Fitness: -11.8469\n",
      "    - New mask: 43 features, Fitness: -6.6786\n",
      "    - New mask: 42 features, Fitness: -5.8420\n",
      "    - New mask: 44 features, Fitness: -6.7363\n",
      "    - New mask: 48 features, Fitness: -9.5711\n",
      "    - New mask: 47 features, Fitness: -8.4153\n",
      "    - New mask: 46 features, Fitness: -8.5352\n",
      "    - New mask: 46 features, Fitness: -8.4142\n",
      "    - New mask: 43 features, Fitness: -6.3815\n",
      "    - New mask: 48 features, Fitness: -8.9260\n",
      "    - New mask: 48 features, Fitness: -10.0732\n",
      "    - New mask: 51 features, Fitness: -10.3806\n",
      "    - New mask: 44 features, Fitness: -7.0240\n",
      "    - New mask: 47 features, Fitness: -8.8942\n",
      "    - New mask: 45 features, Fitness: -7.7522\n",
      "    - New mask: 47 features, Fitness: -8.6066\n",
      "    - New mask: 46 features, Fitness: -8.5772\n",
      "    - New mask: 48 features, Fitness: -9.1732\n",
      "    - New mask: 43 features, Fitness: -7.2266\n",
      "    - New mask: 46 features, Fitness: -7.6262\n",
      "    - New mask: 48 features, Fitness: -11.8568\n",
      "    - New mask: 50 features, Fitness: -14.5413\n",
      "    - New mask: 44 features, Fitness: -9.6125\n",
      "    - New mask: 50 features, Fitness: -15.9205\n",
      "    - New mask: 44 features, Fitness: -9.8767\n",
      "    - New mask: 53 features, Fitness: -17.5810\n",
      "    - New mask: 48 features, Fitness: -12.7419\n",
      "    - New mask: 50 features, Fitness: -15.3661\n",
      "    - New mask: 50 features, Fitness: -15.1658\n",
      "    - New mask: 46 features, Fitness: -11.5715\n",
      "    - New mask: 48 features, Fitness: -14.5398\n",
      "    - New mask: 45 features, Fitness: -11.8207\n",
      "    - New mask: 48 features, Fitness: -13.5610\n",
      "    - New mask: 45 features, Fitness: -11.1542\n",
      "    - New mask: 42 features, Fitness: -9.2464\n",
      "    - New mask: 49 features, Fitness: -13.6316\n",
      "    - New mask: 46 features, Fitness: -11.2782\n",
      "    - New mask: 47 features, Fitness: -12.7097\n",
      "    - New mask: 47 features, Fitness: -12.8650\n",
      "    - New mask: 44 features, Fitness: -9.0573\n",
      "    - New mask: 49 features, Fitness: -15.2932\n",
      "    - New mask: 46 features, Fitness: -10.8302\n",
      "    - New mask: 48 features, Fitness: -13.1690\n",
      "    - New mask: 47 features, Fitness: -12.7960\n",
      "    - New mask: 45 features, Fitness: -10.4371\n",
      "    - New mask: 49 features, Fitness: -14.4988\n",
      "    - New mask: 48 features, Fitness: -13.4464\n",
      "    - New mask: 50 features, Fitness: -13.7131\n",
      "    - New mask: 44 features, Fitness: -10.8164\n",
      "    - New mask: 47 features, Fitness: -12.3773\n",
      "    - New mask: 49 features, Fitness: -16.1581\n",
      "    - New mask: 44 features, Fitness: -10.0297\n",
      "    - New mask: 48 features, Fitness: -12.8184\n",
      "    - New mask: 46 features, Fitness: -11.3699\n",
      "    - New mask: 44 features, Fitness: -10.6682\n",
      "    - New mask: 46 features, Fitness: -12.3165\n",
      "    - New mask: 45 features, Fitness: -11.4372\n",
      "    - New mask: 43 features, Fitness: -11.0003\n",
      "    - New mask: 45 features, Fitness: -11.6101\n",
      "    - New mask: 47 features, Fitness: -13.3750\n",
      "    - New mask: 49 features, Fitness: -11.6861\n",
      "    - New mask: 42 features, Fitness: -7.5404\n",
      "    - New mask: 45 features, Fitness: -9.6960\n",
      "    - New mask: 51 features, Fitness: -14.7862\n",
      "    - New mask: 47 features, Fitness: -11.1655\n",
      "    - New mask: 41 features, Fitness: -6.7183\n",
      "    - New mask: 53 features, Fitness: -14.6476\n",
      "    - New mask: 46 features, Fitness: -9.9314\n",
      "    - New mask: 45 features, Fitness: -9.5546\n",
      "    - New mask: 50 features, Fitness: -12.0677\n",
      "    - New mask: 44 features, Fitness: -8.9160\n",
      "    - New mask: 42 features, Fitness: -7.7781\n",
      "    - New mask: 49 features, Fitness: -11.6847\n",
      "    - New mask: 47 features, Fitness: -11.3512\n",
      "    - New mask: 48 features, Fitness: -10.7881\n",
      "    - New mask: 50 features, Fitness: -13.3267\n",
      "    - New mask: 43 features, Fitness: -8.7310\n",
      "    - New mask: 47 features, Fitness: -11.4793\n",
      "    - New mask: 46 features, Fitness: -10.4267\n",
      "    - New mask: 47 features, Fitness: -10.8787\n",
      "=== End of Round 18: Vote mask selects 39 features (rho: 0.74)\n",
      "    Indices: [4, 5, 6, 8, 10, 12, 13, 14, 16, 18, 19, 22, 25, 26, 27, 29, 30, 33, 34, 35, 40, 41, 42, 43, 44, 45, 46, 47, 49, 52, 54, 55, 57, 61, 63, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 19 ================\n",
      "  Adaptive rho for this round: 0.77\n",
      "    - New mask: 39 features, Fitness: -5.8917\n",
      "    - New mask: 44 features, Fitness: -7.5615\n",
      "    - New mask: 46 features, Fitness: -9.7734\n",
      "    - New mask: 46 features, Fitness: -9.1979\n",
      "    - New mask: 42 features, Fitness: -7.6421\n",
      "    - New mask: 46 features, Fitness: -9.8815\n",
      "    - New mask: 44 features, Fitness: -9.2717\n",
      "    - New mask: 45 features, Fitness: -9.6198\n",
      "    - New mask: 45 features, Fitness: -9.4373\n",
      "    - New mask: 45 features, Fitness: -8.8123\n",
      "    - New mask: 44 features, Fitness: -8.4250\n",
      "    - New mask: 47 features, Fitness: -10.9492\n",
      "    - New mask: 44 features, Fitness: -9.3019\n",
      "    - New mask: 43 features, Fitness: -7.4639\n",
      "    - New mask: 43 features, Fitness: -8.0769\n",
      "    - New mask: 43 features, Fitness: -8.9647\n",
      "    - New mask: 43 features, Fitness: -7.5172\n",
      "    - New mask: 39 features, Fitness: -6.0471\n",
      "    - New mask: 41 features, Fitness: -6.7135\n",
      "    - New mask: 44 features, Fitness: -9.0627\n",
      "    - New mask: 43 features, Fitness: -7.2900\n",
      "    - New mask: 45 features, Fitness: -9.3693\n",
      "    - New mask: 43 features, Fitness: -7.8839\n",
      "    - New mask: 43 features, Fitness: -8.4354\n",
      "    - New mask: 40 features, Fitness: -5.9629\n",
      "    - New mask: 42 features, Fitness: -7.7082\n",
      "    - New mask: 46 features, Fitness: -9.2640\n",
      "    - New mask: 44 features, Fitness: -8.2543\n",
      "    - New mask: 40 features, Fitness: -6.4198\n",
      "    - New mask: 43 features, Fitness: -7.8901\n",
      "    - New mask: 41 features, Fitness: -6.3050\n",
      "    - New mask: 45 features, Fitness: -8.6257\n",
      "    - New mask: 45 features, Fitness: -8.7786\n",
      "    - New mask: 44 features, Fitness: -8.5094\n",
      "    - New mask: 39 features, Fitness: -5.9713\n",
      "    - New mask: 40 features, Fitness: -6.4329\n",
      "    - New mask: 40 features, Fitness: -5.7078\n",
      "    - New mask: 42 features, Fitness: -6.7998\n",
      "    - New mask: 43 features, Fitness: -7.9489\n",
      "    - New mask: 45 features, Fitness: -8.4805\n",
      "    - New mask: 46 features, Fitness: -9.1591\n",
      "    - New mask: 42 features, Fitness: -6.8246\n",
      "    - New mask: 41 features, Fitness: -6.5931\n",
      "    - New mask: 46 features, Fitness: -7.3820\n",
      "    - New mask: 41 features, Fitness: -5.5721\n",
      "    - New mask: 43 features, Fitness: -6.5431\n",
      "    - New mask: 45 features, Fitness: -8.0182\n",
      "    - New mask: 44 features, Fitness: -6.4424\n",
      "    - New mask: 44 features, Fitness: -6.7409\n",
      "    - New mask: 42 features, Fitness: -6.9435\n",
      "    - New mask: 45 features, Fitness: -7.9303\n",
      "    - New mask: 44 features, Fitness: -6.9868\n",
      "    - New mask: 44 features, Fitness: -7.3474\n",
      "    - New mask: 41 features, Fitness: -5.4473\n",
      "    - New mask: 42 features, Fitness: -6.5181\n",
      "    - New mask: 40 features, Fitness: -4.9980\n",
      "    - New mask: 45 features, Fitness: -6.5467\n",
      "    - New mask: 39 features, Fitness: -5.2315\n",
      "    - New mask: 44 features, Fitness: -7.4927\n",
      "    - New mask: 43 features, Fitness: -7.0684\n",
      "    - New mask: 42 features, Fitness: -6.4246\n",
      "    - New mask: 42 features, Fitness: -5.8974\n",
      "    - New mask: 43 features, Fitness: -6.5427\n",
      "    - New mask: 42 features, Fitness: -6.8203\n",
      "    - New mask: 46 features, Fitness: -7.4729\n",
      "    - New mask: 43 features, Fitness: -6.6026\n",
      "    - New mask: 42 features, Fitness: -6.3658\n",
      "    - New mask: 43 features, Fitness: -7.7222\n",
      "    - New mask: 42 features, Fitness: -7.7962\n",
      "    - New mask: 41 features, Fitness: -5.8947\n",
      "    - New mask: 42 features, Fitness: -6.4532\n",
      "    - New mask: 39 features, Fitness: -5.4742\n",
      "    - New mask: 43 features, Fitness: -7.2023\n",
      "    - New mask: 39 features, Fitness: -5.4061\n",
      "    - New mask: 46 features, Fitness: -7.7399\n",
      "    - New mask: 43 features, Fitness: -6.9145\n",
      "    - New mask: 43 features, Fitness: -7.1013\n",
      "    - New mask: 42 features, Fitness: -5.8943\n",
      "    - New mask: 42 features, Fitness: -7.1914\n",
      "    - New mask: 40 features, Fitness: -6.4061\n",
      "    - New mask: 41 features, Fitness: -4.0575\n",
      "    - New mask: 42 features, Fitness: -4.8760\n",
      "    - New mask: 46 features, Fitness: -7.1485\n",
      "    - New mask: 44 features, Fitness: -5.8331\n",
      "    - New mask: 39 features, Fitness: -3.3068\n",
      "    - New mask: 42 features, Fitness: -4.8130\n",
      "    - New mask: 41 features, Fitness: -4.7450\n",
      "    - New mask: 41 features, Fitness: -4.7628\n",
      "    - New mask: 41 features, Fitness: -4.6551\n",
      "    - New mask: 43 features, Fitness: -4.7666\n",
      "    - New mask: 43 features, Fitness: -6.5249\n",
      "    - New mask: 42 features, Fitness: -6.1536\n",
      "    - New mask: 41 features, Fitness: -4.7425\n",
      "    - New mask: 44 features, Fitness: -6.4181\n",
      "    - New mask: 42 features, Fitness: -5.5970\n",
      "    - New mask: 41 features, Fitness: -4.3999\n",
      "    - New mask: 38 features, Fitness: -3.5621\n",
      "    - New mask: 43 features, Fitness: -5.2255\n",
      "    - New mask: 44 features, Fitness: -5.8706\n",
      "    - New mask: 40 features, Fitness: -4.1921\n",
      "    - New mask: 41 features, Fitness: -8.3303\n",
      "    - New mask: 43 features, Fitness: -9.3662\n",
      "    - New mask: 41 features, Fitness: -7.5996\n",
      "    - New mask: 42 features, Fitness: -8.4597\n",
      "    - New mask: 43 features, Fitness: -8.3837\n",
      "    - New mask: 42 features, Fitness: -7.4916\n",
      "    - New mask: 47 features, Fitness: -11.7220\n",
      "    - New mask: 42 features, Fitness: -7.9303\n",
      "    - New mask: 40 features, Fitness: -7.5657\n",
      "    - New mask: 40 features, Fitness: -8.0564\n",
      "    - New mask: 45 features, Fitness: -9.9142\n",
      "    - New mask: 39 features, Fitness: -7.7350\n",
      "    - New mask: 42 features, Fitness: -8.1334\n",
      "    - New mask: 43 features, Fitness: -9.2868\n",
      "    - New mask: 37 features, Fitness: -6.9381\n",
      "    - New mask: 46 features, Fitness: -11.6502\n",
      "    - New mask: 40 features, Fitness: -7.8129\n",
      "    - New mask: 42 features, Fitness: -8.5244\n",
      "    - New mask: 45 features, Fitness: -9.9117\n",
      "    - New mask: 39 features, Fitness: -8.5008\n",
      "    - New mask: 39 features, Fitness: -4.2026\n",
      "    - New mask: 39 features, Fitness: -4.4386\n",
      "    - New mask: 40 features, Fitness: -4.4171\n",
      "    - New mask: 44 features, Fitness: -7.3115\n",
      "    - New mask: 45 features, Fitness: -7.3707\n",
      "    - New mask: 44 features, Fitness: -6.3748\n",
      "    - New mask: 46 features, Fitness: -8.1303\n",
      "    - New mask: 41 features, Fitness: -5.2812\n",
      "    - New mask: 41 features, Fitness: -4.8720\n",
      "    - New mask: 43 features, Fitness: -6.0389\n",
      "    - New mask: 42 features, Fitness: -5.8541\n",
      "    - New mask: 46 features, Fitness: -7.7013\n",
      "    - New mask: 43 features, Fitness: -6.5725\n",
      "    - New mask: 46 features, Fitness: -9.1200\n",
      "    - New mask: 39 features, Fitness: -5.8470\n",
      "    - New mask: 40 features, Fitness: -5.7360\n",
      "    - New mask: 42 features, Fitness: -6.3643\n",
      "    - New mask: 47 features, Fitness: -8.7225\n",
      "    - New mask: 40 features, Fitness: -5.4289\n",
      "    - New mask: 42 features, Fitness: -5.6096\n",
      "    - New mask: 44 features, Fitness: -9.3067\n",
      "    - New mask: 43 features, Fitness: -9.9956\n",
      "    - New mask: 41 features, Fitness: -8.4601\n",
      "    - New mask: 44 features, Fitness: -11.5159\n",
      "    - New mask: 36 features, Fitness: -5.6997\n",
      "    - New mask: 42 features, Fitness: -8.9509\n",
      "    - New mask: 44 features, Fitness: -10.8528\n",
      "    - New mask: 43 features, Fitness: -9.0360\n",
      "    - New mask: 44 features, Fitness: -9.4258\n",
      "    - New mask: 45 features, Fitness: -10.5944\n",
      "    - New mask: 47 features, Fitness: -12.3332\n",
      "    - New mask: 45 features, Fitness: -11.6226\n",
      "    - New mask: 39 features, Fitness: -6.8702\n",
      "    - New mask: 44 features, Fitness: -10.6870\n",
      "    - New mask: 40 features, Fitness: -7.2782\n",
      "    - New mask: 47 features, Fitness: -11.1244\n",
      "    - New mask: 45 features, Fitness: -10.4100\n",
      "    - New mask: 43 features, Fitness: -10.7628\n",
      "    - New mask: 40 features, Fitness: -7.7706\n",
      "    - New mask: 40 features, Fitness: -7.1403\n",
      "    - New mask: 40 features, Fitness: -8.6878\n",
      "    - New mask: 41 features, Fitness: -7.5325\n",
      "    - New mask: 43 features, Fitness: -9.8171\n",
      "    - New mask: 44 features, Fitness: -10.4276\n",
      "    - New mask: 44 features, Fitness: -10.1215\n",
      "    - New mask: 48 features, Fitness: -13.5146\n",
      "    - New mask: 42 features, Fitness: -8.8841\n",
      "    - New mask: 43 features, Fitness: -8.8417\n",
      "    - New mask: 40 features, Fitness: -7.4258\n",
      "    - New mask: 39 features, Fitness: -6.6169\n",
      "    - New mask: 43 features, Fitness: -9.7597\n",
      "    - New mask: 43 features, Fitness: -9.0926\n",
      "    - New mask: 45 features, Fitness: -9.3425\n",
      "    - New mask: 40 features, Fitness: -7.2446\n",
      "    - New mask: 40 features, Fitness: -9.1737\n",
      "    - New mask: 41 features, Fitness: -8.6453\n",
      "    - New mask: 39 features, Fitness: -7.5463\n",
      "    - New mask: 39 features, Fitness: -7.1592\n",
      "    - New mask: 45 features, Fitness: -10.9133\n",
      "    - New mask: 43 features, Fitness: -9.8601\n",
      "    - New mask: 43 features, Fitness: -8.1494\n",
      "    - New mask: 45 features, Fitness: -8.8024\n",
      "    - New mask: 42 features, Fitness: -7.7517\n",
      "    - New mask: 38 features, Fitness: -5.9879\n",
      "    - New mask: 44 features, Fitness: -9.6495\n",
      "    - New mask: 41 features, Fitness: -7.3812\n",
      "    - New mask: 38 features, Fitness: -5.1898\n",
      "    - New mask: 41 features, Fitness: -6.3279\n",
      "    - New mask: 41 features, Fitness: -6.6815\n",
      "    - New mask: 40 features, Fitness: -6.5389\n",
      "    - New mask: 41 features, Fitness: -7.5053\n",
      "    - New mask: 39 features, Fitness: -6.1515\n",
      "    - New mask: 46 features, Fitness: -9.9727\n",
      "    - New mask: 43 features, Fitness: -7.8235\n",
      "    - New mask: 43 features, Fitness: -8.2363\n",
      "    - New mask: 42 features, Fitness: -7.7636\n",
      "    - New mask: 39 features, Fitness: -5.6826\n",
      "    - New mask: 46 features, Fitness: -9.8190\n",
      "    - New mask: 45 features, Fitness: -8.8127\n",
      "    - New mask: 38 features, Fitness: -5.5273\n",
      "=== End of Round 19: Vote mask selects 34 features (rho: 0.77)\n",
      "    Indices: [4, 5, 6, 10, 12, 13, 14, 16, 18, 19, 25, 26, 27, 29, 30, 33, 34, 35, 40, 41, 43, 44, 45, 46, 47, 49, 52, 55, 61, 63, 64, 65, 68, 69]\n",
      "\n",
      "================ Federated BFA Round 20 ================\n",
      "  Adaptive rho for this round: 0.80\n",
      "    - New mask: 39 features, Fitness: -5.3119\n",
      "    - New mask: 39 features, Fitness: -5.8466\n",
      "    - New mask: 39 features, Fitness: -5.6024\n",
      "    - New mask: 42 features, Fitness: -6.9751\n",
      "    - New mask: 41 features, Fitness: -7.1878\n",
      "    - New mask: 41 features, Fitness: -7.0958\n",
      "    - New mask: 37 features, Fitness: -5.1180\n",
      "    - New mask: 41 features, Fitness: -6.8156\n",
      "    - New mask: 39 features, Fitness: -7.1023\n",
      "    - New mask: 41 features, Fitness: -6.8256\n",
      "    - New mask: 41 features, Fitness: -6.2160\n",
      "    - New mask: 39 features, Fitness: -6.6236\n",
      "    - New mask: 37 features, Fitness: -5.2360\n",
      "    - New mask: 40 features, Fitness: -6.4187\n",
      "    - New mask: 38 features, Fitness: -5.9681\n",
      "    - New mask: 40 features, Fitness: -6.8502\n",
      "    - New mask: 41 features, Fitness: -6.6658\n",
      "    - New mask: 40 features, Fitness: -6.6201\n",
      "    - New mask: 37 features, Fitness: -4.9837\n",
      "    - New mask: 38 features, Fitness: -5.1734\n",
      "    - New mask: 38 features, Fitness: -4.7231\n",
      "    - New mask: 39 features, Fitness: -6.8306\n",
      "    - New mask: 37 features, Fitness: -4.5315\n",
      "    - New mask: 40 features, Fitness: -6.1343\n",
      "    - New mask: 37 features, Fitness: -4.6470\n",
      "    - New mask: 35 features, Fitness: -4.1770\n",
      "    - New mask: 44 features, Fitness: -8.6226\n",
      "    - New mask: 38 features, Fitness: -6.5925\n",
      "    - New mask: 38 features, Fitness: -4.9711\n",
      "    - New mask: 35 features, Fitness: -4.7842\n",
      "    - New mask: 38 features, Fitness: -5.8104\n",
      "    - New mask: 38 features, Fitness: -4.9217\n",
      "    - New mask: 39 features, Fitness: -5.7593\n",
      "    - New mask: 44 features, Fitness: -9.0150\n",
      "    - New mask: 38 features, Fitness: -6.3227\n",
      "    - New mask: 38 features, Fitness: -6.3729\n",
      "    - New mask: 35 features, Fitness: -4.8561\n",
      "    - New mask: 34 features, Fitness: -4.2418\n",
      "    - New mask: 37 features, Fitness: -5.4696\n",
      "    - New mask: 42 features, Fitness: -6.6959\n",
      "    - New mask: 34 features, Fitness: -3.5626\n",
      "    - New mask: 36 features, Fitness: -3.9260\n",
      "    - New mask: 38 features, Fitness: -5.3702\n",
      "    - New mask: 41 features, Fitness: -5.2421\n",
      "    - New mask: 39 features, Fitness: -4.3674\n",
      "    - New mask: 44 features, Fitness: -7.2785\n",
      "    - New mask: 37 features, Fitness: -4.3394\n",
      "    - New mask: 39 features, Fitness: -4.3451\n",
      "    - New mask: 38 features, Fitness: -4.2971\n",
      "    - New mask: 44 features, Fitness: -7.6742\n",
      "    - New mask: 38 features, Fitness: -4.4625\n",
      "    - New mask: 39 features, Fitness: -5.1145\n",
      "    - New mask: 39 features, Fitness: -5.0325\n",
      "    - New mask: 37 features, Fitness: -3.7175\n",
      "    - New mask: 42 features, Fitness: -6.2041\n",
      "    - New mask: 36 features, Fitness: -3.6717\n",
      "    - New mask: 36 features, Fitness: -3.4715\n",
      "    - New mask: 35 features, Fitness: -3.3021\n",
      "    - New mask: 38 features, Fitness: -4.4566\n",
      "    - New mask: 40 features, Fitness: -4.9403\n",
      "    - New mask: 40 features, Fitness: -5.4745\n",
      "    - New mask: 40 features, Fitness: -5.4415\n",
      "    - New mask: 39 features, Fitness: -5.1582\n",
      "    - New mask: 36 features, Fitness: -4.2906\n",
      "    - New mask: 38 features, Fitness: -4.6532\n",
      "    - New mask: 38 features, Fitness: -4.9220\n",
      "    - New mask: 35 features, Fitness: -3.4551\n",
      "    - New mask: 38 features, Fitness: -5.0001\n",
      "    - New mask: 42 features, Fitness: -6.2858\n",
      "    - New mask: 40 features, Fitness: -5.3434\n",
      "    - New mask: 39 features, Fitness: -5.9556\n",
      "    - New mask: 37 features, Fitness: -4.4299\n",
      "    - New mask: 39 features, Fitness: -4.8651\n",
      "    - New mask: 35 features, Fitness: -3.6293\n",
      "    - New mask: 42 features, Fitness: -5.8727\n",
      "    - New mask: 39 features, Fitness: -4.6772\n",
      "    - New mask: 41 features, Fitness: -5.6873\n",
      "    - New mask: 39 features, Fitness: -4.6523\n",
      "    - New mask: 39 features, Fitness: -5.6180\n",
      "    - New mask: 37 features, Fitness: -4.9199\n",
      "    - New mask: 40 features, Fitness: -3.3205\n",
      "    - New mask: 40 features, Fitness: -4.0047\n",
      "    - New mask: 36 features, Fitness: -3.0978\n",
      "    - New mask: 40 features, Fitness: -3.7762\n",
      "    - New mask: 31 features, Fitness: -1.6276\n",
      "    - New mask: 43 features, Fitness: -5.1966\n",
      "    - New mask: 43 features, Fitness: -4.6382\n",
      "    - New mask: 36 features, Fitness: -2.5161\n",
      "    - New mask: 37 features, Fitness: -2.8916\n",
      "    - New mask: 43 features, Fitness: -5.5680\n",
      "    - New mask: 37 features, Fitness: -2.8989\n",
      "    - New mask: 41 features, Fitness: -4.2281\n",
      "    - New mask: 38 features, Fitness: -2.8382\n",
      "    - New mask: 42 features, Fitness: -5.7276\n",
      "    - New mask: 39 features, Fitness: -3.5459\n",
      "    - New mask: 37 features, Fitness: -2.1534\n",
      "    - New mask: 35 features, Fitness: -2.1354\n",
      "    - New mask: 39 features, Fitness: -3.5455\n",
      "    - New mask: 39 features, Fitness: -4.4607\n",
      "    - New mask: 33 features, Fitness: -1.1270\n",
      "    - New mask: 40 features, Fitness: -7.2669\n",
      "    - New mask: 37 features, Fitness: -5.5408\n",
      "    - New mask: 38 features, Fitness: -5.6533\n",
      "    - New mask: 35 features, Fitness: -5.2599\n",
      "    - New mask: 37 features, Fitness: -6.3273\n",
      "    - New mask: 39 features, Fitness: -7.5699\n",
      "    - New mask: 41 features, Fitness: -8.1131\n",
      "    - New mask: 40 features, Fitness: -7.1819\n",
      "    - New mask: 36 features, Fitness: -5.9370\n",
      "    - New mask: 40 features, Fitness: -7.6794\n",
      "    - New mask: 39 features, Fitness: -5.7079\n",
      "    - New mask: 37 features, Fitness: -6.1477\n",
      "    - New mask: 36 features, Fitness: -5.4202\n",
      "    - New mask: 40 features, Fitness: -7.5123\n",
      "    - New mask: 36 features, Fitness: -6.4397\n",
      "    - New mask: 40 features, Fitness: -7.5122\n",
      "    - New mask: 37 features, Fitness: -6.1550\n",
      "    - New mask: 38 features, Fitness: -6.7660\n",
      "    - New mask: 37 features, Fitness: -6.0190\n",
      "    - New mask: 36 features, Fitness: -6.1536\n",
      "    - New mask: 36 features, Fitness: -3.3673\n",
      "    - New mask: 39 features, Fitness: -5.0521\n",
      "    - New mask: 36 features, Fitness: -3.3837\n",
      "    - New mask: 39 features, Fitness: -4.8098\n",
      "    - New mask: 37 features, Fitness: -4.2428\n",
      "    - New mask: 38 features, Fitness: -4.1664\n",
      "    - New mask: 40 features, Fitness: -4.7680\n",
      "    - New mask: 40 features, Fitness: -5.8865\n",
      "    - New mask: 39 features, Fitness: -3.8940\n",
      "    - New mask: 41 features, Fitness: -5.8376\n",
      "    - New mask: 39 features, Fitness: -5.3472\n",
      "    - New mask: 39 features, Fitness: -4.9592\n",
      "    - New mask: 37 features, Fitness: -4.3120\n",
      "    - New mask: 40 features, Fitness: -4.9899\n",
      "    - New mask: 36 features, Fitness: -4.4119\n",
      "    - New mask: 36 features, Fitness: -4.4714\n",
      "    - New mask: 36 features, Fitness: -3.5150\n",
      "    - New mask: 38 features, Fitness: -4.6609\n",
      "    - New mask: 36 features, Fitness: -3.7893\n",
      "    - New mask: 42 features, Fitness: -5.7210\n",
      "    - New mask: 42 features, Fitness: -8.2110\n",
      "    - New mask: 40 features, Fitness: -7.8590\n",
      "    - New mask: 38 features, Fitness: -6.2553\n",
      "    - New mask: 40 features, Fitness: -8.4604\n",
      "    - New mask: 37 features, Fitness: -6.3351\n",
      "    - New mask: 37 features, Fitness: -6.7894\n",
      "    - New mask: 39 features, Fitness: -7.4304\n",
      "    - New mask: 36 features, Fitness: -5.3824\n",
      "    - New mask: 41 features, Fitness: -8.9555\n",
      "    - New mask: 38 features, Fitness: -5.9877\n",
      "    - New mask: 41 features, Fitness: -8.4639\n",
      "    - New mask: 39 features, Fitness: -7.0320\n",
      "    - New mask: 32 features, Fitness: -3.8235\n",
      "    - New mask: 36 features, Fitness: -5.6761\n",
      "    - New mask: 35 features, Fitness: -6.0462\n",
      "    - New mask: 38 features, Fitness: -6.8230\n",
      "    - New mask: 36 features, Fitness: -6.0501\n",
      "    - New mask: 41 features, Fitness: -8.4932\n",
      "    - New mask: 36 features, Fitness: -5.4359\n",
      "    - New mask: 35 features, Fitness: -5.2840\n",
      "    - New mask: 37 features, Fitness: -6.3674\n",
      "    - New mask: 39 features, Fitness: -6.5997\n",
      "    - New mask: 36 features, Fitness: -5.6182\n",
      "    - New mask: 37 features, Fitness: -5.4174\n",
      "    - New mask: 40 features, Fitness: -7.3411\n",
      "    - New mask: 41 features, Fitness: -7.7930\n",
      "    - New mask: 39 features, Fitness: -6.4954\n",
      "    - New mask: 42 features, Fitness: -7.8287\n",
      "    - New mask: 39 features, Fitness: -6.4331\n",
      "    - New mask: 38 features, Fitness: -6.3563\n",
      "    - New mask: 37 features, Fitness: -6.6697\n",
      "    - New mask: 41 features, Fitness: -7.8439\n",
      "    - New mask: 38 features, Fitness: -6.2106\n",
      "    - New mask: 38 features, Fitness: -5.7478\n",
      "    - New mask: 34 features, Fitness: -4.8627\n",
      "    - New mask: 43 features, Fitness: -8.9339\n",
      "    - New mask: 41 features, Fitness: -7.6700\n",
      "    - New mask: 35 features, Fitness: -5.1273\n",
      "    - New mask: 42 features, Fitness: -7.7917\n",
      "    - New mask: 40 features, Fitness: -8.2840\n",
      "    - New mask: 40 features, Fitness: -6.9117\n",
      "    - New mask: 37 features, Fitness: -4.3716\n",
      "    - New mask: 38 features, Fitness: -5.2838\n",
      "    - New mask: 40 features, Fitness: -7.3673\n",
      "    - New mask: 38 features, Fitness: -7.3718\n",
      "    - New mask: 37 features, Fitness: -4.9366\n",
      "    - New mask: 38 features, Fitness: -5.1718\n",
      "    - New mask: 38 features, Fitness: -4.7588\n",
      "    - New mask: 34 features, Fitness: -4.1004\n",
      "    - New mask: 36 features, Fitness: -4.8629\n",
      "    - New mask: 38 features, Fitness: -6.2366\n",
      "    - New mask: 38 features, Fitness: -5.3778\n",
      "    - New mask: 38 features, Fitness: -5.1937\n",
      "    - New mask: 38 features, Fitness: -5.0638\n",
      "    - New mask: 39 features, Fitness: -5.3829\n",
      "    - New mask: 38 features, Fitness: -5.6925\n",
      "    - New mask: 34 features, Fitness: -4.4241\n",
      "    - New mask: 41 features, Fitness: -6.9706\n",
      "    - New mask: 40 features, Fitness: -7.0175\n",
      "    - New mask: 38 features, Fitness: -5.3887\n",
      "=== End of Round 20: Vote mask selects 32 features (rho: 0.80)\n",
      "    Indices: [4, 5, 6, 10, 12, 13, 14, 16, 18, 25, 26, 27, 29, 30, 33, 34, 35, 40, 41, 43, 44, 45, 46, 47, 49, 52, 55, 61, 64, 65, 68, 69]\n",
      "\n",
      "Final federated feature count: 32\n",
      "Selected feature names: ['Total Length of Bwd Packet', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Bwd Packet Length Min', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Std', 'Flow IAT Min', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Packet Length Std', 'Packet Length Variance', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'ECE Flag Count', 'Fwd Segment Size Avg', 'Fwd Packet/Bulk Avg', 'Subflow Fwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Active Mean', 'Active Std']\n"
     ]
    }
   ],
   "source": [
    "n_feat_select_rounds = 20\n",
    "n_fireflies = 20           # Number of fireflies per client\n",
    "n_features = X.shape[1]\n",
    "num_clients = len(client_data_np)\n",
    "rho_start, rho_end = 0.2, 0.8\n",
    "penalty_lambda = 0.9\n",
    "\n",
    "# Precompute Fisher scores and correlation matrix for each client\n",
    "client_fisher_scores = []\n",
    "client_corr_matrix = []\n",
    "for Xc, yc in client_data_np:\n",
    "    fisher_scores = compute_fisher_scores(Xc, yc)\n",
    "    corr_matrix = compute_corr_matrix(Xc)\n",
    "    client_fisher_scores.append(fisher_scores)\n",
    "    client_corr_matrix.append(corr_matrix)\n",
    "\n",
    "# Initialize fireflies for each client at round 1\n",
    "client_fireflies = []\n",
    "client_local_bests = []\n",
    "for cid in range(num_clients):\n",
    "    fireflies = []\n",
    "    for _ in range(n_fireflies):\n",
    "        mask = np.random.choice([0, 1], size=n_features)\n",
    "        if np.sum(mask) == 0:\n",
    "            mask[np.random.randint(n_features)] = 1  # Ensure at least one feature is selected\n",
    "        fireflies.append(mask)\n",
    "    # Evaluate and store best\n",
    "    best_fitness = -np.inf\n",
    "    best_mask = None\n",
    "    for mask in fireflies:\n",
    "        sel = np.where(mask)[0]\n",
    "        fit = evaluate_feature_subset(sel, client_fisher_scores[cid], client_corr_matrix[cid], penalty_lambda)\n",
    "        if fit > best_fitness or best_mask is None:\n",
    "            best_fitness = fit\n",
    "            best_mask = mask.copy()\n",
    "    # Fallback: all features if somehow none was found\n",
    "    if best_mask is None:\n",
    "        best_mask = np.ones(n_features, dtype=int)\n",
    "    client_fireflies.append(fireflies)\n",
    "    client_local_bests.append(best_mask.copy())\n",
    "\n",
    "# Start with all features selected in global mask\n",
    "global_mask = np.ones(n_features, dtype=int)\n",
    "\n",
    "for round_fs in range(n_feat_select_rounds):\n",
    "    print(f\"\\n================ Federated BFA Round {round_fs+1} ================\")\n",
    "    # Linear schedule for rho\n",
    "    rho = rho_start + (rho_end - rho_start) * (round_fs / (n_feat_select_rounds - 1))\n",
    "    print(f\"  Adaptive rho for this round: {rho:.2f}\")\n",
    "\n",
    "    client_best_masks = []\n",
    "    # For each client, update fireflies and find new local best\n",
    "    for cid in range(num_clients):\n",
    "        fireflies = client_fireflies[cid]\n",
    "        fisher_scores = client_fisher_scores[cid]\n",
    "        corr_matrix = client_corr_matrix[cid]\n",
    "        local_best = client_local_bests[cid]\n",
    "        new_fireflies = []\n",
    "        best_fitness = -np.inf\n",
    "        best_mask = None\n",
    "        for f in range(n_fireflies):\n",
    "            new_mask = one_step_binary_firefly(\n",
    "                fireflies[f],\n",
    "                global_mask,\n",
    "                local_best,\n",
    "                fisher_scores,\n",
    "                corr_matrix,\n",
    "                penalty_lambda=penalty_lambda,\n",
    "                verbose=True\n",
    "            )\n",
    "            # Ensure at least one feature\n",
    "            if np.sum(new_mask) == 0:\n",
    "                new_mask[np.random.randint(n_features)] = 1\n",
    "            new_fireflies.append(new_mask)\n",
    "            sel = np.where(new_mask)[0]\n",
    "            fit = evaluate_feature_subset(sel, fisher_scores, corr_matrix, penalty_lambda)\n",
    "            if fit > best_fitness or best_mask is None:\n",
    "                best_fitness = fit\n",
    "                best_mask = new_mask.copy()\n",
    "        # Fallback: all features if somehow none was found\n",
    "        if best_mask is None:\n",
    "            best_mask = np.ones(n_features, dtype=int)\n",
    "        # Update client's fireflies and local best\n",
    "        client_fireflies[cid] = new_fireflies\n",
    "        client_local_bests[cid] = best_mask.copy()\n",
    "        client_best_masks.append(best_mask.copy())\n",
    "    client_best_masks = np.array(client_best_masks)\n",
    "    vote_counts = np.sum(client_best_masks, axis=0)\n",
    "    vote_mask = (vote_counts >= (rho * num_clients)).astype(int)\n",
    "    print(f\"=== End of Round {round_fs+1}: Vote mask selects {vote_mask.sum()} features (rho: {rho:.2f})\\n\"\n",
    "          f\"    Indices: {np.where(vote_mask)[0].tolist()}\")\n",
    "    global_mask = vote_mask.copy()\n",
    "\n",
    "selected_indices = np.where(global_mask == 1)[0]\n",
    "print(f\"\\nFinal federated feature count: {len(selected_indices)}\")\n",
    "selected_feature_names = [feature_cols[i] for i in selected_indices]\n",
    "print(\"Selected feature names:\", selected_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final federated feature count: 32\n",
      "Selected feature names: ['Total Length of Bwd Packet', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Bwd Packet Length Min', 'Bwd Packet Length Std', 'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Std', 'Flow IAT Min', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd Header Length', 'Bwd Header Length', 'Fwd Packets/s', 'Packet Length Std', 'Packet Length Variance', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count', 'URG Flag Count', 'ECE Flag Count', 'Fwd Segment Size Avg', 'Fwd Packet/Bulk Avg', 'Subflow Fwd Bytes', 'FWD Init Win Bytes', 'Bwd Init Win Bytes', 'Active Mean', 'Active Std']\n"
     ]
    }
   ],
   "source": [
    "selected_indices = np.where(global_mask == 1)[0]\n",
    "print(f\"\\nFinal federated feature count: {len(selected_indices)}\")\n",
    "selected_feature_names = [feature_cols[i] for i in selected_indices]\n",
    "print(\"Selected feature names:\", selected_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel = X[:, selected_indices]\n",
    "input_dim = X_sel.shape[1]\n",
    "full_dataset = TabularDataset(X_sel, y)\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "client_loaders = []\n",
    "for i in range(num_clients):\n",
    "    idxs = client_indices[i]\n",
    "    client_subset = Subset(train_dataset, idxs)\n",
    "    client_loader = DataLoader(client_subset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    client_loaders.append(client_loader)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=2):\n",
    "        super(TabularMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        features = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(features)\n",
    "        out = self.fc3(x)\n",
    "        if return_features:\n",
    "            return out, features\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Federated Round 1 (Local Epochs: 10)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 37.05% | Acc After: 99.53%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 61.68% | Acc After: 98.19%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 89.79% | Acc After: 99.23%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 58.35% | Acc After: 97.65%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 46.29% | Acc After: 97.64%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 80.37% | Acc After: 99.31%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 54.61% | Acc After: 99.11%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 61.34% | Acc After: 99.83%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 92.27% | Acc After: 99.05%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 87.39% | Acc After: 98.79%\n",
      "\n",
      "[Round 1] Global Test Accuracy: 95.55%\n",
      "Client Acc BEFORE (mean ± std): 66.91% ± 18.35%\n",
      "Client Acc AFTER  (mean ± std): 98.83% ± 0.72%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 2 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 95.61% | Acc After: 99.55%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 91.05% | Acc After: 98.20%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 98.87% | Acc After: 99.37%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 94.52% | Acc After: 97.96%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 92.79% | Acc After: 97.57%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 97.17% | Acc After: 99.42%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 89.55% | Acc After: 99.32%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 97.74% | Acc After: 99.85%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 98.73% | Acc After: 99.45%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 97.41% | Acc After: 98.93%\n",
      "\n",
      "[Round 2] Global Test Accuracy: 97.49%\n",
      "Client Acc BEFORE (mean ± std): 95.34% ± 3.11%\n",
      "Client Acc AFTER  (mean ± std): 98.96% ± 0.73%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 3 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 98.33% | Acc After: 99.57%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.05% | Acc After: 98.30%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.18% | Acc After: 99.40%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.05% | Acc After: 98.09%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 96.32% | Acc After: 97.70%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 98.95% | Acc After: 99.42%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.17% | Acc After: 99.28%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.00% | Acc After: 99.87%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.30% | Acc After: 99.48%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.50% | Acc After: 99.00%\n",
      "\n",
      "[Round 3] Global Test Accuracy: 97.71%\n",
      "Client Acc BEFORE (mean ± std): 97.38% ± 2.10%\n",
      "Client Acc AFTER  (mean ± std): 99.01% ± 0.69%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 4 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 98.81% | Acc After: 99.59%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.51% | Acc After: 98.28%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.21% | Acc After: 99.41%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.37% | Acc After: 97.79%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 96.78% | Acc After: 97.75%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.01% | Acc After: 99.46%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.75% | Acc After: 99.20%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.21% | Acc After: 99.87%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.29% | Acc After: 99.48%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.56% | Acc After: 98.95%\n",
      "\n",
      "[Round 4] Global Test Accuracy: 97.75%\n",
      "Client Acc BEFORE (mean ± std): 97.65% ± 1.94%\n",
      "Client Acc AFTER  (mean ± std): 98.98% ± 0.73%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 5 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.15% | Acc After: 99.61%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.27% | Acc After: 98.29%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.20% | Acc After: 99.41%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.43% | Acc After: 98.00%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 96.88% | Acc After: 97.71%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.12% | Acc After: 99.50%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.52% | Acc After: 99.27%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.42% | Acc After: 99.85%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.28% | Acc After: 99.49%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.53% | Acc After: 99.06%\n",
      "\n",
      "[Round 5] Global Test Accuracy: 97.78%\n",
      "Client Acc BEFORE (mean ± std): 97.68% ± 2.06%\n",
      "Client Acc AFTER  (mean ± std): 99.02% ± 0.71%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 6 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.22% | Acc After: 99.58%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.22% | Acc After: 98.31%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.24% | Acc After: 99.44%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.48% | Acc After: 98.05%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.01% | Acc After: 97.90%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.29% | Acc After: 99.49%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.43% | Acc After: 99.28%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.44% | Acc After: 99.87%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.35% | Acc After: 99.52%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.62% | Acc After: 98.97%\n",
      "\n",
      "[Round 6] Global Test Accuracy: 97.82%\n",
      "Client Acc BEFORE (mean ± std): 97.73% ± 2.11%\n",
      "Client Acc AFTER  (mean ± std): 99.04% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 7 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.20% | Acc After: 99.60%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.44% | Acc After: 98.31%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.23% | Acc After: 99.45%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.54% | Acc After: 98.09%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.03% | Acc After: 97.66%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.19% | Acc After: 99.49%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.72% | Acc After: 99.40%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.46% | Acc After: 99.88%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.32% | Acc After: 99.60%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.60% | Acc After: 99.05%\n",
      "\n",
      "[Round 7] Global Test Accuracy: 97.87%\n",
      "Client Acc BEFORE (mean ± std): 97.77% ± 2.01%\n",
      "Client Acc AFTER  (mean ± std): 99.05% ± 0.72%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 8 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.25% | Acc After: 99.60%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.41% | Acc After: 98.29%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.25% | Acc After: 99.41%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.58% | Acc After: 97.98%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.12% | Acc After: 97.79%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.31% | Acc After: 99.54%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.65% | Acc After: 99.27%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.45% | Acc After: 99.86%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.37% | Acc After: 99.56%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.67% | Acc After: 99.05%\n",
      "\n",
      "[Round 8] Global Test Accuracy: 97.90%\n",
      "Client Acc BEFORE (mean ± std): 97.81% ± 2.04%\n",
      "Client Acc AFTER  (mean ± std): 99.03% ± 0.70%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 9 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.25% | Acc After: 99.60%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.53% | Acc After: 98.30%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.25% | Acc After: 99.45%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.61% | Acc After: 98.11%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.19% | Acc After: 97.86%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.32% | Acc After: 99.51%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.81% | Acc After: 99.25%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.45% | Acc After: 99.86%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.36% | Acc After: 99.51%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.71% | Acc After: 99.12%\n",
      "\n",
      "[Round 9] Global Test Accuracy: 97.99%\n",
      "Client Acc BEFORE (mean ± std): 97.85% ± 1.99%\n",
      "Client Acc AFTER  (mean ± std): 99.06% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 10 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.27% | Acc After: 99.59%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.80% | Acc After: 98.33%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.26% | Acc After: 99.41%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.73% | Acc After: 97.96%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.28% | Acc After: 97.87%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.31% | Acc After: 99.51%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.10% | Acc After: 99.34%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.47% | Acc After: 99.89%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.37% | Acc After: 99.58%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.73% | Acc After: 99.04%\n",
      "\n",
      "[Round 10] Global Test Accuracy: 97.95%\n",
      "Client Acc BEFORE (mean ± std): 97.93% ± 1.88%\n",
      "Client Acc AFTER  (mean ± std): 99.05% ± 0.69%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 11 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.27% | Acc After: 99.60%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.68% | Acc After: 98.30%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.26% | Acc After: 99.42%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.67% | Acc After: 98.10%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.24% | Acc After: 97.94%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.28% | Acc After: 99.55%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.98% | Acc After: 99.37%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.49% | Acc After: 99.88%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.36% | Acc After: 99.58%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.70% | Acc After: 99.10%\n",
      "\n",
      "[Round 11] Global Test Accuracy: 97.97%\n",
      "Client Acc BEFORE (mean ± std): 97.89% ± 1.93%\n",
      "Client Acc AFTER  (mean ± std): 99.08% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 12 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.39% | Acc After: 99.61%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.68% | Acc After: 98.32%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.25% | Acc After: 99.42%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.72% | Acc After: 98.11%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.27% | Acc After: 97.83%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.24% | Acc After: 99.53%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.01% | Acc After: 99.30%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.58% | Acc After: 99.86%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.33% | Acc After: 99.51%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.65% | Acc After: 99.05%\n",
      "\n",
      "[Round 12] Global Test Accuracy: 97.98%\n",
      "Client Acc BEFORE (mean ± std): 97.91% ± 1.93%\n",
      "Client Acc AFTER  (mean ± std): 99.05% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 13 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.24% | Acc After: 99.59%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.91% | Acc After: 98.32%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.24% | Acc After: 99.41%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.73% | Acc After: 98.07%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.27% | Acc After: 97.97%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.18% | Acc After: 99.50%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.26% | Acc After: 99.38%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.46% | Acc After: 99.88%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.32% | Acc After: 99.65%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.67% | Acc After: 99.02%\n",
      "\n",
      "[Round 13] Global Test Accuracy: 98.00%\n",
      "Client Acc BEFORE (mean ± std): 97.93% ± 1.81%\n",
      "Client Acc AFTER  (mean ± std): 99.08% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 14 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.26% | Acc After: 99.61%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.91% | Acc After: 98.30%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.26% | Acc After: 99.44%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.74% | Acc After: 97.95%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.28% | Acc After: 97.93%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.21% | Acc After: 99.50%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.23% | Acc After: 99.28%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.47% | Acc After: 99.89%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.35% | Acc After: 99.54%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.69% | Acc After: 99.02%\n",
      "\n",
      "[Round 14] Global Test Accuracy: 97.99%\n",
      "Client Acc BEFORE (mean ± std): 97.94% ± 1.83%\n",
      "Client Acc AFTER  (mean ± std): 99.05% ± 0.69%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 15 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.39% | Acc After: 99.58%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.73% | Acc After: 98.31%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.26% | Acc After: 99.41%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.73% | Acc After: 98.04%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.29% | Acc After: 97.77%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.25% | Acc After: 99.51%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.05% | Acc After: 99.37%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.58% | Acc After: 99.87%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.35% | Acc After: 99.59%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.67% | Acc After: 99.08%\n",
      "\n",
      "[Round 15] Global Test Accuracy: 98.02%\n",
      "Client Acc BEFORE (mean ± std): 97.93% ± 1.91%\n",
      "Client Acc AFTER  (mean ± std): 99.05% ± 0.70%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 16 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.27% | Acc After: 99.58%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.98% | Acc After: 98.33%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.26% | Acc After: 99.44%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.78% | Acc After: 98.00%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.29% | Acc After: 97.86%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.18% | Acc After: 99.53%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.33% | Acc After: 99.35%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.50% | Acc After: 99.87%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.34% | Acc After: 99.58%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.68% | Acc After: 99.04%\n",
      "\n",
      "[Round 16] Global Test Accuracy: 97.95%\n",
      "Client Acc BEFORE (mean ± std): 97.96% ± 1.79%\n",
      "Client Acc AFTER  (mean ± std): 99.06% ± 0.69%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 17 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.38% | Acc After: 99.58%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.62% | Acc After: 98.30%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.24% | Acc After: 99.47%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.69% | Acc After: 98.20%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.25% | Acc After: 97.77%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.27% | Acc After: 99.39%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.97% | Acc After: 99.33%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.55% | Acc After: 99.88%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.34% | Acc After: 99.60%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.66% | Acc After: 99.01%\n",
      "\n",
      "[Round 17] Global Test Accuracy: 98.03%\n",
      "Client Acc BEFORE (mean ± std): 97.90% ± 1.95%\n",
      "Client Acc AFTER  (mean ± std): 99.05% ± 0.68%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 18 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.19% | Acc After: 99.59%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.08% | Acc After: 98.28%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.27% | Acc After: 99.42%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.78% | Acc After: 98.16%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.28% | Acc After: 97.81%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.17% | Acc After: 99.53%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.41% | Acc After: 99.28%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.45% | Acc After: 99.88%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.35% | Acc After: 99.47%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.71% | Acc After: 99.04%\n",
      "\n",
      "[Round 18] Global Test Accuracy: 97.97%\n",
      "Client Acc BEFORE (mean ± std): 97.97% ± 1.75%\n",
      "Client Acc AFTER  (mean ± std): 99.05% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 19 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.34% | Acc After: 99.60%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.70% | Acc After: 98.31%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.25% | Acc After: 99.39%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.71% | Acc After: 98.00%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.22% | Acc After: 97.70%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.18% | Acc After: 99.52%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.01% | Acc After: 99.31%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.57% | Acc After: 99.87%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.30% | Acc After: 99.40%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.64% | Acc After: 99.08%\n",
      "\n",
      "[Round 19] Global Test Accuracy: 97.95%\n",
      "Client Acc BEFORE (mean ± std): 97.89% ± 1.92%\n",
      "Client Acc AFTER  (mean ± std): 99.02% ± 0.70%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 20 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.46% | Acc After: 99.54%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.53% | Acc After: 98.30%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.25% | Acc After: 99.44%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.69% | Acc After: 97.98%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.26% | Acc After: 97.84%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.29% | Acc After: 99.54%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.81% | Acc After: 99.33%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.62% | Acc After: 99.85%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.35% | Acc After: 99.53%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.66% | Acc After: 99.04%\n",
      "\n",
      "[Round 20] Global Test Accuracy: 98.02%\n",
      "Client Acc BEFORE (mean ± std): 97.89% ± 2.01%\n",
      "Client Acc AFTER  (mean ± std): 99.04% ± 0.69%\n",
      "Client sample count (min, max): 29620, 171708\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_one_client(model, loader, epochs=1, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.cpu()\n",
    "\n",
    "def evaluate_local(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def average_weights(weight_list):\n",
    "    avg_weights = {}\n",
    "    for key in weight_list[0].keys():\n",
    "        avg_weights[key] = sum([w[key] for w in weight_list]) / len(weight_list)\n",
    "    return avg_weights\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "global_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "global_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "num_rounds = 20\n",
    "for rnd in range(1, num_rounds + 1):\n",
    "    adaptive_epochs = max(1, int(10 - 9 * (rnd-1) / (num_rounds-1)))\n",
    "    print(f\"\\n{'='*30}\\nFederated Round {rnd} (Local Epochs: {adaptive_epochs})\\n{'='*30}\")\n",
    "    local_weights = []\n",
    "    client_accuracies_before = []\n",
    "    client_accuracies_after = []\n",
    "    client_sample_counts = []\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        num_samples = len(client_loaders[client_id].dataset)\n",
    "        acc_before = evaluate_local(global_model, client_loaders[client_id])\n",
    "        local_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_model = train_one_client(local_model, client_loaders[client_id], epochs=adaptive_epochs)\n",
    "        acc_after = evaluate_local(local_model, client_loaders[client_id])\n",
    "        local_weights.append(local_model.state_dict())\n",
    "        client_sample_counts.append(num_samples)\n",
    "        client_accuracies_before.append(acc_before)\n",
    "        client_accuracies_after.append(acc_after)\n",
    "        print(f\"  Client {client_id+1:2d} | Samples: {num_samples:4d} | Acc Before: {acc_before:5.2f}% | Acc After: {acc_after:5.2f}%\")\n",
    "\n",
    "    global_model.load_state_dict(average_weights(local_weights))\n",
    "    acc_global = test_model(global_model, test_loader)\n",
    "    print(f\"\\n[Round {rnd}] Global Test Accuracy: {acc_global:.2f}%\")\n",
    "    print(f\"Client Acc BEFORE (mean ± std): {np.mean(client_accuracies_before):.2f}% ± {np.std(client_accuracies_before):.2f}%\")\n",
    "    print(f\"Client Acc AFTER  (mean ± std): {np.mean(client_accuracies_after):.2f}% ± {np.std(client_accuracies_after):.2f}%\")\n",
    "    print(f\"Client sample count (min, max): {min(client_sample_counts)}, {max(client_sample_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.02%\n",
      "Confusion Matrix:\n",
      " [[ 61535    136   1905   1925]\n",
      " [   210   1066      0      0]\n",
      " [   189      1  60881      0]\n",
      " [   502      0      0 117547]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def test_modelv2(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "    acc = 100. * correct / total\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    return acc, cm\n",
    "\n",
    "# Usage\n",
    "acc, cm = test_modelv2(global_model, test_loader)\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "def frhc_local_feature_selection(X, max_clusters=None, comp_feat=1):\n",
    "    \"\"\"\n",
    "    Local representative feature selection by hierarchical clustering of features.\n",
    "    \n",
    "    Parameters:\n",
    "        X: [n_samples, n_features] numpy array (client's local data)\n",
    "        max_clusters: int or None, maximum clusters to try for optimal selection\n",
    "        comp_feat: int, number of compensation features to add\n",
    "\n",
    "    Returns:\n",
    "        selected_feature_indices: list of selected feature indices\n",
    "    \"\"\"\n",
    "    n_features = X.shape[1]\n",
    "    # Step 1: Compute absolute correlation distance between features\n",
    "    corr_matrix = np.corrcoef(X, rowvar=False)\n",
    "    dist_matrix = 1 - np.abs(corr_matrix)\n",
    "    # Ensure distance matrix is valid\n",
    "    np.fill_diagonal(dist_matrix, 0)\n",
    "    # Convert to condensed form for linkage\n",
    "    condensed = squareform(dist_matrix, checks=False)\n",
    "    # Step 2: Hierarchical clustering\n",
    "    Z = linkage(condensed, method='average')\n",
    "    # Step 3: Optimal number of clusters (can be determined by a method, here use max_clusters or sqrt rule)\n",
    "    if max_clusters is None:\n",
    "        K = int(np.sqrt(n_features))\n",
    "    else:\n",
    "        K = min(max_clusters, n_features)\n",
    "    clusters = fcluster(Z, K, criterion='maxclust')\n",
    "    # Step 4: Find the two largest clusters\n",
    "    cluster_sizes = [(c, np.sum(clusters == c)) for c in np.unique(clusters)]\n",
    "    cluster_sizes.sort(key=lambda x: x[1], reverse=True)\n",
    "    selected_features = []\n",
    "    for i in range(min(2, len(cluster_sizes))):\n",
    "        c = cluster_sizes[i][0]\n",
    "        selected_features.extend(np.where(clusters == c)[0].tolist())\n",
    "    # Step 5: Optionally add compensation feature(s)\n",
    "    if comp_feat > 0:\n",
    "        feature_counts = [(c, np.sum(clusters == c)) for c in np.unique(clusters)]\n",
    "        cluster_sorted = sorted(feature_counts, key=lambda x: x[1], reverse=True)\n",
    "        # Add features from next largest clusters if needed\n",
    "        for i in range(2, min(2 + comp_feat, len(cluster_sorted))):\n",
    "            c = cluster_sorted[i][0]\n",
    "            selected_features.append(np.where(clusters == c)[0][0])\n",
    "    # Remove duplicates\n",
    "    selected_features = list(sorted(set(selected_features)))\n",
    "    return selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frhc_global_intersection(selected_lists):\n",
    "    \"\"\"\n",
    "    Compute global overlapping federated features as intersection of local sets.\n",
    "    Parameters:\n",
    "        selected_lists: list of list of feature indices (from each client)\n",
    "    Returns:\n",
    "        final_indices: list of feature indices present in all clients\n",
    "    \"\"\"\n",
    "    # Convert all to set for intersection\n",
    "    final_indices = set(selected_lists[0])\n",
    "    for feat_set in selected_lists[1:]:\n",
    "        final_indices &= set(feat_set)\n",
    "    return sorted(list(final_indices))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 31\n",
      "Global federated feature indices (FRHC): [0, 1, 2, 3, 4, 9, 11, 12, 15, 16, 17, 19, 20, 22, 23, 31, 32, 36, 37, 38, 39, 43, 44, 49, 51, 52, 53, 57, 66, 67, 69]\n",
      "Selected feature names: ['Flow Duration', 'Total Fwd Packet', 'Total Bwd packets', 'Total Length of Fwd Packet', 'Total Length of Bwd Packet', 'Bwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Total', 'Fwd IAT Mean', 'Fwd IAT Max', 'Fwd IAT Min', 'Fwd URG Flags', 'Bwd URG Flags', 'Bwd Packets/s', 'Packet Length Min', 'Packet Length Max', 'Packet Length Mean', 'SYN Flag Count', 'RST Flag Count', 'ECE Flag Count', 'Average Packet Size', 'Fwd Segment Size Avg', 'Bwd Segment Size Avg', 'Bwd Bytes/Bulk Avg', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Std']\n"
     ]
    }
   ],
   "source": [
    "# Suppose client_data_np is a list of (X_local, y_local) for all clients\n",
    "selected_lists = []\n",
    "for Xc, yc in client_data_np:\n",
    "    feats = frhc_local_feature_selection(Xc,max_clusters=11,comp_feat=1)\n",
    "    selected_lists.append(feats)\n",
    "\n",
    "# Global intersection at the server\n",
    "global_frhc_indices = frhc_global_intersection(selected_lists)\n",
    "print(\"Count:\",len(global_frhc_indices))\n",
    "print(\"Global federated feature indices (FRHC):\", global_frhc_indices)\n",
    "print(\"Selected feature names:\", [feature_cols[i] for i in global_frhc_indices])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_indices=global_frhc_indices\n",
    "X_sel = X[:, selected_indices]\n",
    "input_dim = X_sel.shape[1]\n",
    "full_dataset = TabularDataset(X_sel, y)\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "client_loaders = []\n",
    "for i in range(num_clients):\n",
    "    idxs = client_indices[i]\n",
    "    client_subset = Subset(train_dataset, idxs)\n",
    "    client_loader = DataLoader(client_subset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    client_loaders.append(client_loader)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=2):\n",
    "        super(TabularMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        features = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(features)\n",
    "        out = self.fc3(x)\n",
    "        if return_features:\n",
    "            return out, features\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Federated Round 1 (Local Epochs: 10)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before:  7.72% | Acc After: 95.19%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 34.80% | Acc After: 91.07%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 92.63% | Acc After: 97.94%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 37.57% | Acc After: 92.51%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 17.27% | Acc After: 90.25%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 77.57% | Acc After: 96.86%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 21.16% | Acc After: 97.87%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 47.54% | Acc After: 97.65%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 96.75% | Acc After: 97.85%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 87.05% | Acc After: 15.32%\n",
      "\n",
      "[Round 1] Global Test Accuracy: 83.58%\n",
      "Client Acc BEFORE (mean ± std): 52.00% ± 31.90%\n",
      "Client Acc AFTER  (mean ± std): 87.25% ± 24.14%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 2 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 69.82% | Acc After: 96.62%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 75.00% | Acc After: 82.66%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 96.85% | Acc After: 98.03%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 77.89% | Acc After: 92.66%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 71.06% | Acc After: 86.11%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 91.48% | Acc After: 96.98%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 70.26% | Acc After: 97.90%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 82.81% | Acc After: 98.29%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 98.09% | Acc After: 97.91%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 94.44% | Acc After: 96.57%\n",
      "\n",
      "[Round 2] Global Test Accuracy: 83.27%\n",
      "Client Acc BEFORE (mean ± std): 82.77% ± 10.91%\n",
      "Client Acc AFTER  (mean ± std): 94.37% ± 5.28%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 3 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 78.06% | Acc After: 95.68%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 86.91% | Acc After: 82.59%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 84.24% | Acc After: 97.89%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 82.54% | Acc After: 92.35%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 81.42% | Acc After: 86.38%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 82.19% | Acc After: 97.27%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 86.93% | Acc After: 97.70%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 80.81% | Acc After: 98.10%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 84.44% | Acc After: 97.91%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 84.57% | Acc After: 15.32%\n",
      "\n",
      "[Round 3] Global Test Accuracy: 43.28%\n",
      "Client Acc BEFORE (mean ± std): 83.21% ± 2.62%\n",
      "Client Acc AFTER  (mean ± std): 86.12% ± 24.16%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 4 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 64.86% | Acc After: 95.65%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 80.90% | Acc After: 82.68%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.16% | Acc After: 98.06%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 56.96% | Acc After: 65.85%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 72.93% | Acc After: 86.35%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 15.92% | Acc After: 97.07%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 96.06% | Acc After: 96.30%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 36.39% | Acc After: 98.43%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  2.96% | Acc After: 97.92%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.04% | Acc After: 96.57%\n",
      "\n",
      "[Round 4] Global Test Accuracy: 43.76%\n",
      "Client Acc BEFORE (mean ± std): 44.92% ± 31.98%\n",
      "Client Acc AFTER  (mean ± std): 91.49% ± 9.96%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 5 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 66.56% | Acc After: 95.61%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 80.87% | Acc After: 82.67%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.25% | Acc After: 97.99%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 57.70% | Acc After: 92.99%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 73.88% | Acc After: 89.95%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 16.28% | Acc After: 97.13%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 96.02% | Acc After: 97.37%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 37.42% | Acc After: 98.33%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.00% | Acc After: 98.05%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.07% | Acc After: 96.82%\n",
      "\n",
      "[Round 5] Global Test Accuracy: 45.12%\n",
      "Client Acc BEFORE (mean ± std): 45.41% ± 32.12%\n",
      "Client Acc AFTER  (mean ± std): 94.69% ± 4.73%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 6 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 73.00% | Acc After: 94.81%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 79.71% | Acc After: 82.65%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.36% | Acc After: 97.84%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 59.90% | Acc After: 66.36%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 77.26% | Acc After: 86.59%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.60% | Acc After: 96.68%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.97% | Acc After: 97.90%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 41.14% | Acc After: 98.53%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  2.98% | Acc After: 97.79%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 14.87% | Acc After: 14.65%\n",
      "\n",
      "[Round 6] Global Test Accuracy: 43.15%\n",
      "Client Acc BEFORE (mean ± std): 46.88% ± 32.51%\n",
      "Client Acc AFTER  (mean ± std): 83.38% ± 24.88%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 7 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 64.84% | Acc After: 96.96%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 80.56% | Acc After: 82.69%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.14% | Acc After: 97.80%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 56.77% | Acc After: 66.31%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 72.66% | Acc After: 86.16%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 15.86% | Acc After: 97.07%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 95.62% | Acc After: 97.97%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 36.35% | Acc After: 98.49%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  2.93% | Acc After: 97.85%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 14.96% | Acc After: 96.54%\n",
      "\n",
      "[Round 7] Global Test Accuracy: 44.97%\n",
      "Client Acc BEFORE (mean ± std): 44.77% ± 31.86%\n",
      "Client Acc AFTER  (mean ± std): 91.78% ± 9.98%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 8 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 73.12% | Acc After: 94.12%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 78.98% | Acc After: 82.51%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.48% | Acc After: 98.11%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 59.66% | Acc After: 89.32%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 77.03% | Acc After: 86.75%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.81% | Acc After: 97.12%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.05% | Acc After: 98.01%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 41.28% | Acc After: 98.45%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.15% | Acc After: 98.10%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 14.89% | Acc After: 96.48%\n",
      "\n",
      "[Round 8] Global Test Accuracy: 44.30%\n",
      "Client Acc BEFORE (mean ± std): 46.74% ± 32.22%\n",
      "Client Acc AFTER  (mean ± std): 93.90% ± 5.40%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 9 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 70.93% | Acc After: 91.17%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 78.91% | Acc After: 82.68%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.34% | Acc After: 97.93%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 58.76% | Acc After: 65.98%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 75.74% | Acc After: 86.57%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.21% | Acc After: 97.37%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.81% | Acc After: 98.60%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 40.02% | Acc After: 97.89%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  2.95% | Acc After: 96.84%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 14.78% | Acc After: 96.41%\n",
      "\n",
      "[Round 9] Global Test Accuracy: 44.98%\n",
      "Client Acc BEFORE (mean ± std): 46.05% ± 31.98%\n",
      "Client Acc AFTER  (mean ± std): 91.14% ± 9.86%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 10 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 72.36% | Acc After: 95.92%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 79.83% | Acc After: 82.60%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.36% | Acc After: 97.91%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 59.67% | Acc After: 66.07%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 76.98% | Acc After: 90.63%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.50% | Acc After: 97.24%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 95.04% | Acc After: 97.89%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 40.78% | Acc After: 98.38%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  2.99% | Acc After: 97.76%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 14.91% | Acc After: 96.83%\n",
      "\n",
      "[Round 10] Global Test Accuracy: 44.09%\n",
      "Client Acc BEFORE (mean ± std): 46.74% ± 32.46%\n",
      "Client Acc AFTER  (mean ± std): 92.12% ± 9.86%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 11 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 67.03% | Acc After: 96.52%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 81.15% | Acc After: 82.68%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.43% | Acc After: 97.93%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 57.98% | Acc After: 65.42%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 74.29% | Acc After: 86.51%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 16.55% | Acc After: 97.14%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 96.31% | Acc After: 95.50%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 37.80% | Acc After: 98.49%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.19% | Acc After: 97.28%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.28% | Acc After: 96.35%\n",
      "\n",
      "[Round 11] Global Test Accuracy: 42.30%\n",
      "Client Acc BEFORE (mean ± std): 45.70% ± 32.17%\n",
      "Client Acc AFTER  (mean ± std): 91.38% ± 10.02%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 12 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 60.34% | Acc After: 97.49%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 81.78% | Acc After: 82.70%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.02% | Acc After: 97.77%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 55.47% | Acc After: 66.42%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 70.46% | Acc After: 86.41%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 14.92% | Acc After: 97.26%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 96.82% | Acc After: 97.77%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 33.82% | Acc After: 98.16%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  2.91% | Acc After: 98.01%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.15% | Acc After: 96.60%\n",
      "\n",
      "[Round 12] Global Test Accuracy: 43.94%\n",
      "Client Acc BEFORE (mean ± std): 43.87% ± 31.86%\n",
      "Client Acc AFTER  (mean ± std): 91.86% ± 9.96%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 13 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 67.56% | Acc After: 96.88%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 80.49% | Acc After: 82.69%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.34% | Acc After: 97.99%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 57.93% | Acc After: 66.31%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 74.32% | Acc After: 86.54%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 16.52% | Acc After: 97.30%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 95.54% | Acc After: 97.75%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 38.10% | Acc After: 98.48%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.12% | Acc After: 98.07%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.08% | Acc After: 96.21%\n",
      "\n",
      "[Round 13] Global Test Accuracy: 45.26%\n",
      "Client Acc BEFORE (mean ± std): 45.60% ± 32.05%\n",
      "Client Acc AFTER  (mean ± std): 91.82% ± 9.97%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 14 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 73.02% | Acc After: 96.72%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 80.15% | Acc After: 82.69%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.41% | Acc After: 98.14%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 60.08% | Acc After: 66.32%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 77.46% | Acc After: 86.47%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.67% | Acc After: 97.22%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 95.35% | Acc After: 97.52%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 41.17% | Acc After: 98.55%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.05% | Acc After: 98.03%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 14.98% | Acc After: 96.72%\n",
      "\n",
      "[Round 14] Global Test Accuracy: 44.96%\n",
      "Client Acc BEFORE (mean ± std): 47.03% ± 32.60%\n",
      "Client Acc AFTER  (mean ± std): 91.84% ± 9.98%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 15 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 71.59% | Acc After: 95.33%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 80.36% | Acc After: 82.70%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.39% | Acc After: 98.04%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 59.60% | Acc After: 66.55%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 76.79% | Acc After: 86.65%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.40% | Acc After: 97.45%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 95.59% | Acc After: 97.89%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 40.36% | Acc After: 98.19%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.05% | Acc After: 97.78%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.05% | Acc After: 96.39%\n",
      "\n",
      "[Round 15] Global Test Accuracy: 44.78%\n",
      "Client Acc BEFORE (mean ± std): 46.72% ± 32.51%\n",
      "Client Acc AFTER  (mean ± std): 91.70% ± 9.82%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 16 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 70.74% | Acc After: 96.59%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 80.52% | Acc After: 82.70%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.41% | Acc After: 97.89%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 59.31% | Acc After: 66.31%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 76.24% | Acc After: 86.64%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.22% | Acc After: 97.17%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 95.76% | Acc After: 98.58%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 39.86% | Acc After: 98.64%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.07% | Acc After: 97.60%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.10% | Acc After: 96.39%\n",
      "\n",
      "[Round 16] Global Test Accuracy: 45.35%\n",
      "Client Acc BEFORE (mean ± std): 46.52% ± 32.44%\n",
      "Client Acc AFTER  (mean ± std): 91.85% ± 9.98%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 17 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 73.54% | Acc After: 97.51%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 79.78% | Acc After: 82.65%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.49% | Acc After: 97.88%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 60.18% | Acc After: 66.21%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 77.65% | Acc After: 86.34%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.86% | Acc After: 97.19%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.98% | Acc After: 97.93%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 41.48% | Acc After: 97.98%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.12% | Acc After: 97.89%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.00% | Acc After: 96.01%\n",
      "\n",
      "[Round 17] Global Test Accuracy: 45.09%\n",
      "Client Acc BEFORE (mean ± std): 47.11% ± 32.53%\n",
      "Client Acc AFTER  (mean ± std): 91.76% ± 9.99%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 18 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 72.87% | Acc After: 91.04%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 79.48% | Acc After: 82.68%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.53% | Acc After: 97.91%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 59.77% | Acc After: 93.02%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 77.13% | Acc After: 86.44%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 17.79% | Acc After: 96.85%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.55% | Acc After: 97.84%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 41.14% | Acc After: 98.08%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.16% | Acc After: 97.94%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 14.98% | Acc After: 96.54%\n",
      "\n",
      "[Round 18] Global Test Accuracy: 44.49%\n",
      "Client Acc BEFORE (mean ± std): 46.84% ± 32.32%\n",
      "Client Acc AFTER  (mean ± std): 93.83% ± 5.21%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 19 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 68.35% | Acc After: 96.68%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 81.38% | Acc After: 82.68%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.44% | Acc After: 97.85%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 58.62% | Acc After: 66.17%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 75.17% | Acc After: 86.76%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 16.74% | Acc After: 97.38%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 96.63% | Acc After: 97.84%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 38.53% | Acc After: 98.39%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.17% | Acc After: 97.84%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.29% | Acc After: 96.54%\n",
      "\n",
      "[Round 19] Global Test Accuracy: 44.31%\n",
      "Client Acc BEFORE (mean ± std): 46.13% ± 32.40%\n",
      "Client Acc AFTER  (mean ± std): 91.81% ± 9.98%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 20 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 67.61% | Acc After: 95.83%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 81.49% | Acc After: 82.62%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  7.43% | Acc After: 97.71%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 58.37% | Acc After: 92.92%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 74.73% | Acc After: 86.58%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 16.59% | Acc After: 97.29%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 96.76% | Acc After: 96.36%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 38.12% | Acc After: 98.61%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  3.15% | Acc After: 97.65%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 15.29% | Acc After: 96.15%\n",
      "\n",
      "[Round 20] Global Test Accuracy: 43.69%\n",
      "Client Acc BEFORE (mean ± std): 45.95% ± 32.37%\n",
      "Client Acc AFTER  (mean ± std): 94.17% ± 5.08%\n",
      "Client sample count (min, max): 29620, 171708\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_one_client(model, loader, epochs=1, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.cpu()\n",
    "\n",
    "def evaluate_local(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def average_weights(weight_list):\n",
    "    avg_weights = {}\n",
    "    for key in weight_list[0].keys():\n",
    "        avg_weights[key] = sum([w[key] for w in weight_list]) / len(weight_list)\n",
    "    return avg_weights\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "global_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "global_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "num_rounds = 20\n",
    "for rnd in range(1, num_rounds + 1):\n",
    "    adaptive_epochs = max(1, int(10 - 9 * (rnd-1) / (num_rounds-1)))\n",
    "    print(f\"\\n{'='*30}\\nFederated Round {rnd} (Local Epochs: {adaptive_epochs})\\n{'='*30}\")\n",
    "    local_weights = []\n",
    "    client_accuracies_before = []\n",
    "    client_accuracies_after = []\n",
    "    client_sample_counts = []\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        num_samples = len(client_loaders[client_id].dataset)\n",
    "        acc_before = evaluate_local(global_model, client_loaders[client_id])\n",
    "        local_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_model = train_one_client(local_model, client_loaders[client_id], epochs=adaptive_epochs)\n",
    "        acc_after = evaluate_local(local_model, client_loaders[client_id])\n",
    "        local_weights.append(local_model.state_dict())\n",
    "        client_sample_counts.append(num_samples)\n",
    "        client_accuracies_before.append(acc_before)\n",
    "        client_accuracies_after.append(acc_after)\n",
    "        print(f\"  Client {client_id+1:2d} | Samples: {num_samples:4d} | Acc Before: {acc_before:5.2f}% | Acc After: {acc_after:5.2f}%\")\n",
    "\n",
    "    global_model.load_state_dict(average_weights(local_weights))\n",
    "    acc_global = test_model(global_model, test_loader)\n",
    "    print(f\"\\n[Round {rnd}] Global Test Accuracy: {acc_global:.2f}%\")\n",
    "    print(f\"Client Acc BEFORE (mean ± std): {np.mean(client_accuracies_before):.2f}% ± {np.std(client_accuracies_before):.2f}%\")\n",
    "    print(f\"Client Acc AFTER  (mean ± std): {np.mean(client_accuracies_after):.2f}% ± {np.std(client_accuracies_after):.2f}%\")\n",
    "    print(f\"Client sample count (min, max): {min(client_sample_counts)}, {max(client_sample_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 43.69%\n",
      "Confusion Matrix:\n",
      " [[ 64488     12    762    239]\n",
      " [   851    411      4     10]\n",
      " [ 18716      0  42319     36]\n",
      " [117818      0     24    207]]\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "acc, cm = test_modelv2(global_model, test_loader)\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sel = X\n",
    "input_dim = X_sel.shape[1]\n",
    "full_dataset = TabularDataset(X_sel, y)\n",
    "train_dataset = Subset(full_dataset, train_idx)\n",
    "test_dataset = Subset(full_dataset, test_idx)\n",
    "\n",
    "client_loaders = []\n",
    "for i in range(num_clients):\n",
    "    idxs = client_indices[i]\n",
    "    client_subset = Subset(train_dataset, idxs)\n",
    "    client_loader = DataLoader(client_subset, batch_size=128, shuffle=True, drop_last=True)\n",
    "    client_loaders.append(client_loader)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(np.unique(y))\n",
    "\n",
    "class TabularMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_classes=2):\n",
    "        super(TabularMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "    def forward(self, x, return_features=False):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        features = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout(features)\n",
    "        out = self.fc3(x)\n",
    "        if return_features:\n",
    "            return out, features\n",
    "        else:\n",
    "            return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Federated Round 1 (Local Epochs: 10)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 23.49% | Acc After: 99.53%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 27.38% | Acc After: 98.22%\n",
      "  Client  3 | Samples: 171708 | Acc Before:  3.13% | Acc After: 99.37%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 19.92% | Acc After: 98.04%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 25.50% | Acc After: 97.81%\n",
      "  Client  6 | Samples: 75831 | Acc Before:  6.14% | Acc After: 99.36%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 32.67% | Acc After: 99.26%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 13.70% | Acc After: 99.84%\n",
      "  Client  9 | Samples: 29620 | Acc Before:  1.60% | Acc After: 99.51%\n",
      "  Client 10 | Samples: 123281 | Acc Before:  5.67% | Acc After: 99.04%\n",
      "\n",
      "[Round 1] Global Test Accuracy: 96.32%\n",
      "Client Acc BEFORE (mean ± std): 15.92% ± 10.73%\n",
      "Client Acc AFTER  (mean ± std): 99.00% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 2 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 97.88% | Acc After: 99.60%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 91.06% | Acc After: 98.34%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.08% | Acc After: 99.48%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 95.50% | Acc After: 98.04%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 94.39% | Acc After: 97.94%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 98.12% | Acc After: 99.44%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 89.62% | Acc After: 99.29%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.00% | Acc After: 99.86%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.02% | Acc After: 99.57%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 97.68% | Acc After: 99.13%\n",
      "\n",
      "[Round 2] Global Test Accuracy: 97.75%\n",
      "Client Acc BEFORE (mean ± std): 96.14% ± 3.26%\n",
      "Client Acc AFTER  (mean ± std): 99.07% ± 0.66%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 3 (Local Epochs: 9)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.00% | Acc After: 99.62%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.17% | Acc After: 98.26%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.26% | Acc After: 99.48%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.36% | Acc After: 98.15%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 96.82% | Acc After: 97.88%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.20% | Acc After: 99.50%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.38% | Acc After: 99.32%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.38% | Acc After: 99.90%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.39% | Acc After: 99.51%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.58% | Acc After: 99.14%\n",
      "\n",
      "[Round 3] Global Test Accuracy: 97.97%\n",
      "Client Acc BEFORE (mean ± std): 97.65% ± 2.12%\n",
      "Client Acc AFTER  (mean ± std): 99.07% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 4 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.36% | Acc After: 99.64%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.62% | Acc After: 98.18%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.30% | Acc After: 99.51%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.72% | Acc After: 98.19%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.22% | Acc After: 97.99%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.29% | Acc After: 99.53%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 93.94% | Acc After: 99.32%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.57% | Acc After: 99.87%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.38% | Acc After: 99.62%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.68% | Acc After: 99.17%\n",
      "\n",
      "[Round 4] Global Test Accuracy: 98.10%\n",
      "Client Acc BEFORE (mean ± std): 97.91% ± 1.96%\n",
      "Client Acc AFTER  (mean ± std): 99.10% ± 0.67%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 5 (Local Epochs: 8)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.38% | Acc After: 99.64%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.09% | Acc After: 98.30%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.30% | Acc After: 99.52%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.87% | Acc After: 98.16%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.44% | Acc After: 98.09%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.31% | Acc After: 99.61%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.51% | Acc After: 99.43%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.55% | Acc After: 99.89%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.36% | Acc After: 99.67%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.75% | Acc After: 99.17%\n",
      "\n",
      "[Round 5] Global Test Accuracy: 98.18%\n",
      "Client Acc BEFORE (mean ± std): 98.05% ± 1.76%\n",
      "Client Acc AFTER  (mean ± std): 99.15% ± 0.66%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 6 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.40% | Acc After: 99.64%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.27% | Acc After: 98.43%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.34% | Acc After: 99.53%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.94% | Acc After: 98.22%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.54% | Acc After: 98.01%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.35% | Acc After: 99.56%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.71% | Acc After: 99.37%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.57% | Acc After: 99.89%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.42% | Acc After: 99.69%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.80% | Acc After: 99.15%\n",
      "\n",
      "[Round 6] Global Test Accuracy: 98.13%\n",
      "Client Acc BEFORE (mean ± std): 98.13% ± 1.70%\n",
      "Client Acc AFTER  (mean ± std): 99.15% ± 0.64%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 7 (Local Epochs: 7)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.39% | Acc After: 99.67%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.04% | Acc After: 98.31%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.37% | Acc After: 99.54%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.89% | Acc After: 97.28%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.43% | Acc After: 98.07%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.37% | Acc After: 99.62%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.46% | Acc After: 99.36%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.60% | Acc After: 99.89%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.47% | Acc After: 99.67%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.79% | Acc After: 99.22%\n",
      "\n",
      "[Round 7] Global Test Accuracy: 98.07%\n",
      "Client Acc BEFORE (mean ± std): 98.08% ± 1.81%\n",
      "Client Acc AFTER  (mean ± std): 99.06% ± 0.82%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 8 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.48% | Acc After: 99.70%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 94.87% | Acc After: 98.43%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.30% | Acc After: 99.51%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.85% | Acc After: 98.33%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.46% | Acc After: 98.05%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.35% | Acc After: 99.56%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.27% | Acc After: 99.42%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.62% | Acc After: 99.90%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.38% | Acc After: 99.71%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.70% | Acc After: 99.26%\n",
      "\n",
      "[Round 8] Global Test Accuracy: 98.23%\n",
      "Client Acc BEFORE (mean ± std): 98.03% ± 1.86%\n",
      "Client Acc AFTER  (mean ± std): 99.19% ± 0.63%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 9 (Local Epochs: 6)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.42% | Acc After: 99.63%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.33% | Acc After: 98.37%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.40% | Acc After: 99.55%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 97.99% | Acc After: 98.11%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.56% | Acc After: 98.07%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.39% | Acc After: 99.61%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.78% | Acc After: 99.40%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.61% | Acc After: 99.91%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.49% | Acc After: 99.68%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.86% | Acc After: 99.28%\n",
      "\n",
      "[Round 9] Global Test Accuracy: 98.25%\n",
      "Client Acc BEFORE (mean ± std): 98.18% ± 1.70%\n",
      "Client Acc AFTER  (mean ± std): 99.16% ± 0.66%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 10 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.43% | Acc After: 99.68%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.37% | Acc After: 98.40%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.40% | Acc After: 99.54%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.01% | Acc After: 98.24%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.57% | Acc After: 98.01%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.40% | Acc After: 99.61%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.80% | Acc After: 99.45%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.61% | Acc After: 99.91%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.48% | Acc After: 99.69%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.85% | Acc After: 99.23%\n",
      "\n",
      "[Round 10] Global Test Accuracy: 98.27%\n",
      "Client Acc BEFORE (mean ± std): 98.19% ± 1.69%\n",
      "Client Acc AFTER  (mean ± std): 99.17% ± 0.65%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 11 (Local Epochs: 5)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.42% | Acc After: 99.67%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.48% | Acc After: 98.41%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.41% | Acc After: 99.52%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.05% | Acc After: 98.26%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.63% | Acc After: 98.04%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.41% | Acc After: 99.62%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.95% | Acc After: 99.42%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.61% | Acc After: 99.90%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.51% | Acc After: 99.68%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.88% | Acc After: 99.21%\n",
      "\n",
      "[Round 11] Global Test Accuracy: 98.27%\n",
      "Client Acc BEFORE (mean ± std): 98.23% ± 1.64%\n",
      "Client Acc AFTER  (mean ± std): 99.17% ± 0.64%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 12 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.40% | Acc After: 99.68%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.48% | Acc After: 98.44%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.41% | Acc After: 99.48%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.05% | Acc After: 98.27%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.63% | Acc After: 98.09%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.40% | Acc After: 99.60%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.94% | Acc After: 99.42%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.60% | Acc After: 99.91%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.51% | Acc After: 99.65%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.88% | Acc After: 99.23%\n",
      "\n",
      "[Round 12] Global Test Accuracy: 98.27%\n",
      "Client Acc BEFORE (mean ± std): 98.23% ± 1.64%\n",
      "Client Acc AFTER  (mean ± std): 99.18% ± 0.62%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 13 (Local Epochs: 4)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.44% | Acc After: 99.64%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.45% | Acc After: 98.42%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.41% | Acc After: 99.53%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.05% | Acc After: 98.19%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.62% | Acc After: 98.02%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.40% | Acc After: 99.61%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.86% | Acc After: 99.48%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.62% | Acc After: 99.89%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.51% | Acc After: 99.70%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.87% | Acc After: 99.22%\n",
      "\n",
      "[Round 13] Global Test Accuracy: 98.27%\n",
      "Client Acc BEFORE (mean ± std): 98.22% ± 1.66%\n",
      "Client Acc AFTER  (mean ± std): 99.17% ± 0.65%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 14 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.41% | Acc After: 99.64%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.45% | Acc After: 98.43%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.42% | Acc After: 99.46%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.04% | Acc After: 98.35%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.62% | Acc After: 98.06%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.41% | Acc After: 99.61%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.89% | Acc After: 99.42%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.61% | Acc After: 99.91%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.50% | Acc After: 99.68%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.88% | Acc After: 99.28%\n",
      "\n",
      "[Round 14] Global Test Accuracy: 98.29%\n",
      "Client Acc BEFORE (mean ± std): 98.22% ± 1.66%\n",
      "Client Acc AFTER  (mean ± std): 99.18% ± 0.62%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 15 (Local Epochs: 3)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.43% | Acc After: 99.64%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.49% | Acc After: 98.46%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.41% | Acc After: 99.52%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.05% | Acc After: 97.86%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.65% | Acc After: 98.08%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.41% | Acc After: 99.62%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.93% | Acc After: 99.44%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.62% | Acc After: 99.90%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.50% | Acc After: 99.70%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.88% | Acc After: 99.22%\n",
      "\n",
      "[Round 15] Global Test Accuracy: 98.26%\n",
      "Client Acc BEFORE (mean ± std): 98.24% ± 1.64%\n",
      "Client Acc AFTER  (mean ± std): 99.15% ± 0.70%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 16 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.44% | Acc After: 99.68%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.41% | Acc After: 98.43%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.41% | Acc After: 99.52%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.02% | Acc After: 98.17%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.62% | Acc After: 98.05%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.40% | Acc After: 99.56%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.85% | Acc After: 99.42%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.62% | Acc After: 99.90%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.49% | Acc After: 99.70%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.86% | Acc After: 99.24%\n",
      "\n",
      "[Round 16] Global Test Accuracy: 98.27%\n",
      "Client Acc BEFORE (mean ± std): 98.21% ± 1.67%\n",
      "Client Acc AFTER  (mean ± std): 99.17% ± 0.65%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 17 (Local Epochs: 2)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.43% | Acc After: 99.65%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.41% | Acc After: 98.42%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.42% | Acc After: 99.49%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.04% | Acc After: 98.29%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.64% | Acc After: 98.12%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.41% | Acc After: 99.59%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.86% | Acc After: 99.39%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.63% | Acc After: 99.90%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.52% | Acc After: 99.69%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.87% | Acc After: 99.25%\n",
      "\n",
      "[Round 17] Global Test Accuracy: 98.30%\n",
      "Client Acc BEFORE (mean ± std): 98.22% ± 1.67%\n",
      "Client Acc AFTER  (mean ± std): 99.18% ± 0.62%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 18 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.44% | Acc After: 99.62%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.52% | Acc After: 98.38%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.43% | Acc After: 99.54%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.08% | Acc After: 98.31%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.71% | Acc After: 98.08%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.41% | Acc After: 99.62%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.96% | Acc After: 99.37%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.63% | Acc After: 99.90%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.52% | Acc After: 99.70%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.88% | Acc After: 99.24%\n",
      "\n",
      "[Round 18] Global Test Accuracy: 98.32%\n",
      "Client Acc BEFORE (mean ± std): 98.26% ± 1.63%\n",
      "Client Acc AFTER  (mean ± std): 99.18% ± 0.63%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 19 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.46% | Acc After: 99.63%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.52% | Acc After: 98.42%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.42% | Acc After: 99.47%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.09% | Acc After: 98.15%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.73% | Acc After: 98.05%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.41% | Acc After: 99.60%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 95.00% | Acc After: 99.43%\n",
      "  Client  8 | Samples: 129987 | Acc Before: 99.64% | Acc After: 99.90%\n",
      "  Client  9 | Samples: 29620 | Acc Before: 99.51% | Acc After: 99.58%\n",
      "  Client 10 | Samples: 123281 | Acc Before: 98.90% | Acc After: 99.17%\n",
      "\n",
      "[Round 19] Global Test Accuracy: 98.28%\n",
      "Client Acc BEFORE (mean ± std): 98.27% ± 1.62%\n",
      "Client Acc AFTER  (mean ± std): 99.14% ± 0.64%\n",
      "Client sample count (min, max): 29620, 171708\n",
      "\n",
      "==============================\n",
      "Federated Round 20 (Local Epochs: 1)\n",
      "==============================\n",
      "  Client  1 | Samples: 76882 | Acc Before: 99.47% | Acc After: 99.60%\n",
      "  Client  2 | Samples: 87516 | Acc Before: 95.38% | Acc After: 98.36%\n",
      "  Client  3 | Samples: 171708 | Acc Before: 99.43% | Acc After: 99.53%\n",
      "  Client  4 | Samples: 118701 | Acc Before: 98.04% | Acc After: 98.38%\n",
      "  Client  5 | Samples: 74510 | Acc Before: 97.65% | Acc After: 98.04%\n",
      "  Client  6 | Samples: 75831 | Acc Before: 99.42% | Acc After: 99.57%\n",
      "  Client  7 | Samples: 95549 | Acc Before: 94.84% | Acc After: 99.39%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def train_one_client(model, loader, epochs=1, lr=0.01):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    return model.cpu()\n",
    "\n",
    "def evaluate_local(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def test_model(model, loader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += data.size(0)\n",
    "    acc = 100. * correct / total\n",
    "    return acc\n",
    "\n",
    "def average_weights(weight_list):\n",
    "    avg_weights = {}\n",
    "    for key in weight_list[0].keys():\n",
    "        avg_weights[key] = sum([w[key] for w in weight_list]) / len(weight_list)\n",
    "    return avg_weights\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "global_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "global_model.load_state_dict(global_model.state_dict())\n",
    "\n",
    "num_rounds = 20\n",
    "for rnd in range(1, num_rounds + 1):\n",
    "    adaptive_epochs = max(1, int(10 - 9 * (rnd-1) / (num_rounds-1)))\n",
    "    print(f\"\\n{'='*30}\\nFederated Round {rnd} (Local Epochs: {adaptive_epochs})\\n{'='*30}\")\n",
    "    local_weights = []\n",
    "    client_accuracies_before = []\n",
    "    client_accuracies_after = []\n",
    "    client_sample_counts = []\n",
    "\n",
    "    for client_id in range(num_clients):\n",
    "        num_samples = len(client_loaders[client_id].dataset)\n",
    "        acc_before = evaluate_local(global_model, client_loaders[client_id])\n",
    "        local_model = TabularMLP(input_dim=input_dim, num_classes=num_classes)\n",
    "        local_model.load_state_dict(global_model.state_dict())\n",
    "        local_model = train_one_client(local_model, client_loaders[client_id], epochs=adaptive_epochs)\n",
    "        acc_after = evaluate_local(local_model, client_loaders[client_id])\n",
    "        local_weights.append(local_model.state_dict())\n",
    "        client_sample_counts.append(num_samples)\n",
    "        client_accuracies_before.append(acc_before)\n",
    "        client_accuracies_after.append(acc_after)\n",
    "        print(f\"  Client {client_id+1:2d} | Samples: {num_samples:4d} | Acc Before: {acc_before:5.2f}% | Acc After: {acc_after:5.2f}%\")\n",
    "\n",
    "    global_model.load_state_dict(average_weights(local_weights))\n",
    "    acc_global = test_model(global_model, test_loader)\n",
    "    print(f\"\\n[Round {rnd}] Global Test Accuracy: {acc_global:.2f}%\")\n",
    "    print(f\"Client Acc BEFORE (mean ± std): {np.mean(client_accuracies_before):.2f}% ± {np.std(client_accuracies_before):.2f}%\")\n",
    "    print(f\"Client Acc AFTER  (mean ± std): {np.mean(client_accuracies_after):.2f}% ± {np.std(client_accuracies_after):.2f}%\")\n",
    "    print(f\"Client sample count (min, max): {min(client_sample_counts)}, {max(client_sample_counts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "acc, cm = test_modelv2(global_model, test_loader)\n",
    "print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1303049,
     "sourceId": 2260912,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
